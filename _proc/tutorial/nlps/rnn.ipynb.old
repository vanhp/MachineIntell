{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f7157eda",
   "metadata": {},
   "source": [
    "---\n",
    "title: Recurrent Neural Network (RNN)\n",
    "subtitle: Looking under the hood of RNN\n",
    "about:\n",
    "  template: marquee\n",
    "  image: ../../images/ai-pic7cp.jpg\n",
    "  links:\n",
    "    - icon: twitter\n",
    "      text: twitter\n",
    "      href: https://twitter.com\n",
    "    - icon: github\n",
    "      text: Github\n",
    "      href: https://github.com\n",
    "listing:\n",
    "  sort: \"date desc\"\n",
    "  contents: \"posts\"\n",
    "  sort-ui: false\n",
    "  filter-ui: false\n",
    "  categories: true\n",
    "  feed: true\n",
    "\n",
    "page-layout: full\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843eb827",
   "metadata": {},
   "source": [
    "![](./image/rnn.jpg)\n",
    "\n",
    "The Markov model weakness is that it's limit to context window(scope) that was choosen.\n",
    "Using the info that was stored to predict prior data then feed that info to model while it was considering current tag.\n",
    "\n",
    "## Bidirectional RNN\n",
    "A technique to train two independent RNN where one process from start to end the other process from end to start then combine the output from both into single one\n",
    "\n",
    "![](./image/birnn.jpg)\n",
    "\n",
    "\n",
    "## Long short term memory (LSTM)\n",
    "![](./image/rnn-lstm.jpg)\n",
    "\n",
    "An RNN that has the capability to forget the info that is not relevant to the current task.\n",
    "\n",
    "![](./image/lstm.jpg)\n",
    "\n",
    "### LSTM has \n",
    "- forget gate to delete info of non relevant from current context\n",
    "- add gate to select new info into current context with tanh activation that indicate the direction of info(should care about) and a sigmoid to indicate the scaling(how much should be care about) factor of the info to be add to forget gate to produce state context\n",
    "- out gate with sigmoid combine with state context to output result\n",
    "\n",
    "\n",
    "## Creating a Language Model from Scratch\n",
    "\n",
    "A language model is a model that predict the next word in the sentence.\n",
    "\n",
    "### First Language Model \n",
    "\n",
    "Build a model to predict each word based on the previous three words\n",
    "by create a list of every sequence of three words as independent variables, and the next word after each sequence as the dependent variable.\n",
    "\n",
    "The model takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. It  use the standard three linear layers, but with two tweaks.\n",
    "\n",
    "1. The first linear layer will use only the first word's embedding as activations, \n",
    "    - The second layer will use the second word's embedding plus the first layer's output activations, and \n",
    "    - The third layer will use the third word's embedding plus the second layer's output activations. \n",
    "    - The key effect of this is that every word is interpreted in the information context of any words preceding it.\n",
    "2. Each of these three layers will use the same weight matrix\n",
    "    - The way that one word impacts the activations from previous words should not change depending on the position of a word. \n",
    "    - Activation values will change as data moves through the layers, but the layer's weights themselves will not change from layer to layer. \n",
    "    - So, a layer does not learn one sequence position; it must learn to handle all positions.\n",
    "\n",
    "## The architect of the first model\n",
    "![simple linear LM](./image/simpleLinear1.png)\n",
    "\n",
    "Here the figure the model. Where word is the input, FC is fully connected layer and triangular is output prediction\n",
    "\n",
    "### 3 layers model code\n",
    "\n",
    "The first cut of the code for 3 layers model use:\n",
    "\n",
    "1. The embedding layer (input2_hidden, for input to hidden) \n",
    "2. The linear layer to create the activations for the next word (hidden2_hidden, for hidden to hidden)\n",
    "3. A final linear layer to predict the fourth word (hidden2_output, for hidden to output)\n",
    "\n",
    "They all use the same embedding since they come from same data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce14468",
   "metadata": {},
   "source": [
    "```python\n",
    "class LMModel1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n",
    "  # input2_hidden is embedding layer\n",
    "  # hidden2_hidden is linear layer\n",
    "  # hidden2_output is linear layer   \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.hidden2_hidden(self.input2_hidden(x[:,0])))\n",
    "        h = h + self.input2_hidden(x[:,1])\n",
    "        h = F.relu(self.hidden2_hidden(h))\n",
    "        h = h + self.input2_hidden(x[:,2])\n",
    "        h = F.relu(self.hidden2_hidden(h))\n",
    "        return self.hidden2_output(h)\n",
    "\n",
    "L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n",
    "# tensor of numericalized value for model\n",
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
    "seqs\n",
    "bs = 64\n",
    "cut = int(len(seqs) * 0.8)\n",
    "# create batch\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n",
    "\n",
    "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)   \n",
    "```\n",
    "compare to the simplest model which always predict the next word which is 'thousand' to see how it performs:\n",
    "\n",
    "```python\n",
    "# a simplest model that always predict 'thousand' on each input sentence\n",
    "c = Counter(tokens[cut:])\n",
    "mc = c.most_common(5)\n",
    "mc\n",
    "mc[0][1] / len(tokens[cut:])\n",
    "\n",
    "n,counts = 0,torch.zeros(len(vocab))\n",
    "for x,y in dls.valid:\n",
    "    n += y.shape[0]\n",
    "    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
    "#index of the most common words ('thousand')\n",
    "idx = torch.argmax(counts)\n",
    "idx, vocab[idx.item()], counts[idx].item()/n\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Refactor to use loop: the RNN\n",
    "![refactor2RNN](./image/refac_rnn.png)\n",
    "\n",
    "Rewrite the code to use loop this is look closer to RNN\n",
    "```\n",
    "class LMModel2(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n",
    "        \n",
    "    # refactor to use for loop the RNN!\n",
    "    def forward(self, x):\n",
    "        h = 0.               # using broascast\n",
    "        for i in range(3):\n",
    "            h = h + self.input2_hidden(x[:,i])\n",
    "            h = F.relu(self.hidden2_hidden(h))\n",
    "        return self.hidden2_output(h)\n",
    "\n",
    "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)\n",
    "\n",
    "```\n",
    "\n",
    "### Refactor to add memory to RNN\n",
    "\n",
    "Add the ability to retain previous word instead of start up new every time\n",
    "![stack-layer](./image/stacklayer_rnn.png)\n",
    "```\n",
    "class LMModel3(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0.  # using broascast\n",
    "        \n",
    "    # refactor to use for loop the RNN!\n",
    "    def forward(self, x):\n",
    "                   \n",
    "        for i in range(3):\n",
    "            self.h = self.h + self.input2_hidden(x[:,i])\n",
    "            self.h = F.relu(self.hidden2_hidden(h))\n",
    "            \n",
    "        out = self.hidden2_output(self.h)\n",
    "        self.h = self.h.detach() # do bptt\n",
    "        return out\n",
    "    def reset(self): self.h = 0.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### BPTT Backpropagation through time \n",
    "This model hidden state now can remember previous activation from previous batch and the gradient will be calculate on sequence length token of the past not instead of recalculate with each new stream this is call BPTT\n",
    "\n",
    "```\n",
    "# rearrange data so model see in particular sequence\n",
    "m = len(seqs)//bs\n",
    "m,bs,len(seqs)\n",
    "\n",
    "# reindex model see as contiguous batch with each epoch\n",
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs\n",
    "    new_ds = L()\n",
    "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
    "    return new_ds\n",
    "\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(\n",
    "    group_chunks(seqs[:cut], bs), \n",
    "    group_chunks(seqs[cut:], bs), \n",
    "    bs=bs, drop_last=True, # drop last batch that have diff shape\n",
    "    shuffle=False) # maintain sequence\n",
    "\n",
    "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                metrics=accuracy, \n",
    "                cbs=ModelResetter) # callback to reset each epoch\n",
    "learn.fit_one_cycle(10, 3e-3)\n",
    "```\n",
    "\n",
    "Add More signal: keep the output\n",
    "\n",
    "The model no longer throw away the output from previous run but add them as input to current run\n",
    "\n",
    "```\n",
    "sl = 16\n",
    "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
    "         for i in range(0,len(nums)-sl-1,sl))\n",
    "cut = int(len(seqs) * 0.8)\n",
    "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
    "                             group_chunks(seqs[cut:], bs),\n",
    "                             bs=bs, drop_last=True, shuffle=False)\n",
    "\n",
    "# check if it still offset by 1\n",
    "[L(vocab[o] for o in s) for s in seqs[0]]\n",
    "```\n",
    "\n",
    "Rewrite the model to now output every word instead of every 3 words in order to feed this into next run\n",
    "\n",
    "```\n",
    "class LMModel4(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
    "        self.h = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for i in range(sl):\n",
    "            self.h = self.h + self.i_h(x[:,i])\n",
    "            self.h = F.relu(self.h_h(self.h))\n",
    "            outs.append(self.h_o(self.h))\n",
    "        self.h = self.h.detach()\n",
    "        return torch.stack(outs, dim=1)\n",
    "    \n",
    "    def reset(self): self.h = 0\n",
    "\n",
    "    def loss_func(inp, targ):\n",
    "        return F.cross_entropy(inp.view(-1, len(vocab)), # flatten out to match bs x sl x vocab_sz from model\n",
    "        targ.view(-1)) # flatten out to match bs x sl x vocab_sz from model\n",
    "\n",
    "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)       \n",
    "```\n",
    "\n",
    "### Recurrent Neural Network: RNN\n",
    "![RNN](./image/rnn-pic1.jpeg)\n",
    "RNN feed the output activation value back into hidden layer to help the hidden layers retain info about previous run therefore have some *memory* of the past.\n",
    "\n",
    "### Multi-Layer RNN\n",
    "since the current model use the same weight matrix for each hidden layer which mean there no new info to be learn from. One way to \n",
    "To improve the model further is to stack more layers by feed the output from one layer into the next layer so on.\n",
    "\n",
    "\n",
    "Look at it in unrolling way\n",
    "![stack-layer](./image/stacklayer_rnn2.png)\n",
    "\n",
    "Refactoring to use PyTorch\n",
    "\n",
    "The model now has too deep layers this could lead to problem of *vanishing* or *exploding* gradient\n",
    "\n",
    "```\n",
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = h.detach()\n",
    "        return self.h_o(res)\n",
    "\n",
    "    def reset(self): self.h.zero_()\n",
    "\n",
    "learn = Learner(dls, LMModel5(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)\n",
    "```\n",
    "### Exploding or Disappearing Activations\n",
    "\n",
    "The problem stem from the gradient value calcutated from the output layer won't propagated back to the earlier layer. This is because the network is too deep.\n",
    "\n",
    "Vanishing Gradient:\n",
    "\n",
    "As the gradient value travel backward the especially the small value is diminishing as the floating point value get computed and recomputed many time each time it get round off closer and closer to 0 and finally become 0.\n",
    "\n",
    "Exploding Gradient:\n",
    "\n",
    "This the opposite of vanishing gradient. This phenomena happen espcially when the large value get larger with each computation it get exponentially large until it get large closer to infinity and become useless.\n",
    "![vanish](./image/vanish1.jpg)\n",
    "\n",
    "### The floating point problem\n",
    "One problem with doing floating point math with computer is that computer has limit amount of storage and floating point number is imprecise e.g. 1.0000012 or 1.00000000000000000000000012 is these two numbers are the same value? The answer is it's depend on what do you want to do with it. On some task the first one is good enough but on other the second is what require. To store the second number require more storage. \n",
    "\n",
    "The impreciseness is rather harder to understand. Float number get denser and denser when value get closer and closer to 0. In practice how do you represent this to the computer so it do some useful work. The IEEE organization come up with a standard which require how much storage should be devoted to certain type of floating point value e.g. 8-bit folating point,16-bit, 32-bit which mean the storage depend on the precision the user requirement not the actual value it may case the value get round-off read more from [here](http://www.volkerschatz.com/science/float.html). This mean do more calculation may lead less precision. This is what happen when doing lot and lot of gradient calculations the value may end up at 0 which is call vanishing  gradient or the value get larger and larger exponentially or explode to infinite.\n",
    "\n",
    "These problems are the main reason why RNN model is hard to train than CNN model,however research is very active to try new way to reduce or avoid these problems.\n",
    "\n",
    "### Neural Network that have Memory\n",
    "\n",
    "In Machine Learning two approach to handle Explode and Vanishing gradient is to use Neural network that can retain previous activation and also can figure out what value should be retain and which should be removed. The two popular approch are LSTM and GRU. GRU is a simplify version of LSTM\n",
    "\n",
    "![lstm-gru](./image/rnn_lstm_gru.png)\n",
    "\n",
    "### Comparison LSTM and GRU\n",
    "\n",
    "![gru](./image/lstm-gru1.png)\n",
    "\n",
    "### LSTM architecture \n",
    "\n",
    "Internally LSTM comprise with little neural net that decise how much gradient is needed and which one to update or delete.\n",
    "![lstm-gru](./image/lstm1.png)\n",
    "\n",
    "\n",
    "*sigmoid equation:*\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1} = \\frac12 + \\frac12 \\tanh\\left(\\frac{x}{2}\\right)$\n",
    "\n",
    "Sigmoid only let positive value between 0 and 1 pass through\n",
    "\n",
    "*tanh equation:*\n",
    "\n",
    "$f(x) = \\tanh x = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "\n",
    "Tanh only let value between -1 and 1 pass through\n",
    "\n",
    "\n",
    "\n",
    "### Another look at LSTM internal:\n",
    "![lstm-gru](./image/lstm_eq.png)\n",
    "\n",
    "The little NN is compose of gates call forget gate,input gate, cell gate, output gate. These gates work together to provide LSTM the capability to remember activation value that is important and forget the unneccessary activation value\n",
    "\n",
    "#### The forget $f_{t}$ gate:\n",
    "\n",
    "Take input $x$, hidden state $h_{t-1}$ then gated them via the sigmoid $\\sigma$ activation to get only positive value then multiply them with previous cell state(memory) $C_{t-1}$. It decides should the value be kept or discarded. If result from $\\sigma$ value closer to 1 the value is kept else the value is discarded.\n",
    "\n",
    "#### The input $i_{t}$gate:\n",
    "\n",
    "Together with cell gate to update the cell state this gate also decide should the value be kept or discard depend on the value of the sigmoid\n",
    "\n",
    "#### The cell $\\tilde{C_{t}}$ gate:\n",
    "Decide what value to update from the range of -1 to 1 output from tanh function the value then add with previou cell state $C_{t-1}$ value to get $C_{t}$ the new value in memeory\n",
    "\n",
    "#### The Output $o_{t}$ gate:\n",
    "\n",
    "Decide which value to output to be multiply with the cell state value that was filter with tanh before output as new hidden state for the next layer.\n",
    "\n",
    "The code for LSTM cell:\n",
    "\n",
    "```\n",
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.forget_gate = nn.Linear(ni + nh, nh)\n",
    "        self.input_gate  = nn.Linear(ni + nh, nh)\n",
    "        self.cell_gate   = nn.Linear(ni + nh, nh)\n",
    "        self.output_gate = nn.Linear(ni + nh, nh)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        h = torch.stack([h, input], dim=1)\n",
    "        forget = torch.sigmoid(self.forget_gate(h))\n",
    "        c = c * forget\n",
    "        inp = torch.sigmoid(self.input_gate(h))\n",
    "        cell = torch.tanh(self.cell_gate(h))\n",
    "        c = c + inp * cell\n",
    "        out = torch.sigmoid(self.output_gate(h))\n",
    "        h = outgate * torch.tanh(c)\n",
    "        return h, (h,c)\n",
    "```\n",
    "Refactor to take advantage of the GPU by combine all small matrix into one big one to avoid moving too frequence data in and out of GPU and allow GPU to parallelize the task.\n",
    "\n",
    "```\n",
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.ih = nn.Linear(ni,4*nh)\n",
    "        self.hh = nn.Linear(nh,4*nh)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        # One big multiplication for all the gates is better than 4 smaller ones\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) #split tensor into 4 then combine with input\n",
    "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "\n",
    "        c = (forgetgate*c) + (ingate*cellgate)\n",
    "        h = outgate * c.tanh()\n",
    "        return h, (h,c)\n",
    "\n",
    "```\n",
    "\n",
    "### Train the LSTM\n",
    "```\n",
    "class LMModel6(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()\n",
    "\n",
    "learn = Learner(dls, LMModel6(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 1e-2)\n",
    "\n",
    "```\n",
    "### Regularizing an LSTM\n",
    "Although hard to train and prone to overfitting, there some trick such as using data augmentation by translate the same text into different language then retranslate back to generate more variance. And using various regularization method to alleviate the overfitting problem some of the techniques describe here:\n",
    "\n",
    "### Dropout\n",
    "\n",
    "![dropout](./image/dropout2.png)\n",
    "Dropout is one of the regularization technique use to combat overfitting tendency of the model. The method usually apply at training time. This method is to randomly change some activations value to zero which temporary remove the neural nodes from the network. \n",
    "\n",
    "It makes the neural less relie on the input from the source that the neural regularly receive the input from since these sources may not be there. \n",
    "It makes sure all neurons actively work toward the general concept rather than try to fit specify pattern in the current data. \n",
    "\n",
    "Dropout has different behavior in training and validation mode which is why the flag training must set to True when traing and False when evaluating.\n",
    "\n",
    "```\n",
    "class Dropout(Module):\n",
    "    def __init__(self, p): self.p = p\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        mask = x.new(*x.shape).bernoulli_(1-p) # create probability of random value \n",
    "        return x * mask.div_(1-p)\n",
    "```\n",
    "### Activation Regularization (AR) and Temporal Activation Regularization (TAR)\n",
    "\n",
    "two regularization methods very similar to weight decay.\n",
    "\n",
    "#### AR \n",
    "\n",
    "This approach is apply at the final activation from LSTM to reduce its size. AR is often applied on the dropped-out activations. \n",
    "The code is \n",
    "\n",
    "`loss += alpha * activations.pow(2).mean()`\n",
    "\n",
    "#### TAR \n",
    "\n",
    "This approach is to encourage the model to output sensible value by adding a penalty to the loss to make the difference between two consecutive activations as small as possible.\n",
    "TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps)\n",
    "with activations tensor has a shape `bs x sl x n_hid` the code is:\n",
    "\n",
    "`loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()`\n",
    "\n",
    "To use these is required:\n",
    "\n",
    "1. the proper output, \n",
    "2. the activations of the LSTM pre-dropout, and \n",
    "3. the activations of the LSTM post-dropout\n",
    "\n",
    "In practive it's often used a callback `RNNRegularizer` to apply the regularization.\n",
    "\n",
    "### Training AWD-LSTM: a Weight-Tied Regularized LSTM\n",
    "Apply regularization can be combined together dropout, AR, TAR\n",
    "This method uses:\n",
    "- Embedding dropout (just after the embedding layer)\n",
    "- Input dropout (after the embedding layer)\n",
    "- Weight dropout (applied to the weights of the LSTM at each training step)\n",
    "- Hidden dropout (applied to the hidden state between two layers)\n",
    "\n",
    "```\n",
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h_o.weight = self.i_h.weight\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h)\n",
    "        out = self.drop(raw)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(out),raw,out\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()\n",
    "\n",
    "```\n",
    "Another trick is *weight-tying* [the AWD LSTM paper](https://arxiv.org/abs/1708.02182) \n",
    "by realize that input embedding is a mapping from English words to activation value. And output from hidden layer is a mapping from activations value to English words are the same thing. And assign the same weight matrix to these layers\n",
    "\n",
    "`self.h_o.weight  self.i_h.weight`\n",
    "\n",
    "The final code tweak become:\n",
    "\n",
    "```\n",
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
    "        self.h_o.weight = self.i_h.weight\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h)\n",
    "        out = self.drop(raw)\n",
    "        self.h = [h_.detach() for h_ in h]\n",
    "        return self.h_o(out),raw,out\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()\n",
    "\n",
    "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
    "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
    "                cbs=[ModelResetter,  # add callback\n",
    "                RNNRegularizer(alpha=2, beta=1)]) # add callback to learner\n",
    "\n",
    "# or use the TextLearner that will call add the callback\n",
    "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
    "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)                       \n",
    "\n",
    "learn.fit_one_cycle(15, 1e-2, wd=0.1)\n",
    "```        \n",
    "\n",
    "\n",
    "### GRU: Gate Recurrent Units architecture\n",
    "\n",
    "![gru](./image/gru1.jpg)\n",
    "\n",
    "GRU is a simplify version of LSTM and work them same way.\n",
    "\n",
    "## RNN application\n",
    "- POS\n",
    "- NER\n",
    "- Deidentification\n",
    "- Translation\n",
    "- sequence-to-sequence\n",
    "- chatbot\n",
    "- question-answer\n",
    "- sequence classification\n",
    "- sentiment\n",
    "\n",
    "\n",
    "### Jargons:\n",
    "- hidden state: The activations that are updated at each step of a recurrent neural network.\n",
    "- A neural network that is defined using a loop like this is called a recurrent neural network (RNN)\n",
    "- Back propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which \"detaches\" the history of computation steps in the hidden state every few time steps. In essence calculate the gradient and propagate backward as usual but don't store them.\n",
    "- The bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p)\n",
    "- alpha and beta are two hyperparameters to tune\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "",
    "text": "Originally posted on the fast.ai blog"
  },
  {
    "objectID": "blog/index.html#our-new-secret-weapon-for-productivity",
    "href": "blog/index.html#our-new-secret-weapon-for-productivity",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Our new secret weapon for productivity",
    "text": "Our new secret weapon for productivity\nToday we’re excited to announce that we’ve teamed up with Quarto to give nbdev superpowers. nbdev offers Python programmers a common set of tools for using Jupyter notebooks to:\n\nWrite & distribute software packages\nTest code, and\nAuthor documentation and technical articles\n\nAlthough notebooks are already widely used for once-off exploratory work, it’s less well-known that they are perfectly capable of writing quality software. In fact, we’ve used nbdev for a wide range of software projects over the last three years, including deep learning libraries, API clients, Python language extensions, terminal user interfaces, and more. We discovered that it is not only capable of writing great software but that it has also increased our productivity by 300% or more. With nbdev, developers simply write notebooks with lightweight markup and get high-quality documentation, tests, continuous integration, and packaging for free! Nbdev has allowed us to maintain and scale manyopen source projects. Pull requests are often accompanied by detailed documentation and tests–contributors simply write notebooks.\nThis is why we’re excited to share nbdev v2. It’s rewritten from the ground up, with much-anticipated features including:\n\nInteroperation with non-nbdev codebases for tasks like documentation\nSupport for any static site generator\nWide variety of output mediums such as blogs, papers, slides, and websites\nA faster Jupyter kernel, which also means faster tests\nCleaner and more extensible API, which supports custom directives, custom module exporters, and more"
  },
  {
    "objectID": "blog/index.html#nbdev-in-industry",
    "href": "blog/index.html#nbdev-in-industry",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "nbdev in industry",
    "text": "nbdev in industry\nWe have piloted nbdev at several companies. We were delighted to receive the following feedback, which fits our own experience using and developing nbdev:\n\n\n\nDavid Berg, on using nbdev for internal documentation at Netflix: “Prior to using nbdev, documentation was the most cumbersome aspect of our software development process… Using nbdev allows us to spend more time creating rich prose around the many code snippets guaranteeing the whole experience is robust. nbdev has turned what was once a chore into a natural extension of the notebook-based testing we were already doing.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nErik Gaasedelen, on using nbdev in production at Lyft: “I use this in production at my company. It’s an awesome tool… nbdev streamlines everything so I can write docs, tests, and code all in one place… The packaging is also really well thought out. From my point of view it is close to a Pareto improvement over traditional Python library development.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nHugo Bowne-Anderson, on using nbdev for Outerbounds: “nbdev has transformed the way we write documentation. Gone are the days of worrying about broken code examples when our API changes or [due to] human errors associated with copying & pasting code into markdown files. The authoring experience of nbdev… [allows] us to write prose and live code in a unified interface, which allows more experimentation… On top of this, nbdev allows us to include unit tests in our documentation which mitigates the burden of maintaining the docs over time.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoxanna Pourzand, on using nbdev for Transform: “We’re so excited about using nbdev. Our product is technical so our resulting documentation includes a lot of code-based examples. Before nbdev, we had no way of maintaining our code examples and ensuring that it was up-to-date for both command inputs and outputs. It was all manual. With nbdev, we now have this under control in a sustainable way. Since we’ve deployed these docs, we also had a situation where we were able to identify a bug in one of our interfaces, which we found by seeing the error that was output in the documentation.”"
  },
  {
    "objectID": "blog/index.html#whats-nbdev",
    "href": "blog/index.html#whats-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "What’s nbdev?",
    "text": "What’s nbdev?\nNbdev embraces the dynamic nature of python and REPL-driven development in ways that traditional IDEs and software development workflows cannot. We thoroughly discussed the motivation, history, and goals of nbdev in this initial launch post three years ago. The creator of Jupyter, Fernando Pérez, told us:\n\n[Nbdev] should be celebrated and used a lot more - I have kept a tab with your original nbdev blog post open for months in Chrome because of how often I refer to it and point others to this work\n\nIn short, nbdev embraces ideas from literate programming and exploratory programming. These paradigms have been revisited in platforms like XCode Playgrounds and languages like Smalltalk, LISP, and Mathematica. With nbdev, we sought to push these paradigms even further by enabling it for one of the most popular dynamic programming languages in the world: Python.\n\n\n\nState of the Octoverse 2021, GitHub\n\n\nEven though nbdev is most widely used in scientific computing communities due to its integration with Jupyter Notebooks, we’ve found that nbdev is well suited for a much wider range of software. We have used nbdev to write deep learning libraries, API clients, python language extensions,terminal user interfaces, and more!\nHamel: When I use nbdev, my colleagues are often astounded by how quickly I can create and distribute high-quality python packages. I consider nbdev to be a superpower that allows me to create tests and documentation without any additional friction, which makes all of my projects more maintainable. I also find writing software with nbdev to be more fun and productive as I can iterate very fast on ideas relative to more traditional software engineering workflows. Lastly, with nbdev I can also use traditional text-based IDEs if I want to, so I get the best of both worlds."
  },
  {
    "objectID": "blog/index.html#what-we-learned-after-three-years-of-using-nbdev",
    "href": "blog/index.html#what-we-learned-after-three-years-of-using-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "What we learned after three years of using nbdev",
    "text": "What we learned after three years of using nbdev\nWhile nbdev was originally developed to simplify the software development workflow for various fast.ai projects, we found that users wanted to extend nbdev to:\n\nWrite and publish blog posts, books, papers, and other types of documents with Jupyter Notebooks\nDocument existing codebases not written in nbdev\nAccommodate traditional Python conventions–for those constrained in how their code is organized and formatted\nPublish content using any static site generator\n\nWhile we created projects such as fastpages and fastdoc to accomplish some of these tasks, we realized that it would be better to have a single set of flexible tools to accomplish all of them. To this end, we were extremely excited to discover Quarto, an open-source technical publishing system built on pandoc.\nHamel: The more I used nbdev for creating Python modules, the more I wanted to use it for writing blogs and documenting existing codebases. The ability to customize the way notebooks are rendered (hiding vs. showing cells, stripping output, etc.), along with the facilities for including unit tests, made it my go-to authoring tool for all technical content. I’m excited that nbdev2 unlocks all of these possibilities for everyone!"
  },
  {
    "objectID": "blog/index.html#enter-quarto-a-pandoc-super-processor",
    "href": "blog/index.html#enter-quarto-a-pandoc-super-processor",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Enter Quarto: A pandoc super-processor",
    "text": "Enter Quarto: A pandoc super-processor\nQuarto is a project that enables technical publishing with support for Jupyter Notebook, VSCode, Observable, and plaintext editors. Furthermore, Quarto enables the publishing of high-quality articles, reports, websites, and blogs in HTML, PDF, ePub, PowerPoint slides, and more. Quarto is maintained by RStudio, a company with a long history of products supporting literate programming, such as RMarkdown and RStudio.\nQuarto is built on top of Pandoc, a universal document converter that supports nearly any format you can think of. Pandoc achieves this seemingly magical feat by representing documents in a common abstract syntax tree (AST) that serves as the medium through which different formats can be translated. By extension, Quarto allows you to generate content in almost any format you wish! You can use pandoc filters to modify the AST and the output format, which allows you to use any static site generator you want, and programmatically modify and generate content.\nQuarto allows you to compose pandoc filters in a processing pipeline and apply them to specific documents or entire projects. You can also distribute filters as Quarto extensions, which makes Quarto extremely customizable.\nWe also find Quarto compelling because user interfaces such as comment directives (comments that start with #|) correlate with nbdev. In fact, we even learned that nbdev inspired Quarto in this regard! In general, Quarto and nbdev share many goals, and the Quarto team has been incredibly responsive to our suggestions. For example, the ability to create notebook filters to modify notebooks before rendering. Below is a screenshot of a Jupyter notebook rendered with Quarto and nbdev.\n\n\n\n\nQuarto rendering a Jupyter notebook written with nbdev\n\n\n\nFinally, Quarto supports more programming languages than just Python and has been adding new features and fixing bugs at an impressive speed. This gives us confidence that we will be able to expand nbdev to support more use cases in the future. We discuss some of these future directions in the closing section."
  },
  {
    "objectID": "blog/index.html#a-blazing-fast-notebook-kernel-execnb",
    "href": "blog/index.html#a-blazing-fast-notebook-kernel-execnb",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "A blazing fast notebook kernel: execnb",
    "text": "A blazing fast notebook kernel: execnb\nA core component of nbdev is executing and testing notebooks programmatically. It is important that this notebook runner executes with minimal overhead to maintain our goal of providing a delightful developer experience. This is why we built execnb, a lightweight notebook runner for Python kernels, which executes notebooks blazingly fast. Furthermore, execnb allows parameterized execution of notebooks.\nHamel: I have been an enthusiastic user of tools like papermill that programmatically run notebooks for use-cases like creating dashboards or enabling new kinds of machine learning workflows. I believe execnb unlocks even more possibilities with its ability to inject arbitrary code at any place in a notebook, as well as the ability to pass callbacks that run before and/or after cells are executed. This opens up possibilities to create new types of workflows with notebooks that I am excited about exploring in the near future."
  },
  {
    "objectID": "blog/index.html#towards-a-dialect-of-python-that-embraces-its-dynamic-nature",
    "href": "blog/index.html#towards-a-dialect-of-python-that-embraces-its-dynamic-nature",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Towards a dialect of python that embraces its dynamic nature",
    "text": "Towards a dialect of python that embraces its dynamic nature\nOne way to understand nbdev is part of an ecosystem that is designed to embrace Python’s dynamic properties for REPL-driven software engineering. Similar to Clojure, our goal is to provide tools that remove all friction from using the REPL in your programming workflow. We believe that the REPL enhances developer workflows thanks to context-sensitive auto-completion, signature inspection, and documentation–all based on the actual state of your code, and none of which are available in IDEs that depend solely on static analysis. We have found that for this reason, nbdev, with its Jupyter notebook foundation, makes programming significantly more productive and enjoyable.\nOur efforts to support REPL-driven development and literate programming are not limited to nbdev. We maintain a number of libraries that extend python to bolster this programming experience. The most notable of these libraries is fastcore, which extends Python in terms of testing, documenting code, metaprogramming, attribute helpers, enhanced representations of objects, and notebook-friendly patching. This blog post offers a gentle introduction to fastcore. In addition to literate programming, fastcore encourages conventions such as brevity and efficient use of vertical space so you can accomplish more with significantly less code. For example, below is a simple decorator that enables notebook-friendly patching:\n\n\n\n@patch decorator from fastcore\n\n\nWe believe that this combination of a new developer workflow (nbdev), Python extensions (fastcore), and associated norms form a new dialect of Python that is centered on leveraging its dynamic nature–in contrast to an ever-growing trend toward static analysis. We suspect that this dialect of Python will be more productive for programmers in many scenarios. We are framing this ecosystem as a “dialect” as it is still very much Python and is approachable by anyone who is familiar with the language. Furthermore, despite nbdev’s notebook workflow, our tools generate plaintext modules that can be navigated and edited with text-based IDEs, allowing programmers to experience the best of both worlds, if they desire.\nHamel: I believe this framing of a Python dialect is key to properly understanding what nbdev is. While it may be tempting to get stuck on specific features or technical details of nbdev, it is useful to zoom out to understand the overall intent of creating a better workflow rather than conforming too rigidly to existing ones. A good analogy is TypeScript’s relationship with JavaScript: it is an extension of an existing programming language that supports a new way of programming. I encourage you to treat nbdev in a similar fashion: be willing to try new ways of programming and observe which tradeoffs resonate with you. At the very least, I believe nbdev is a fun way to experience a different way of writing software, which will broaden your horizons about programming in general, all without having to learn an entirely new programming language!"
  },
  {
    "objectID": "blog/index.html#the-future-of-nbdev",
    "href": "blog/index.html#the-future-of-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "The future of nbdev",
    "text": "The future of nbdev\nWhile we are excited about nbdev2, we believe we have only scratched the surface of what’s possible. We are considering the following features:\n\nSupporting more languages beyond Python, such as Julia, R and JavaScript\nOffering interfaces for executing parameterized notebooks that mimic Python scripts\nExtensions for more static site generators and filters\nSupporting alternate testing backends, such as pytest\nSupporting a greater number of docstring formats, such as Google-style docstrings\nMore options to use plain-text or human readable notebook backends other than JSON\n\nIf you have interesting ideas about how nbdev can be extended, please drop and chat with us on discord or post a message in the forums."
  },
  {
    "objectID": "blog/index.html#how-you-can-get-started-with-nbdev",
    "href": "blog/index.html#how-you-can-get-started-with-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "How you can get started with nbdev",
    "text": "How you can get started with nbdev\nOur project’s website is at nbdev.fast.ai, where we will be posting tutorials, examples, and more documentation in the coming days."
  },
  {
    "objectID": "blog/index.html#thank-you",
    "href": "blog/index.html#thank-you",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Thank You",
    "text": "Thank You\nThis new version of nbdev was a team effort by many wonderful people. We want to highlight two people who have made outstanding contributions:\n\nWasim Lorgat was instrumental across different areas, including significant contributions to fastcore, execnb, and nbdev, as well as the implementation of the new nbdev home page. With Wasim’s help, we were able to push nbdev to a new level of functionality and quality.\nJJ Allaire is not only the CEO of RStudio but also the steward of Quarto. JJ was incredibly responsive and eager to work with us on nbdev and added many features to Quarto specifically with nbdev in mind, such as notebook filters. We were also astounded by the attention to detail and the pace at which bugs are addressed. This new version of nbdev would not have been possible without JJ’s help, and we are excited to continue to work with him.\n\nWe also want to thank the amazing fastai community, notably Isaac Flath, Benjamin Warner and Zach Mueller for their tireless work on this project."
  },
  {
    "objectID": "blog/index.html#a-conversation-with-jj-allaire",
    "href": "blog/index.html#a-conversation-with-jj-allaire",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "A conversation with JJ Allaire",
    "text": "A conversation with JJ Allaire\nTo celebrate the launch of nbdev v2 and Quarto, Jeremy sat down with the CEO of Posit (previously known as RStudio, the company behind Quarto), JJ Allaire, to talk about software development, scientific publishing, R, Python, literate programming, and much more."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Vanh Phomsavanh",
    "section": "",
    "text": "I’ve been working as software engineer for two decades."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Vanh Phomsavanh",
    "section": "Education",
    "text": "Education\nSan Jose state University Computer Engineering 1989"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Vanh Phomsavanh",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine intelligence",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "MachineIntell",
    "section": "",
    "text": "import nbdev"
  },
  {
    "objectID": "tutorial/tutoring.html#linear-algebra-for-maching-learning",
    "href": "tutorial/tutoring.html#linear-algebra-for-maching-learning",
    "title": "Tutorials",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/tutoring.html#differential-equation-in-machine-learning",
    "href": "tutorial/tutoring.html#differential-equation-in-machine-learning",
    "title": "Tutorials",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "title": "Linear Algebra",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nTo ask NN to analyze whetherthe picture is a dog or a cat these pictures must be digitized and feed to the NN. Machine Learning deal with large amount of data for example a picture of a dog taken with smart phone consist of thousand up to many million pixels. To feed this image to NN for analysis is must be transform into array of data that NN can use.\nIn Math the branch that work with array of data is called Linear Algebra and the field is most appropriate is matrices.\nArray of data may have multidimension. Name of array with different dimension:\n\nVector an array with one dimension or rank\nMatrix an array with two dimension or rank\nTensor an array with three or more dimension or rank\n\nOne of the most frequent uses operation in NN is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(C = A \\times B\\)\n\n\n\nFramework Provide tools\nAI framework such as Pytorch, TensorFlow, Flux have many of the math tools build in. Also many programming languages e.g. Python, Julia, R have support for Matrix operations build in to the language."
  },
  {
    "objectID": "tutorial/math/linearalgebra/lna1.html#differential-equation-in-machine-learning",
    "href": "tutorial/math/linearalgebra/lna1.html#differential-equation-in-machine-learning",
    "title": "Linear Algebra",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]\n\n\nSoftmax (softargmax)\nA function that take the vector of input value and convert them to output value as probability between 0 and 1. The input value may be zero, negative, positive.\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar.\nFirst, we will need to compute the softmax of our activations. This is defined by:\n\\(\\color{chocolate}{ \\sigma : \\R^K\\to (0,1)^K}\\) is defined when \\(\\color{chocolate}{ K \\ge 1 }\\) by the formula\n\\[\\color{chocolate}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in\\R^K  \\]\nor more concisely:\n\\[\\color{chocolate}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] input vector \\(\\color{chocolate}{x}\\) and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector \\(\\color{chocolate}{ \\sigma(x)}\\) is \\(\\color{chocolate}{1}\\).\n\n\nArgmax function\nThe argmax function convert all the value in the input vector to zero except the max value in the vector. Which it’s convert to one with the result of vector of all zero except one value that is one.\nThe arg max function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg): \\[\\color{chocolate}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{chocolate}{0}\\) or \\(\\color{chocolate}{1}\\).\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))\nNote that the formula\n\\[\\color{chocolate}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{chocolate}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{chocolate}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{chocolate}{ a }\\) is the maximum of the \\(\\color{chocolate}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{chocolate}{ x}\\) and some prediction \\(\\color{chocolate}{ p(x)}\\) is given by:\n\\[ \\color{chocolate}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(x\\)s are 1-hot encoded, this can be rewritten as \\(\\color{chocolate}{ -\\log(p_{i})}\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than 1 (probability) is result in negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1"
  },
  {
    "objectID": "tutorial/math/diffEq/diffeq1.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/diffEq/diffeq1.html#linear-algebra-for-maching-learning",
    "title": "Differential Equation",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/math/diffEq/diffeq1.html#differential-equation-in-machine-learning",
    "href": "tutorial/math/diffEq/diffeq1.html#differential-equation-in-machine-learning",
    "title": "Differential Equation",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "title": "Mathematic",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/math/maths.html#differential-equation-in-machine-learning",
    "href": "tutorial/math/maths.html#differential-equation-in-machine-learning",
    "title": "Mathematic",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "href": "tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "title": "NLP",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "href": "tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "title": "NLP",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/nlps/rnn.html",
    "href": "tutorial/nlps/rnn.html",
    "title": "Recurrent Neural Network (RNN)",
    "section": "",
    "text": "The Markov model weakness is that it’s limit to context window(scope) that was choosen. Using the info that was stored to predict prior data then feed that info to model while it was considering current tag."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#bidirectional-rnn",
    "href": "tutorial/nlps/rnn.html#bidirectional-rnn",
    "title": "Recurrent Neural Network (RNN)",
    "section": "Bidirectional RNN",
    "text": "Bidirectional RNN\nA technique to train two independent RNN where one process from start to end the other process from end to start then combine the output from both into single one"
  },
  {
    "objectID": "tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "href": "tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "title": "Recurrent Neural Network (RNN)",
    "section": "Long short term memory (LSTM)",
    "text": "Long short term memory (LSTM)\n\nAn RNN that has the capability to forget the info that is not relevant to the current task.\n\n\nLSTM has\n\nforget gate to delete info of non relevant from current context\nadd gate to select new info into current context with tanh activation that indicate the direction of info(should care about) and a sigmoid to indicate the scaling(how much should be care about) factor of the info to be add to forget gate to produce state context\nout gate with sigmoid combine with state context to output result"
  },
  {
    "objectID": "tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "href": "tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "title": "Recurrent Neural Network (RNN)",
    "section": "Creating a Language Model from Scratch",
    "text": "Creating a Language Model from Scratch\nA language model is a model that predict the next word in the sentence.\n\nFirst Language Model\nBuild a model to predict each word based on the previous three words by create a list of every sequence of three words as independent variables, and the next word after each sequence as the dependent variable.\nThe model takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. It use the standard three linear layers, but with two tweaks.\n\nThe first linear layer will use only the first word’s embedding as activations,\n\nThe second layer will use the second word’s embedding plus the first layer’s output activations, and\nThe third layer will use the third word’s embedding plus the second layer’s output activations.\nThe key effect of this is that every word is interpreted in the information context of any words preceding it.\n\nEach of these three layers will use the same weight matrix\n\nThe way that one word impacts the activations from previous words should not change depending on the position of a word.\nActivation values will change as data moves through the layers, but the layer’s weights themselves will not change from layer to layer.\nSo, a layer does not learn one sequence position; it must learn to handle all positions."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "href": "tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "title": "Recurrent Neural Network (RNN)",
    "section": "The architect of the first model",
    "text": "The architect of the first model\n\n\n\nsimple linear LM\n\n\nHere the figure the model. Where word is the input, FC is fully connected layer and triangular is output prediction\n\n3 layers model code\nThe first cut of the code for 3 layers model use:\n\nThe embedding layer (input2_hidden, for input to hidden)\nThe linear layer to create the activations for the next word (hidden2_hidden, for hidden to hidden)\nA final linear layer to predict the fourth word (hidden2_output, for hidden to output)\n\nThey all use the same embedding since they come from same data\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n  # input2_hidden is embedding layer\n  # hidden2_hidden is linear layer\n  # hidden2_output is linear layer   \n    def forward(self, x):\n        h = F.relu(self.hidden2_hidden(self.input2_hidden(x[:,0])))\n        h = h + self.input2_hidden(x[:,1])\n        h = F.relu(self.hidden2_hidden(h))\n        h = h + self.input2_hidden(x[:,2])\n        h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\n\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n# tensor of numericalized value for model\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\nbs = 64\ncut = int(len(seqs) * 0.8)\n# create batch\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)   \ncompare to the simplest model which always predict the next word which is ‘thousand’ to see how it performs:\n# a simplest model that always predict 'thousand' on each input sentence\nc = Counter(tokens[cut:])\nmc = c.most_common(5)\nmc\nmc[0][1] / len(tokens[cut:])\n\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n#index of the most common words ('thousand')\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n\nRefactor to use loop: the RNN\n\n\n\nrefactor2RNN\n\n\nRewrite the code to use loop this is look closer to RNN\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n    # refactor to use for loop the RNN!\n    def forward(self, x):\n        h = 0.               # using broascast\n        for i in range(3):\n            h = h + self.input2_hidden(x[:,i])\n            h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\n\nRefactor to add memory to RNN\nAdd the ability to retain previous word instead of start up new every time \nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0.  # using broascast\n    # refactor to use for loop the RNN!\n    def forward(self, x):\n        for i in range(3):\n            self.h = self.h + self.input2_hidden(x[:,i])\n            self.h = F.relu(self.hidden2_hidden(h))\n        out = self.hidden2_output(self.h)\n        self.h = self.h.detach() # do bptt\n        return out\n    def reset(self): self.h = 0.\n\n\n\nBPTT Backpropagation through time\nThis model hidden state now can remember previous activation from previous batch and the gradient will be calculate on sequence length token of the past not instead of recalculate with each new stream this is call BPTT\n# rearrange data so model see in particular sequence\nm = len(seqs)//bs\nm,bs,len(seqs)\n# reindex model see as contiguous batch with each epoch\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, drop_last=True, # drop last batch that have diff shape\n    shuffle=False) # maintain sequence\nlearn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, \n                cbs=ModelResetter) # callback to reset each epoch\nlearn.fit_one_cycle(10, 3e-3)\nAdd More signal: keep the output\nThe model no longer throw away the output from previous run but add them as input to current run\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n# check if it still offset by 1\n[L(vocab[o] for o in s) for s in seqs[0]]\nRewrite the model to now output every word instead of every 3 words in order to feed this into next run\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n    def reset(self): self.h = 0\n    def loss_func(inp, targ):\n        return F.cross_entropy(inp.view(-1, len(vocab)), # flatten out to match bs x sl x vocab_sz from model\n        targ.view(-1)) # flatten out to match bs x sl x vocab_sz from model\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)       \n\n\nRecurrent Neural Network: RNN\n RNN feed the output activation value back into hidden layer to help the hidden layers retain info about previous run therefore have some memory of the past.\n\n\nMulti-Layer RNN\nsince the current model use the same weight matrix for each hidden layer which mean there no new info to be learn from. One way to To improve the model further is to stack more layers by feed the output from one layer into the next layer so on.\nLook at it in unrolling way \nRefactoring to use PyTorch\nThe model now has too deep layers this could lead to problem of vanishing or exploding gradient\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n    def reset(self): self.h.zero_()\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\nExploding or Disappearing Activations\nThe problem stem from the gradient value calcutated from the output layer won’t propagated back to the earlier layer. This is because the network is too deep.\nVanishing Gradient:\nAs the gradient value travel backward the especially the small value is diminishing as the floating point value get computed and recomputed many time each time it get round off closer and closer to 0 and finally become 0.\nExploding Gradient:\nThis the opposite of vanishing gradient. This phenomena happen espcially when the large value get larger with each computation it get exponentially large until it get large closer to infinity and become useless. \n\n\nThe floating point problem\nOne problem with doing floating point math with computer is that computer has limit amount of storage and floating point number is imprecise e.g. 1.0000012 or 1.00000000000000000000000012 is these two numbers are the same value? The answer is it’s depend on what do you want to do with it. On some task the first one is good enough but on other the second is what require. To store the second number require more storage.\nThe impreciseness is rather harder to understand. Float number get denser and denser when value get closer and closer to 0. In practice how do you represent this to the computer so it do some useful work. The IEEE organization come up with a standard which require how much storage should be devoted to certain type of floating point value e.g. 8-bit folating point,16-bit, 32-bit which mean the storage depend on the precision the user requirement not the actual value it may case the value get round-off read more from here. This mean do more calculation may lead less precision. This is what happen when doing lot and lot of gradient calculations the value may end up at 0 which is call vanishing gradient or the value get larger and larger exponentially or explode to infinite.\nThese problems are the main reason why RNN model is hard to train than CNN model,however research is very active to try new way to reduce or avoid these problems.\n\n\nNeural Network that have Memory\nIn Machine Learning two approach to handle Explode and Vanishing gradient is to use Neural network that can retain previous activation and also can figure out what value should be retain and which should be removed. The two popular approch are LSTM and GRU. GRU is a simplify version of LSTM\n\n\n\nlstm-gru\n\n\n\n\nComparison LSTM and GRU\n\n\n\ngru\n\n\n\n\nLSTM architecture\nInternally LSTM comprise with little neural net that decise how much gradient is needed and which one to update or delete. \nsigmoid equation:\n\\(f(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1} = \\frac12 + \\frac12 \\tanh\\left(\\frac{x}{2}\\right)\\)\nSigmoid only let positive value between 0 and 1 pass through\ntanh equation:\n\\(f(x) = \\tanh x = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\)\nTanh only let value between -1 and 1 pass through\n\n\nAnother look at LSTM internal:\n\n\n\nlstm-gru\n\n\nThe little NN is compose of gates call forget gate,input gate, cell gate, output gate. These gates work together to provide LSTM the capability to remember activation value that is important and forget the unneccessary activation value\n\nThe forget \\(f_{t}\\) gate:\nTake input \\(x\\), hidden state \\(h_{t-1}\\) then gated them via the sigmoid \\(\\sigma\\) activation to get only positive value then multiply them with previous cell state(memory) \\(C_{t-1}\\). It decides should the value be kept or discarded. If result from \\(\\sigma\\) value closer to 1 the value is kept else the value is discarded.\n\n\nThe input \\(i_{t}\\)gate:\nTogether with cell gate to update the cell state this gate also decide should the value be kept or discard depend on the value of the sigmoid\n\n\nThe cell \\(\\tilde{C_{t}}\\) gate:\nDecide what value to update from the range of -1 to 1 output from tanh function the value then add with previou cell state \\(C_{t-1}\\) value to get \\(C_{t}\\) the new value in memeory\n\n\nThe Output \\(o_{t}\\) gate:\nDecide which value to output to be multiply with the cell state value that was filter with tanh before output as new hidden state for the next layer.\nThe code for LSTM cell:\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n    def forward(self, input, state):\n        h,c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h,c)\nRefactor to take advantage of the GPU by combine all small matrix into one big one to avoid moving too frequence data in and out of GPU and allow GPU to parallelize the task.\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) #split tensor into 4 then combine with input\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n\n\n\n\nTrain the LSTM\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    def reset(self): \n        for h in self.h: h.zero_()\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)\n\n\n\nRegularizing an LSTM\nAlthough hard to train and prone to overfitting, there some trick such as using data augmentation by translate the same text into different language then retranslate back to generate more variance. And using various regularization method to alleviate the overfitting problem some of the techniques describe here:\n\n\nDropout\n Dropout is one of the regularization technique use to combat overfitting tendency of the model. The method usually apply at training time. This method is to randomly change some activations value to zero which temporary remove the neural nodes from the network.\nIt makes the neural less relie on the input from the source that the neural regularly receive the input from since these sources may not be there. It makes sure all neurons actively work toward the general concept rather than try to fit specify pattern in the current data.\nDropout has different behavior in training and validation mode which is why the flag training must set to True when traing and False when evaluating.\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p) # create probability of random value \n        return x * mask.div_(1-p)\n\n\nActivation Regularization (AR) and Temporal Activation Regularization (TAR)\ntwo regularization methods very similar to weight decay.\n\nAR\nThis approach is apply at the final activation from LSTM to reduce its size. AR is often applied on the dropped-out activations. The code is\nloss += alpha * activations.pow(2).mean()\n\n\nTAR\nThis approach is to encourage the model to output sensible value by adding a penalty to the loss to make the difference between two consecutive activations as small as possible. TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps) with activations tensor has a shape bs x sl x n_hid the code is:\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\nTo use these is required:\n\nthe proper output,\nthe activations of the LSTM pre-dropout, and\nthe activations of the LSTM post-dropout\n\nIn practive it’s often used a callback RNNRegularizer to apply the regularization.\n\n\n\nTraining AWD-LSTM: a Weight-Tied Regularized LSTM\nApply regularization can be combined together dropout, AR, TAR This method uses: - Embedding dropout (just after the embedding layer) - Input dropout (after the embedding layer) - Weight dropout (applied to the weights of the LSTM at each training step) - Hidden dropout (applied to the hidden state between two layers)\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    def reset(self): \n        for h in self.h: h.zero_()\n\nAnother trick is weight-tying the AWD LSTM paper by realize that input embedding is a mapping from English words to activation value. And output from hidden layer is a mapping from activations value to English words are the same thing. And assign the same weight matrix to these layers\nself.h_o.weight  self.i_h.weight\nThe final code tweak become:\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    def reset(self): \n        for h in self.h: h.zero_()\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter,  # add callback\n                RNNRegularizer(alpha=2, beta=1)]) # add callback to learner\n# or use the TextLearner that will call add the callback\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)                       \nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n\n\nGRU: Gate Recurrent Units architecture\n\n\n\ngru\n\n\nGRU is a simplify version of LSTM and work them same way."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#rnn-application",
    "href": "tutorial/nlps/rnn.html#rnn-application",
    "title": "Recurrent Neural Network (RNN)",
    "section": "RNN application",
    "text": "RNN application\n\nPOS\nNER\nDeidentification\nTranslation\nsequence-to-sequence\nchatbot\nquestion-answer\nsequence classification\nsentiment\n\n\nJargons:\n\nhidden state: The activations that are updated at each step of a recurrent neural network.\nA neural network that is defined using a loop like this is called a recurrent neural network (RNN)\nBack propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which “detaches” the history of computation steps in the hidden state every few time steps. In essence calculate the gradient and propagate backward as usual but don’t store them.\nThe bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p)\nalpha and beta are two hyperparameters to tune"
  },
  {
    "objectID": "tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "href": "tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "title": "Machine Vision",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "href": "tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "title": "Machine Vision",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tip/tips.html",
    "href": "tip/tips.html",
    "title": "matplotlib demo",
    "section": "",
    "text": "Originally posted on the fast.ai blog"
  },
  {
    "objectID": "tip/tips.html#our-new-secret-weapon-for-productivity",
    "href": "tip/tips.html#our-new-secret-weapon-for-productivity",
    "title": "Tips and Tricks",
    "section": "Our new secret weapon for productivity",
    "text": "Our new secret weapon for productivity\n\ntitle: “matplotlib demo” format: html: code-fold: true jupyter: python3\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "news/newsall.html",
    "href": "news/newsall.html",
    "title": "Latest News",
    "section": "",
    "text": "Stable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "news/newsall.html#deep-learning-from-fastai-library",
    "href": "news/newsall.html#deep-learning-from-fastai-library",
    "title": "Latest News",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nnbdev version was release"
  },
  {
    "objectID": "news/news1.html#natural-language-processing",
    "href": "news/news1.html#natural-language-processing",
    "title": "News 1",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "news/news2.html",
    "href": "news/news2.html",
    "title": "News 2",
    "section": "",
    "text": "Using fastai library is the fastest way to get your Machine Learning working with less head ache"
  },
  {
    "objectID": "news/news2.html#natural-language-processing",
    "href": "news/news2.html#natural-language-processing",
    "title": "News 2",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "help/helps.html",
    "href": "help/helps.html",
    "title": "Helping Hand",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "resources/resource.html",
    "href": "resources/resource.html",
    "title": "Resources",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "MachineIntell",
    "section": "",
    "text": "The main module for machine learning and deep learning library\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef foo(): pass\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport nbdev; nbdev.nbdev_export()\n:::"
  },
  {
    "objectID": "tutorial/math/nnMath/nnmath.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/nnMath/nnmath.html#linear-algebra-for-maching-learning",
    "title": "Neural Network Math",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(C = A \\times B\\)\n\n\n\nVector\nMatrix\n\n\n\nSoftmax (softargmax)\nSoftmax is one of the useful function in Neural Network computation. It’s allowed the data that output from the Neural Network which may not relate to one another be grouped into a single group and relate to each other as a posibility.\nIn mathematically term it’s a function that take the vector as input value and convert them to vector of output value and organized them as a probability value that sum to 1. The input value may be zero, negative, positive.\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\nSoftmax equation is defined by:\n\\(\\color{chocolate}{ \\sigma : \\R^K\\to (0,1)^K}\\) is defined when \\(\\color{chocolate}{ K \\ge 1 }\\) by the formula\n\\[\\color{chocolate}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in\\R^K  \\]\nor more concisely:\n\\[\\color{chocolate}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] The input vector \\(\\color{chocolate}{x}\\) values are normalized by dividing each value in the vector by the sum of all values; this normalization ensures that the sum of all the components of the output vector \\(\\color{chocolate}{ \\sigma(x)}\\) is \\(\\color{chocolate}{= 1}\\).\n\nSoftmax and Sigmoid\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar. \\[\\color{chocolate}{\\hbox{sigmoid S(x)} = \\frac{1}{1 + e^{-x}} }\\] Sigmoid can have two possibility that must sum to \\(\\color{chocolate}{1}\\). When softmax has only two possibility then it is equal to sigmoid function.\n\n\nArgmax function\nThe argmax function convert all the value in the input vector to zero except the max value in the vector. Which it’s convert to one with the result of vector of all zero except one value that is one.\nThe arg max function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg): \\[\\color{chocolate}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{chocolate}{0}\\) or \\(\\color{chocolate}{1}\\).\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))\nNote that the formula\n\\[\\color{chocolate}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{chocolate}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{chocolate}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{chocolate}{ a }\\) is the maximum of the \\(\\color{chocolate}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{chocolate}{ x}\\) and some prediction \\(\\color{chocolate}{ p(x)}\\) is given by:\n\\[ \\color{chocolate}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(x\\)s are 1-hot encoded, this can be rewritten as \\(\\color{chocolate}{ -\\log(p_{i})}\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than 1 (probability) is result in negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1"
  },
  {
    "objectID": "tutorial/math/nnMath/nnmath.html#maching-learning",
    "href": "tutorial/math/nnMath/nnmath.html#maching-learning",
    "title": "Neural Network Math",
    "section": "Maching Learning",
    "text": "Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(C = A \\times B\\)\n\n\n\nVector\nMatrix\n\n\n\nSoftmax (softargmax)\nSoftmax is one of the useful function in Neural Network computation. It’s allowed the data that output from the Neural Network which may not relate to one another be grouped into a single group and relate to each other as a posibility.\nIn mathematically term it’s a function that take the vector as input value and convert them to vector of output value and organized them as a probability value that sum to 1. The input value may be zero, negative, positive.\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\nSoftmax equation is defined by:\n\\(\\color{chocolate}{ \\sigma : \\R^K\\to (0,1)^K}\\) is defined when \\(\\color{chocolate}{ K \\ge 1 }\\) by the formula\n\\[\\color{chocolate}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in\\R^K  \\]\nor more concisely:\n\\[\\color{chocolate}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] The input vector \\(\\color{chocolate}{x}\\) values are normalized by dividing each value in the vector by the sum of all values; this normalization ensures that the sum of all the components of the output vector \\(\\color{chocolate}{ \\sigma(x)}\\) is \\(\\color{chocolate}{= 1}\\).\n\nSoftmax and Sigmoid\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar. \\[\\color{chocolate}{\\hbox{sigmoid S(x)} = \\frac{1}{1 + e^{-x}} }\\] Sigmoid can have two possibility that must sum to \\(\\color{chocolate}{1}\\). When softmax has only two possibility then it is equal to sigmoid function.\n\n\nArgmax function\nThe argmax function convert all the value in the input vector to zero except the max value in the vector. Which it’s convert to one with the result of vector of all zero except one value that is one.\nThe arg max function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg): \\[\\color{chocolate}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{chocolate}{0}\\) or \\(\\color{chocolate}{1}\\).\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))\nNote that the formula\n\\[\\color{chocolate}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{chocolate}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{chocolate}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{chocolate}{ a }\\) is the maximum of the \\(\\color{chocolate}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{chocolate}{ x}\\) and some prediction \\(\\color{chocolate}{ p(x)}\\) is given by:\n\\[ \\color{chocolate}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(x\\)s are 1-hot encoded, this can be rewritten as \\(\\color{chocolate}{ -\\log(p_{i})}\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than 1 (probability) is result in negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1"
  },
  {
    "objectID": "tutorial/math/nnMath/nnmath.html#mathmaching-learning",
    "href": "tutorial/math/nnMath/nnmath.html#mathmaching-learning",
    "title": "Neural Network Math",
    "section": "MathMaching Learning",
    "text": "MathMaching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(C = A \\times B\\)\n\n\n\nVector\nMatrix\n\n\n\nSoftmax (softargmax)\nSoftmax is one of the useful function in Neural Network computation. It’s allowed the data that output from the Neural Network which may not relate to one another be grouped into a single group and relate to each other as a posibility.\nIn mathematically term it’s a function that take the vector as input value and convert them to vector of output value and organized them as a probability value that sum to 1. The input value may be zero, negative, positive.\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\nSoftmax equation is defined by:\n\\(\\color{chocolate}{ \\sigma : \\R^K\\to (0,1)^K}\\) is defined when \\(\\color{chocolate}{ K \\ge 1 }\\) by the formula\n\\[\\color{chocolate}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in\\R^K  \\]\nor more concisely:\n\\[\\color{chocolate}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] The input vector \\(\\color{chocolate}{x}\\) values are normalized by dividing each value in the vector by the sum of all values; this normalization ensures that the sum of all the components of the output vector \\(\\color{chocolate}{ \\sigma(x)}\\) is \\(\\color{chocolate}{= 1}\\).\n\nSoftmax and Sigmoid\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar. \\[\\color{chocolate}{\\hbox{sigmoid S(x)} = \\frac{1}{1 + e^{-x}} }\\] Sigmoid can have two possibility that must sum to \\(\\color{chocolate}{1}\\). When softmax has only two possibility then it is equal to sigmoid function.\n\n\nArgmax function\nThe argmax function convert all the value in the input vector to zero except the max value in the vector. Which it’s convert to one with the result of vector of all zero except one value that is one.\nThe arg max function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg): \\[\\color{chocolate}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{chocolate}{0}\\) or \\(\\color{chocolate}{1}\\).\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))\nNote that the formula\n\\[\\color{chocolate}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{chocolate}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{chocolate}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{chocolate}{ a }\\) is the maximum of the \\(\\color{chocolate}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{chocolate}{ x}\\) and some prediction \\(\\color{chocolate}{ p(x)}\\) is given by:\n\\[ \\color{chocolate}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(x\\)s are 1-hot encoded, this can be rewritten as \\(\\color{chocolate}{ -\\log(p_{i})}\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than 1 (probability) is result in negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1"
  },
  {
    "objectID": "tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "href": "tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "title": "Neural Network Math",
    "section": "Math use in Maching Learning",
    "text": "Math use in Maching Learning\nHere are some essential math use in Machine Learning. It’s important to understand them in order to get insight into the inner working of the Neural Network. And how it come about the result.\n\nHere is a list of Math that is used in Neural Network:\n\nVector\nMatrix\nLoss functions\nCross-entropy loss\nsigmoid function\nsoftmax\nargmax function\nPartial derivative\ndifferential equation\n\n\n\n\nSoftmax (softargmax)\n\nSoftmax is one of the useful function in Neural Network computation. It’s allowed the data that output from the Neural Network which may not relate to one another be grouped into a single group and relate to each other as a posibility.\n\nIn mathematically term it’s a function that take the vector as input value and convert them to vector of output value and organized them as a probability value that sum to 1. The input value may be zero, negative, positive.\n\n\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\n\nSoftmax equation is defined by:\n\\(\\color{brown}{ \\sigma : \\mathbb{R}^K\\to (0,1)^K}\\) is defined when \\(\\color{chocolate}{ K \\ge 1 }\\) by the formula\n\\[\\color{chocolate}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in\\R^K  \\]\nor more concisely:\n\\[\\color{chocolate}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] The input vector \\(\\color{chocolate}{x}\\) values are normalized by dividing each value in the vector by the sum of all values; this normalization ensures that the sum of all the components of the output vector \\(\\color{chocolate}{ \\sigma(x)}\\) is \\(\\color{chocolate}{= 1}\\).\n\n\nSoftmax and Sigmoid\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a generalization version of Sigmoid function and the graph is identical\n\n\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar. \\[\\color{chocolate}{\\hbox{sigmoid S(x)} = \\frac{1}{1 + e^{-x}} }\\] Sigmoid can have two possibility that must sum to \\(\\color{chocolate}{1}\\). When softmax has only two possibility then it is equal to sigmoid function.\n\n\n\nArgmax function\n\nThe argmax function convert all the value in the input vector to zero except the maximum value in that vector which it’s convert to one. The resulting vector contain mostly 0 except the max value that is one.\n\nThe argmax function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg):\n\n\\[\\color{chocolate}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{chocolate}{0}\\) or \\(\\color{chocolate}{1}\\).\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a smoother verion of argmax\n\n\nHere is both softmax and argmax in one picture. The vector \\(\\color{chocolate}{v}\\) is softmax and vector \\(\\color{chocolate}{y}\\) is argmax\n\ncode\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))"
  },
  {
    "objectID": "tutorial/math/nnMath/loss.html",
    "href": "tutorial/math/nnMath/loss.html",
    "title": "Loss functions",
    "section": "",
    "text": "Tip\n\n\n\nsoftmax is a smoother verion of argmax\n\n\nNote that the formula\n\\[\\color{chocolate}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{chocolate}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{chocolate}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{chocolate}{ a }\\) is the maximum of the \\(\\color{chocolate}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{chocolate}{ x}\\) and some prediction \\(\\color{chocolate}{ p(x)}\\) is given by:\n\\[ \\color{chocolate}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(x\\)s are 1-hot encoded, this can be rewritten as \\(\\color{chocolate}{ -\\log(p_{i})}\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than 1 (probability) is result in negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorial/math/nnMath/loss/loss.html",
    "href": "tutorial/math/nnMath/loss/loss.html",
    "title": "Loss functions",
    "section": "",
    "text": "Tip\n\n\n\nsoftmax is a smoother verion of argmax\n\n\n\nNote that the formula\n\\[\\color{chocolate}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{chocolate}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{chocolate}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{chocolate}{ a }\\) is the maximum of the \\(\\color{chocolate}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{chocolate}{ x}\\) and some prediction \\(\\color{chocolate}{ p(x)}\\) is given by:\n\\[ \\color{chocolate}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(x\\)s are 1-hot encoded, this can be rewritten as \\(\\color{chocolate}{ -\\log(p_{i})}\\) where i is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than 1 (probability) is result in negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorial/math/nnMath/loss/loss.html#loss-functions",
    "href": "tutorial/math/nnMath/loss/loss.html#loss-functions",
    "title": "Loss or Cost functions",
    "section": "Loss Functions",
    "text": "Loss Functions\nLoss functions are important in Neural Network. It’s at the heart of how the machine can learn by trial and error. The value calculate by this function is used to adjust the weight of the input data to nudge the NN to lean toward output the correct answer.\n\n\n\n\n\n\nTip\n\n\n\nLoss function refer a single value calculation, while cost function refers to whole or group of value combine together\n\n\n\nThere are many loss functions that work well for particular problem. So choosing the appropriate one is critical to get the machine the work and archieve your goal.\n\nCost functions\nThe cost function refer to the sum of all loss functions\n\n\nType of Loss function\nThere are many type of Loss functions. Each one is approriate for certain tasks understand how each of them work is essential to reach your result.\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{chocolate}{ x}\\) and some prediction \\(\\color{chocolate}{ p(x)}\\) is given by:\n\\[ \\color{chocolate}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(\\color{chocolate}{x}\\) are 1-hot encoded, this can be rewritten as \\(\\color{chocolate}{ -\\log(p_{i})}\\) where \\(\\color{chocolate}{i}\\) is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\n\nLogarithm\nUsing logarithm is important to get stable compute in practic. Since in actual compute the machine using floating point number to do caculation. And exact precision is depend on the number of bit uses. If the number of bit is too small the value compute is underflow or the carry bit is large the value will be overflow, on the other hand, if the number of bit is too much the bit are waste.\nLog can help speed up the compute by convert the multiplication and division which is slow into addition and substraction which is fast. Not only that in some case it may help avoid the overflow and underflow problem.\nNote that the formula\n\\[\\color{chocolate}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{chocolate}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{chocolate}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{chocolate}{ a }\\) is the maximum of the \\(\\color{chocolate}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n    \ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than \\(\\color{chocolate}{1}\\) (probability) the result is negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1\n\n\\[\\color{chocolate}{ \\log(p_{i}) = -p_{i}\\ \\ \\text{when}\\ i \\lt 1 }\\]\nTherefore the minus sign is used to convert it to positive value"
  },
  {
    "objectID": "tip/tips.html#sa",
    "href": "tip/tips.html#sa",
    "title": "Tips and Tricks",
    "section": "Sa",
    "text": "Sa\n\ntitle: “matplotlib demo” format: html: code-fold: true jupyter: python3\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "tip/tips.html#sample-of-code-block-that",
    "href": "tip/tips.html#sample-of-code-block-that",
    "title": "Tips and Tricks",
    "section": "Sample of code block that",
    "text": "Sample of code block that\n\ntitle: “matplotlib demo” format: html: code-fold: true jupyter: python3\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "tip/tips.html#sample-of-code-block-that-execute-by",
    "href": "tip/tips.html#sample-of-code-block-that-execute-by",
    "title": "Tips and Tricks",
    "section": "Sample of code block that execute by",
    "text": "Sample of code block that execute by\n\ntitle: “matplotlib demo” format: html: code-fold: true jupyter: python3\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "tip/tips.html#sample-of-code-block-that-execute-by-quato",
    "href": "tip/tips.html#sample-of-code-block-that-execute-by-quato",
    "title": "Tips and Tricks",
    "section": "Sample of code block that execute by quato",
    "text": "Sample of code block that execute by quato\n\ntitle: “matplotlib demo” format: html: code-fold: true jupyter: python3\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "tip/tips.html#sample-of-code-block-that-execute-by-quarto",
    "href": "tip/tips.html#sample-of-code-block-that-execute-by-quarto",
    "title": "matplotlib demo",
    "section": "Sample of code block that execute by quarto",
    "text": "Sample of code block that execute by quarto\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  }
]
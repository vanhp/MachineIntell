[
  {
    "objectID": "tip/tips.html#quarto-execute-julia-code",
    "href": "tip/tips.html#quarto-execute-julia-code",
    "title": "Julia Plots",
    "section": "quarto execute Julia code",
    "text": "quarto execute Julia code"
  },
  {
    "objectID": "tip/tips.html#parametric-plots",
    "href": "tip/tips.html#parametric-plots",
    "title": "Julia Plots",
    "section": "Parametric Plots",
    "text": "Parametric Plots\nPlot function pair (x(u), y(u)). See Figure 1 for an example.\n\n\nCode\nusing Plots\n\nplot(sin, \n     x->sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))\n\n\n\n\n\nFigure 1: Parametric Plots"
  },
  {
    "objectID": "tip/sample.html",
    "href": "tip/sample.html",
    "title": "MachineIntell",
    "section": "",
    "text": "%pip install numpy\n%pip install matplotlib"
  },
  {
    "objectID": "tip/sample.html#live-python-code",
    "href": "tip/sample.html#live-python-code",
    "title": "MachineIntell",
    "section": "Live Python code",
    "text": "Live Python code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "news/newsall.html",
    "href": "news/newsall.html",
    "title": "Latest News",
    "section": "",
    "text": "Stable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "news/newsall.html#deep-learning-from-fastai-library",
    "href": "news/newsall.html#deep-learning-from-fastai-library",
    "title": "Latest News",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nnbdev version was release"
  },
  {
    "objectID": "news/news1.html#natural-language-processing",
    "href": "news/news1.html#natural-language-processing",
    "title": "News 1",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "news/news2.html",
    "href": "news/news2.html",
    "title": "News 2",
    "section": "",
    "text": "[]"
  },
  {
    "objectID": "news/news2.html#natural-language-processing",
    "href": "news/news2.html#natural-language-processing",
    "title": "News 2",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "help/helps.html",
    "href": "help/helps.html",
    "title": "Helping Hand",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "resources/resource.html",
    "href": "resources/resource.html",
    "title": "Resources",
    "section": "",
    "text": "[]"
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "MachineIntell",
    "section": "",
    "text": "The main module for machine learning and deep learning library\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef foo(): pass\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport nbdev; nbdev.nbdev_export()\n:::"
  },
  {
    "objectID": "tutorial/math/diffEq/compoundint.html#ordinary-differential-equation",
    "href": "tutorial/math/diffEq/compoundint.html#ordinary-differential-equation",
    "title": "ODE Plot",
    "section": "Ordinary Differential Equation",
    "text": "Ordinary Differential Equation"
  },
  {
    "objectID": "tutorial/math/diffEq/compoundint.html#julia-plots",
    "href": "tutorial/math/diffEq/compoundint.html#julia-plots",
    "title": "ODE Plot",
    "section": "Julia Plots",
    "text": "Julia Plots\nPlot function pair (x(u), y(u)). See ?@fig-parametric for an example.\n#| label: fig-parametric\n#| fig-cap: \"Parametric Plots\"\n\nusing IJulia\n\nusing Plots\n\nplot(sin, \n     x->sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine intelligence",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "href": "tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "title": "Neural Network Math",
    "section": "Math use in Maching Learning",
    "text": "Math use in Maching Learning\nHere are some essential math use in Machine Learning. It’s important to understand them in order to get insight into the inner working of the Neural Network. And how it come about the result.\n\nHere is a list of Math that is used in Neural Network:\n\nVector\nMatrix\nLoss functions\nCross-entropy loss\nsigmoid function\nsoftmax\nargmax function\nPartial derivative\ndifferential equation\n\n\n\n\nSoftmax (softargmax)\n\nSoftmax is one of the useful function in Neural Network computation. It’s allowed the data that output from the Neural Network which may not relate to one another be grouped into a single group and relate to each other as a posibility.\n\nIn mathematically term it’s a function that take the vector as input value and convert them to vector of output value and organized them as a probability value that sum to 1. The input value may be zero, negative, positive.\n\n\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\n\nSoftmax equation is defined by:\n\\(\\color{coral}{ \\sigma : \\mathbb{R}^K\\to (0,1)^K}\\) is defined when \\(\\color{orange}{ K \\ge 1 }\\) by the formula\n\\[\\color{orange}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in R^K  \\]\nor more concisely:\n\\[\\color{orange}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] The input vector \\(\\color{orange}{x}\\) values are normalized by dividing each value in the vector by the sum of all values; this normalization ensures that the sum of all the components of the output vector \\(\\color{orange}{ \\sigma(x)}\\) is \\(\\color{orange}{= 1}\\).\n\n\nSoftmax and Sigmoid\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a generalization version of Sigmoid function and the graph is identical\n\n\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar. \\[\\color{orange}{\\hbox{sigmoid S(x)} = \\frac{1}{1 + e^{-x}} }\\] Sigmoid can have two possibility that must sum to \\(\\color{orange}{1}\\). When softmax has only two possibility then it is equal to sigmoid function.\n\n\n\nArgmax function\n\nThe argmax function convert all the value in the input vector to zero except the maximum value in that vector which it’s convert to one. The resulting vector contain mostly 0 except the max value that is one.\n\nThe argmax function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg):\n\n\\[\\color{orange}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{orange}{0}\\) or \\(\\color{orange}{1}\\).\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a smoother verion of argmax\n\n\nHere is both softmax and argmax in one picture. The vector \\(\\color{orange}{v}\\) is softmax and vector \\(\\color{orange}{y}\\) is argmax\n\ncode\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))"
  },
  {
    "objectID": "tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "title": "Linear Algebra",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nTo ask NN to analyze whetherthe picture is a dog or a cat these pictures must be digitized and feed to the NN. Machine Learning deal with large amount of data for example a picture of a dog taken with smart phone consist of thousand up to many million pixels. To feed this image to NN for analysis is must be transform into array of data that NN can use.\nIn Math the branch that work with array of data is called Linear Algebra and the field is most appropriate is matrices.\nArray of data may have multidimension. Name of array with different dimension:\n\nVector an array with one dimension or rank\nMatrix an array with two dimension or rank\nTensor an array with three or more dimension or rank\n\nOne of the most frequent uses operation in NN is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(C = A \\times B\\)\n\n\n\nFramework Provide tools\nAI framework such as Pytorch, TensorFlow, Flux have many of the math tools build in. Also many programming languages e.g. Python, Julia, R have support for Matrix operations build in to the language."
  },
  {
    "objectID": "tutorial/math/nnMath/loss/loss.html#loss-functions",
    "href": "tutorial/math/nnMath/loss/loss.html#loss-functions",
    "title": "Loss or Cost functions",
    "section": "Loss Functions",
    "text": "Loss Functions\nLoss functions are important in Neural Network. It’s at the heart of how the machine can learn by trial and error. The value calculate by this function is used to adjust the weight of the input data to nudge the NN to lean toward output the correct answer.\n\n\n\n\n\n\nTip\n\n\n\nLoss function refer a single value calculation, while cost function refers to whole or group of value combine together\n\n\n\nThere are many loss functions that work well for particular problem. So choosing the appropriate one is critical to get the machine the work and archieve your goal.\n\nCost functions\nThe cost function refer to the sum of all loss functions\n\n\nType of Loss function\nThere are many type of Loss functions. Each one is approriate for certain tasks understand how each of them work is essential to reach your result.\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{orange}{ x}\\) and some prediction \\(\\color{orange}{ p(x)}\\) is given by:\n\\[ \\color{orange}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(\\color{orange}{x}\\) are 1-hot encoded, this can be rewritten as \\(\\color{orange}{ -\\log(p_{i})}\\) where \\(\\color{orange}{i}\\) is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\n\nLogarithm\nUsing logarithm is important to get stable compute in practic. Since in actual compute the machine using floating point number to do caculation. And exact precision is depend on the number of bit uses. If the number of bit is too small the value compute is underflow or the carry bit is large the value will be overflow, on the other hand, if the number of bit is too much the bit are waste.\nLog can help speed up the compute by convert the multiplication and division which is slow into addition and substraction which is fast. Not only that in some case it may help avoid the overflow and underflow problem.\nNote that the formula\n\\[\\color{orange}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{orange}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{orange}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{orange}{ a }\\) is the maximum of the \\(\\color{orange}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n    \ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than \\(\\color{chocolate}{1}\\) (probability) the result is negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1\n\n\\[\\color{orange}{ \\log(p_{i}) = -p_{i}\\ \\ \\text{when}\\ i \\lt 1 }\\]\nTherefore the minus sign is used to convert it to positive value"
  },
  {
    "objectID": "tutorial/nlps/rnn.html",
    "href": "tutorial/nlps/rnn.html",
    "title": "Recurrent Neural Network",
    "section": "",
    "text": "The Markov model weakness is that it’s limit to context window(scope) that was choosen. Using the info that was stored to predict prior data then feed that info to model while it was considering current tag."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#bidirectional-rnn",
    "href": "tutorial/nlps/rnn.html#bidirectional-rnn",
    "title": "Recurrent Neural Network",
    "section": "Bidirectional RNN",
    "text": "Bidirectional RNN\nA technique to train two independent RNN where one process from start to end the other process from end to start then combine the output from both into single one"
  },
  {
    "objectID": "tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "href": "tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "title": "Recurrent Neural Network",
    "section": "Long short term memory (LSTM)",
    "text": "Long short term memory (LSTM)\n\nAn RNN that has the capability to forget the info that is not relevant to the current task.\n\n\nLSTM\n\nforget gate to delete info of non relevant from current context\nadd gate to select new info into current context with tanh activation that indicate the direction of info(should care about) and a sigmoid to indicate the scaling(how much should be care about) factor of the info to be add to forget gate to produce state context\nout gate with sigmoid combine with state context to output result"
  },
  {
    "objectID": "tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "href": "tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "title": "Recurrent Neural Network",
    "section": "Creating a Language Model from Scratch",
    "text": "Creating a Language Model from Scratch\nA language model is a model that predict the next word in the sentence.\n\nFirst Language Model\nBuild a model to predict each word based on the previous three words by create a list of every sequence of three words as independent variables, and the next word after each sequence as the dependent variable.\nThe model takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. It use the standard three linear layers, but with two tweaks.\n\nThe first linear layer will use only the first word’s embedding as activations,\n\nThe second layer will use the second word’s embedding plus the first layer’s output activations, and\nThe third layer will use the third word’s embedding plus the second layer’s output activations.\nThe key effect of this is that every word is interpreted in the information context of any words preceding it.\n\nEach of these three layers will use the same weight matrix\n\nThe way that one word impacts the activations from previous words should not change depending on the position of a word.\nActivation values will change as data moves through the layers, but the layer’s weights themselves will not change from layer to layer.\nSo, a layer does not learn one sequence position; it must learn to handle all positions."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "href": "tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "title": "Recurrent Neural Network",
    "section": "The architect of the first model",
    "text": "The architect of the first model\n\nHere the figure the model. Where word is the input, FC is fully connected layer and triangular is output prediction\n\n3 layers model code\nThe first cut of the code for 3 layers model use:\n\nThe embedding layer (input2_hidden, for input to hidden)\nThe linear layer to create the activations for the next word (hidden2_hidden, for hidden to hidden)\nA final linear layer to predict the fourth word (hidden2_output, for hidden to output)\n\nThey all use the same embedding since they come from same data\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n  # input2_hidden is embedding layer\n  # hidden2_hidden is linear layer\n  # hidden2_output is linear layer   \n    def forward(self, x):\n        h = F.relu(self.hidden2_hidden(self.input2_hidden(x[:,0])))\n        h = h + self.input2_hidden(x[:,1])\n        h = F.relu(self.hidden2_hidden(h))\n        h = h + self.input2_hidden(x[:,2])\n        h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\n\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n# tensor of numericalized value for model\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\nbs = 64\ncut = int(len(seqs) * 0.8)\n# create batch\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)   \ncompare to the simplest model which always predict the next word which is ‘thousand’ to see how it performs:\n# a simplest model that always predict 'thousand' on each input sentence\nc = Counter(tokens[cut:])\nmc = c.most_common(5)\nmc\nmc[0][1] / len(tokens[cut:])\n\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n#index of the most common words ('thousand')\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n\nRefactor to use loop:\nThe RNN \nRewrite the code to use loop this is look closer to RNN\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n        \n    # refactor to use for loop the RNN!\n    def forward(self, x):\n        h = 0.               # using broascast\n        for i in range(3):\n            h = h + self.input2_hidden(x[:,i])\n            h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\n\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\nRefactor to add memory to RNN\nAdd the ability to retain previous word instead of start up new every time \nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0.  # using broascast\n        \n    # refactor to use for loop the RNN!\n    def forward(self, x):\n                   \n        for i in range(3):\n            self.h = self.h + self.input2_hidden(x[:,i])\n            self.h = F.relu(self.hidden2_hidden(h))\n            \n        out = self.hidden2_output(self.h)\n        self.h = self.h.detach() # do bptt\n        return out\n    def reset(self): self.h = 0.\n\n\n\nBPTT Backpropagation through time\nThis model hidden state now can remember previous activation from previous batch and the gradient will be calculate on sequence length token of the past not instead of recalculate with each new stream this is call BPTT\n# rearrange data so model see in particular sequence\nm = len(seqs)//bs\nm,bs,len(seqs)\n\n# reindex model see as contiguous batch with each epoch\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, drop_last=True, # drop last batch that have diff shape\n    shuffle=False) # maintain sequence\n\nlearn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, \n                cbs=ModelResetter) # callback to reset each epoch\nlearn.fit_one_cycle(10, 3e-3)\nAdd More signal: keep the output\nThe model no longer throw away the output from previous run but add them as input to current run\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n\n# check if it still offset by 1\n[L(vocab[o] for o in s) for s in seqs[0]]\nRewrite the model to now output every word instead of every 3 words in order to feed this into next run\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n    \n    def reset(self): self.h = 0\n\n    def loss_func(inp, targ):\n        return F.cross_entropy(inp.view(-1, len(vocab)), # flatten out to match bs x sl x vocab_sz from model\n        targ.view(-1)) # flatten out to match bs x sl x vocab_sz from model\n\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)     \n\n```python\n\n### Recurrent Neural Network: RNN\n![](./image/rnn-pic1.jpeg)\nRNN feed the output activation value back into hidden layer to help the hidden layers retain info about previous run therefore have some *memory* of the past.\n\n### Multi-Layer RNN\nsince the current model use the same weight matrix for each hidden layer which mean there no new info to be learn from. One way to \nTo improve the model further is to stack more layers by feed the output from one layer into the next layer so on.\n\n\nLook at it in unrolling way\n![stack-layer](./image/stacklayer_rnn2.png)\n\nRefactoring to use PyTorch\n\nThe model now has too deep layers this could lead to problem of *vanishing* or *exploding* gradient\n\n```python\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n\n    def reset(self): self.h.zero_()\n\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\nExploding or Disappearing Activations\nThe problem stem from the gradient value calcutated from the output layer won’t propagated back to the earlier layer. This is because the network is too deep.\nVanishing Gradient:\nAs the gradient value travel backward the especially the small value is diminishing as the floating point value get computed and recomputed many time each time it get round off closer and closer to 0 and finally become 0.\nExploding Gradient:\nThis the opposite of vanishing gradient. This phenomena happen espcially when the large value get larger with each computation it get exponentially large until it get large closer to infinity and become useless. \n\n\nThe floating point problem\nOne problem with doing floating point math with computer is that computer has limit amount of storage and floating point number is imprecise e.g. 1.0000012 or 1.00000000000000000000000012 is these two numbers are the same value? The answer is it’s depend on what do you want to do with it. On some task the first one is good enough but on other the second is what require. To store the second number require more storage.\nThe impreciseness is rather harder to understand. Float number get denser and denser when value get closer and closer to 0. In practice how do you represent this to the computer so it do some useful work. The IEEE organization come up with a standard which require how much storage should be devoted to certain type of floating point value e.g. 8-bit folating point,16-bit, 32-bit which mean the storage depend on the precision the user requirement not the actual value it may case the value get round-off read more from here. This mean do more calculation may lead less precision. This is what happen when doing lot and lot of gradient calculations the value may end up at 0 which is call vanishing gradient or the value get larger and larger exponentially or explode to infinite.\nThese problems are the main reason why RNN model is hard to train than CNN model,however research is very active to try new way to reduce or avoid these problems.\n\n\nNeural Network that have Memory\nIn Machine Learning two approach to handle Explode and Vanishing gradient is to use Neural network that can retain previous activation and also can figure out what value should be retain and which should be removed. The two popular approch are LSTM and GRU. GRU is a simplify version of LSTM\n\n\n\nComparison LSTM and GRU\n\n\n\nLSTM architecture\nInternally LSTM comprise with little neural net that decise how much gradient is needed and which one to update or delete. \nsigmoid equation:\n\\(\\color{orange}{ f(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1} = \\frac12 + \\frac12 \\tanh\\left(\\frac{x}{2}\\right)}\\)\nSigmoid only let positive value between 0 and 1 pass through\ntanh equation:\n\\(\\color{orange}{ f(x) = \\tanh x = \\frac{e^x-e^{-x}}{e^x+e^{-x}}}\\)\nTanh only let value between -1 and 1 pass through\n\n\nAnother look at LSTM internal:\n\nThe little NN is compose of gates call forget gate,input gate, cell gate, output gate. These gates work together to provide LSTM the capability to remember activation value that is important and forget the unneccessary activation value\n\nThe forget \\(\\color{orange}{ f_{t}}\\) gate:\nTake input \\(\\color{orange}{ x}\\), hidden state \\(\\color{orange}{ h_{t-1}}\\) then gated them via the sigmoid \\(\\color{orange}{ \\sigma}\\) activation to get only positive value then multiply them with previous cell state(memory) \\(\\color{orange}{ C_{t-1}}\\). It decides should the value be kept or discarded. If result from \\(\\color{orange}{ \\sigma}\\) value closer to 1 the value is kept else the value is discarded.\n\n\nThe input \\(\\color{orange}{ i_{t}}\\)gate:\nTogether with cell gate to update the cell state this gate also decide should the value be kept or discard depend on the value of the sigmoid\n\n\nThe cell \\(\\color{orange}{ \\tilde{C_{t}}}\\) gate:\nDecide what value to update from the range of -1 to 1 output from tanh function the value then add with previou cell state \\(\\color{orange}{ C_{t-1}}\\) value to get \\(\\color{orange}{ C_{t}}\\) the new value in memeory\n\n\nThe Output \\(\\color{orange}{ o_{t}}\\) gate:\nDecide which value to output to be multiply with the cell state value that was filter with tanh before output as new hidden state for the next layer.\nThe code for LSTM cell:\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h,c)\nRefactor to take advantage of the GPU by combine all small matrix into one big one to avoid moving too frequence data in and out of GPU and allow GPU to parallelize the task.\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) #split tensor into 4 then combine with input\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n\n\n\nTrain the LSTM\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)\n\n\nRegularizing an LSTM\nAlthough hard to train and prone to overfitting, there some trick such as using data augmentation by translate the same text into different language then retranslate back to generate more variance. And using various regularization method to alleviate the overfitting problem some of the techniques describe here:\n\n\nDropout\n Dropout is one of the regularization technique use to combat overfitting tendency of the model. The method usually apply at training time. This method is to randomly change some activations value to zero which temporary remove the neural nodes from the network.\nIt makes the neural less relie on the input from the source that the neural regularly receive the input from since these sources may not be there. It makes sure all neurons actively work toward the general concept rather than try to fit specify pattern in the current data.\nDropout has different behavior in training and validation mode which is why the flag training must set to True when traing and False when evaluating.\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p) # create probability of random value \n        return x * mask.div_(1-p)\n\n\nActivation Regularization (AR) and Temporal Activation Regularization (TAR)\ntwo regularization methods very similar to weight decay.\n\nAR\nThis approach is apply at the final activation from LSTM to reduce its size. AR is often applied on the dropped-out activations. The code is\nloss += alpha * activations.pow(2).mean()\n\n\nTAR\nThis approach is to encourage the model to output sensible value by adding a penalty to the loss to make the difference between two consecutive activations as small as possible. TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps) with activations tensor has a shape bs x sl x n_hid the code is:\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\nTo use these is required:\n\nthe proper output,\nthe activations of the LSTM pre-dropout, and\nthe activations of the LSTM post-dropout\n\nIn practive it’s often used a callback RNNRegularizer to apply the regularization.\n\n\n\nTraining AWD-LSTM: a Weight-Tied Regularized LSTM\nApply regularization can be combined together dropout, AR, TAR This method uses: - Embedding dropout (just after the embedding layer) - Input dropout (after the embedding layer) - Weight dropout (applied to the weights of the LSTM at each training step) - Hidden dropout (applied to the hidden state between two layers)\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\nAnother trick is weight-tying  by realize that input embedding is a mapping from English words to activation value. And output from hidden layer is a mapping from activations value to English words are the same thing. And assign the same weight matrix to these layersself.h_o.weight  self.i_h.weight\nThe final code tweak become:\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter,  # add callback\n                RNNRegularizer(alpha=2, beta=1)]) # add callback to learner\n\n# or use the TextLearner that will call add the callback\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)                       \n\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n\n\nGRU: Gate Recurrent Units architecture\n\nGRU is a simplify version of LSTM and work them same way."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#rnn-application",
    "href": "tutorial/nlps/rnn.html#rnn-application",
    "title": "Recurrent Neural Network",
    "section": "RNN application",
    "text": "RNN application\n\nPOS\nNER\nDeidentification\nTranslation\nsequence-to-sequence\nchatbot\nquestion-answer\nsequence classification\nsentiment\n\n\nJargons:\n\nhidden state: The activations that are updated at each step of a recurrent neural network.\nA neural network that is defined using a loop like this is called a recurrent neural network (RNN)\nBack propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which “detaches” the history of computation steps in the hidden state every few time steps. In essence calculate the gradient and propagate backward as usual but don’t store them.\nThe bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p)\nalpha and beta are two hyperparameters to tune"
  },
  {
    "objectID": "tutorial/math/diffEq/diffeq1.html",
    "href": "tutorial/math/diffEq/diffeq1.html",
    "title": "Calculus",
    "section": "",
    "text": "A Mathematic branch that try to understand the changing world by study things that are continuouly changing. Calculus has two branches:"
  },
  {
    "objectID": "tutorial/math/diffEq/diffeq1.html#differential-equation",
    "href": "tutorial/math/diffEq/diffeq1.html#differential-equation",
    "title": "Calculus",
    "section": "Differential Equation",
    "text": "Differential Equation\nA branch of Differential Calculus that work with problem involving how some thing change affected other related thing is called Differential Equation. It try to understand how an independent variable \\(\\color{lime}{x}\\) change induce a dependent variable \\(\\color{lime}{y}\\) to change and try to find the solution to it. It is used in Physic, Economy, Biology etc…\nIn Mathematic Differential means a proces to find the derivative or slope or in lay term the rate of change. Differential Equation means an equation involving with thing that changing which has the derivative as part of the equation.\nThere are many types of Differential Equations here are a few of the main one are:\n\nOrdinary Differential Equation ODE\n\nPartial Differential Equations PDE\nNon-linear Differential Equations NLDE\n\nThe ODE dealing with only one independent variable. While Partial Differential Equation dealing with multiple independent variables and Non-linear Differential Equation work with non linear system.\n\nOrdinary Differential Equation (ODE)\nODE dealing with how a dependent variable changed with respect to a single independent variable changed(derivative).\n\n\n\n\n\n\nTip\n\n\n\nODE general equation form: \\(y^{(n)}= F\\left (x,y,y',\\ldots, y^{(n-1)} \\right )\\)\n\n\n\nProblem: compute the compound Interest of a bank account\nHere an example of ODE problem: figure out the compound interest of a bank account. A bank account has interest which accrues when the interest get added to original balance. What would be the banlance at the end of one year, five years or any day, month, year?\n\n\n\n\n\n\nTip\n\n\n\nsimplify form: \\(\\frac{du}{dt} = pu = f(u,p,t)\\)"
  },
  {
    "objectID": "news/news2.html#deep-learning-from-fastai-library",
    "href": "news/news2.html#deep-learning-from-fastai-library",
    "title": "News 2",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nUsing fastai library is the fastest way to get your Machine Learning working with less head ache"
  },
  {
    "objectID": "tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "href": "tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "title": "Machine Vision",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "href": "tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "title": "Machine Vision",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/visions/vision.html",
    "href": "tutorial/visions/vision.html",
    "title": "Machine Vision",
    "section": "",
    "text": "[] (/../images/ai-pic7cp.jpg)"
  },
  {
    "objectID": "tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "href": "tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "title": "NLP",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "href": "tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "title": "NLP",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/nlps/nlp.html",
    "href": "tutorial/nlps/nlp.html",
    "title": "NLP",
    "section": "",
    "text": "[](/images../../images/ai-pic7cp.jpg"
  },
  {
    "objectID": "tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "title": "Mathematic",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/math/maths.html#differential-equation-in-machine-learning",
    "href": "tutorial/math/maths.html#differential-equation-in-machine-learning",
    "title": "Mathematic",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/math/maths.html",
    "href": "tutorial/math/maths.html",
    "title": "Mathematic",
    "section": "",
    "text": "../../images/ai-pic7cp.jpg"
  },
  {
    "objectID": "resources/resource.html#help-from-expert",
    "href": "resources/resource.html#help-from-expert",
    "title": "Resources",
    "section": "Help from expert",
    "text": "Help from expert\nHere you can find helping hand when you need it"
  }
]
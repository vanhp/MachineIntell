[
  {
    "objectID": "tutorial/math/diffEq/compoundint.html#ordinary-differential-equation",
    "href": "tutorial/math/diffEq/compoundint.html#ordinary-differential-equation",
    "title": "ODE Plot",
    "section": "Ordinary Differential Equation",
    "text": "Ordinary Differential Equation"
  },
  {
    "objectID": "tutorial/math/diffEq/compoundint.html#julia-plots",
    "href": "tutorial/math/diffEq/compoundint.html#julia-plots",
    "title": "ODE Plot",
    "section": "Julia Plots",
    "text": "Julia Plots\nPlot function pair (x(u), y(u)).\nSee ?@fig-parametric for an example.\n#| label: fig-parametric\n#| fig-cap: \"Parametric Plots\"\n\n# using IJulia\n\nusing Plots\n\nplot(sin, \n     x->sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))"
  },
  {
    "objectID": "tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "title": "Mathematic",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/math/maths.html#differential-equation-in-machine-learning",
    "href": "tutorial/math/maths.html#differential-equation-in-machine-learning",
    "title": "Mathematic",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "href": "tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "title": "Neural Network Math",
    "section": "Math use in Maching Learning",
    "text": "Math use in Maching Learning\nHere are some essential math use in Machine Learning. It’s important to understand them in order to get insight into the inner working of the Neural Network. And how it come about the result.\n\nHere is a list of Math that is used in Neural Network:\n\nVector\nMatrix\nLoss functions\nCross-entropy loss\nsigmoid function\nsoftmax\nargmax function\nPartial derivative\ndifferential equation\n\n\n\n\nSoftmax (softargmax)\n\nSoftmax is one of the useful function in Neural Network computation. It’s allowed the data that output from the Neural Network which may not relate to one another be grouped into a single group and relate to each other as a posibility.\n\nIn mathematically term it’s a function that take the vector as input value and convert them to vector of output value and organized them as a probability value that sum to 1. The input value may be zero, negative, positive.\n\n\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\n\nSoftmax equation is defined by:\n\\(\\color{coral}{ \\sigma : \\mathbb{R}^K\\to (0,1)^K}\\) is defined when \\(\\color{orange}{ K \\ge 1 }\\) by the formula\n\\[\\color{orange}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in R^K  \\]\nor more concisely:\n\\[\\color{orange}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] The input vector \\(\\color{orange}{x}\\) values are normalized by dividing each value in the vector by the sum of all values; this normalization ensures that the sum of all the components of the output vector \\(\\color{orange}{ \\sigma(x)}\\) is \\(\\color{orange}{= 1}\\).\n\n\nSoftmax and Sigmoid\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a generalization version of Sigmoid function and the graph is identical\n\n\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar. \\[\\color{orange}{\\hbox{sigmoid S(x)} = \\frac{1}{1 + e^{-x}} }\\] Sigmoid can have two possibility that must sum to \\(\\color{orange}{1}\\). When softmax has only two possibility then it is equal to sigmoid function.\n\n\n\nArgmax function\n\nThe argmax function convert all the value in the input vector to zero except the maximum value in that vector which it’s convert to one. The resulting vector contain mostly 0 except the max value that is one.\n\nThe argmax function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg):\n\n\\[\\color{orange}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{orange}{0}\\) or \\(\\color{orange}{1}\\).\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a smoother verion of argmax\n\n\nHere is both softmax and argmax in one picture. The vector \\(\\color{orange}{v}\\) is softmax and vector \\(\\color{orange}{y}\\) is argmax\n\ncode\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))"
  },
  {
    "objectID": "tutorial/math/nnMath/loss/loss.html#loss-functions",
    "href": "tutorial/math/nnMath/loss/loss.html#loss-functions",
    "title": "Loss or Cost functions",
    "section": "Loss Functions",
    "text": "Loss Functions\nLoss functions are important in Neural Network. It’s at the heart of how the machine can learn by trial and error. The value calculate by this function is used to adjust the weight of the input data to nudge the NN to lean toward output the correct answer.\n\n\n\n\n\n\nTip\n\n\n\nLoss function refer a single value calculation, while cost function refers to whole or group of value combine together\n\n\n\nThere are many loss functions that work well for particular problem. So choosing the appropriate one is critical to get the machine the work and archieve your goal.\n\nCost functions\nThe cost function refer to the sum of all loss functions\n\n\nType of Loss function\nThere are many type of Loss functions. Each one is approriate for certain tasks understand how each of them work is essential to reach your result.\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{orange}{ x}\\) and some prediction \\(\\color{orange}{ p(x)}\\) is given by:\n\\[ \\color{orange}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(\\color{orange}{x}\\) are 1-hot encoded, this can be rewritten as \\(\\color{orange}{ -\\log(p_{i})}\\) where \\(\\color{orange}{i}\\) is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\n\nLogarithm\nUsing logarithm is important to get stable compute in practic. Since in actual compute the machine using floating point number to do caculation. And exact precision is depend on the number of bit uses. If the number of bit is too small the value compute is underflow or the carry bit is large the value will be overflow, on the other hand, if the number of bit is too much the bit are waste.\nLog can help speed up the compute by convert the multiplication and division which is slow into addition and substraction which is fast. Not only that in some case it may help avoid the overflow and underflow problem.\nNote that the formula\n\\[\\color{orange}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{orange}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{orange}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{orange}{ a }\\) is the maximum of the \\(\\color{orange}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n    \ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than \\(\\color{chocolate}{1}\\) (probability) the result is negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1\n\n\\[\\color{orange}{ \\log(p_{i}) = -p_{i}\\ \\ \\text{when}\\ i \\lt 1 }\\]\nTherefore the minus sign is used to convert it to positive value"
  },
  {
    "objectID": "tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "href": "tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "title": "NLP",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "href": "tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "title": "NLP",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/nlps/rnn.html",
    "href": "tutorial/nlps/rnn.html",
    "title": "Recurrent Neural Network",
    "section": "",
    "text": "The Markov model weakness is that it’s limit to context window(scope) that was choosen. Using the info that was stored to predict prior data then feed that info to model while it was considering current tag."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#bidirectional-rnn",
    "href": "tutorial/nlps/rnn.html#bidirectional-rnn",
    "title": "Recurrent Neural Network",
    "section": "Bidirectional RNN",
    "text": "Bidirectional RNN\nA technique to train two independent RNN where one process from start to end the other process from end to start then combine the output from both into single one"
  },
  {
    "objectID": "tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "href": "tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "title": "Recurrent Neural Network",
    "section": "Long short term memory (LSTM)",
    "text": "Long short term memory (LSTM)\n\nAn RNN that has the capability to forget the info that is not relevant to the current task.\n\n\nLSTM\n\nforget gate to delete info of non relevant from current context\nadd gate to select new info into current context with tanh activation that indicate the direction of info(should care about) and a sigmoid to indicate the scaling(how much should be care about) factor of the info to be add to forget gate to produce state context\nout gate with sigmoid combine with state context to output result"
  },
  {
    "objectID": "tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "href": "tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "title": "Recurrent Neural Network",
    "section": "Creating a Language Model from Scratch",
    "text": "Creating a Language Model from Scratch\nA language model is a model that predict the next word in the sentence.\n\nFirst Language Model\nBuild a model to predict each word based on the previous three words by create a list of every sequence of three words as independent variables, and the next word after each sequence as the dependent variable.\nThe model takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. It use the standard three linear layers, but with two tweaks.\n\nThe first linear layer will use only the first word’s embedding as activations,\n\nThe second layer will use the second word’s embedding plus the first layer’s output activations, and\nThe third layer will use the third word’s embedding plus the second layer’s output activations.\nThe key effect of this is that every word is interpreted in the information context of any words preceding it.\n\nEach of these three layers will use the same weight matrix\n\nThe way that one word impacts the activations from previous words should not change depending on the position of a word.\nActivation values will change as data moves through the layers, but the layer’s weights themselves will not change from layer to layer.\nSo, a layer does not learn one sequence position; it must learn to handle all positions."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "href": "tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "title": "Recurrent Neural Network",
    "section": "The architect of the first model",
    "text": "The architect of the first model\n\nHere the figure the model. Where word is the input, FC is fully connected layer and triangular is output prediction\n\n3 layers model code\nThe first cut of the code for 3 layers model use:\n\nThe embedding layer (input2_hidden, for input to hidden)\nThe linear layer to create the activations for the next word (hidden2_hidden, for hidden to hidden)\nA final linear layer to predict the fourth word (hidden2_output, for hidden to output)\n\nThey all use the same embedding since they come from same data\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n  # input2_hidden is embedding layer\n  # hidden2_hidden is linear layer\n  # hidden2_output is linear layer   \n    def forward(self, x):\n        h = F.relu(self.hidden2_hidden(self.input2_hidden(x[:,0])))\n        h = h + self.input2_hidden(x[:,1])\n        h = F.relu(self.hidden2_hidden(h))\n        h = h + self.input2_hidden(x[:,2])\n        h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\n\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n# tensor of numericalized value for model\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\nbs = 64\ncut = int(len(seqs) * 0.8)\n# create batch\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)   \ncompare to the simplest model which always predict the next word which is ‘thousand’ to see how it performs:\n# a simplest model that always predict 'thousand' on each input sentence\nc = Counter(tokens[cut:])\nmc = c.most_common(5)\nmc\nmc[0][1] / len(tokens[cut:])\n\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n#index of the most common words ('thousand')\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n\nRefactor to use loop:\nThe RNN \nRewrite the code to use loop this is look closer to RNN\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n        \n    # refactor to use for loop the RNN!\n    def forward(self, x):\n        h = 0.               # using broascast\n        for i in range(3):\n            h = h + self.input2_hidden(x[:,i])\n            h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\n\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\nRefactor to add memory to RNN\nAdd the ability to retain previous word instead of start up new every time \nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0.  # using broascast\n        \n    # refactor to use for loop the RNN!\n    def forward(self, x):\n                   \n        for i in range(3):\n            self.h = self.h + self.input2_hidden(x[:,i])\n            self.h = F.relu(self.hidden2_hidden(h))\n            \n        out = self.hidden2_output(self.h)\n        self.h = self.h.detach() # do bptt\n        return out\n    def reset(self): self.h = 0.\n\n\n\nBPTT Backpropagation through time\nThis model hidden state now can remember previous activation from previous batch and the gradient will be calculate on sequence length token of the past not instead of recalculate with each new stream this is call BPTT\n# rearrange data so model see in particular sequence\nm = len(seqs)//bs\nm,bs,len(seqs)\n\n# reindex model see as contiguous batch with each epoch\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, drop_last=True, # drop last batch that have diff shape\n    shuffle=False) # maintain sequence\n\nlearn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, \n                cbs=ModelResetter) # callback to reset each epoch\nlearn.fit_one_cycle(10, 3e-3)\nAdd More signal: keep the output\nThe model no longer throw away the output from previous run but add them as input to current run\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n\n# check if it still offset by 1\n[L(vocab[o] for o in s) for s in seqs[0]]\nRewrite the model to now output every word instead of every 3 words in order to feed this into next run\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n    \n    def reset(self): self.h = 0\n\n    def loss_func(inp, targ):\n        return F.cross_entropy(inp.view(-1, len(vocab)), # flatten out to match bs x sl x vocab_sz from model\n        targ.view(-1)) # flatten out to match bs x sl x vocab_sz from model\n\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)     \n\n```python\n\n### Recurrent Neural Network: RNN\n![](./image/rnn-pic1.jpeg)\nRNN feed the output activation value back into hidden layer to help the hidden layers retain info about previous run therefore have some *memory* of the past.\n\n### Multi-Layer RNN\nsince the current model use the same weight matrix for each hidden layer which mean there no new info to be learn from. One way to \nTo improve the model further is to stack more layers by feed the output from one layer into the next layer so on.\n\n\nLook at it in unrolling way\n![stack-layer](./image/stacklayer_rnn2.png)\n\nRefactoring to use PyTorch\n\nThe model now has too deep layers this could lead to problem of *vanishing* or *exploding* gradient\n\n```python\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n\n    def reset(self): self.h.zero_()\n\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\nExploding or Disappearing Activations\nThe problem stem from the gradient value calcutated from the output layer won’t propagated back to the earlier layer. This is because the network is too deep.\nVanishing Gradient:\nAs the gradient value travel backward the especially the small value is diminishing as the floating point value get computed and recomputed many time each time it get round off closer and closer to 0 and finally become 0.\nExploding Gradient:\nThis the opposite of vanishing gradient. This phenomena happen espcially when the large value get larger with each computation it get exponentially large until it get large closer to infinity and become useless. \n\n\nThe floating point problem\nOne problem with doing floating point math with computer is that computer has limit amount of storage and floating point number is imprecise e.g. 1.0000012 or 1.00000000000000000000000012 is these two numbers are the same value? The answer is it’s depend on what do you want to do with it. On some task the first one is good enough but on other the second is what require. To store the second number require more storage.\nThe impreciseness is rather harder to understand. Float number get denser and denser when value get closer and closer to 0. In practice how do you represent this to the computer so it do some useful work. The IEEE organization come up with a standard which require how much storage should be devoted to certain type of floating point value e.g. 8-bit folating point,16-bit, 32-bit which mean the storage depend on the precision the user requirement not the actual value it may case the value get round-off read more from here. This mean do more calculation may lead less precision. This is what happen when doing lot and lot of gradient calculations the value may end up at 0 which is call vanishing gradient or the value get larger and larger exponentially or explode to infinite.\nThese problems are the main reason why RNN model is hard to train than CNN model,however research is very active to try new way to reduce or avoid these problems.\n\n\nNeural Network that have Memory\nIn Machine Learning two approach to handle Explode and Vanishing gradient is to use Neural network that can retain previous activation and also can figure out what value should be retain and which should be removed. The two popular approch are LSTM and GRU. GRU is a simplify version of LSTM\n\n\n\nComparison LSTM and GRU\n\n\n\nLSTM architecture\nInternally LSTM comprise with little neural net that decise how much gradient is needed and which one to update or delete. \nsigmoid equation:\n\\(\\color{orange}{ f(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1} = \\frac12 + \\frac12 \\tanh\\left(\\frac{x}{2}\\right)}\\)\nSigmoid only let positive value between 0 and 1 pass through\ntanh equation:\n\\(\\color{orange}{ f(x) = \\tanh x = \\frac{e^x-e^{-x}}{e^x+e^{-x}}}\\)\nTanh only let value between -1 and 1 pass through\n\n\nAnother look at LSTM internal:\n\nThe little NN is compose of gates call forget gate,input gate, cell gate, output gate. These gates work together to provide LSTM the capability to remember activation value that is important and forget the unneccessary activation value\n\nThe forget \\(\\color{orange}{ f_{t}}\\) gate:\nTake input \\(\\color{orange}{ x}\\), hidden state \\(\\color{orange}{ h_{t-1}}\\) then gated them via the sigmoid \\(\\color{orange}{ \\sigma}\\) activation to get only positive value then multiply them with previous cell state(memory) \\(\\color{orange}{ C_{t-1}}\\). It decides should the value be kept or discarded. If result from \\(\\color{orange}{ \\sigma}\\) value closer to 1 the value is kept else the value is discarded.\n\n\nThe input \\(\\color{orange}{ i_{t}}\\)gate:\nTogether with cell gate to update the cell state this gate also decide should the value be kept or discard depend on the value of the sigmoid\n\n\nThe cell \\(\\color{orange}{ \\tilde{C_{t}}}\\) gate:\nDecide what value to update from the range of -1 to 1 output from tanh function the value then add with previou cell state \\(\\color{orange}{ C_{t-1}}\\) value to get \\(\\color{orange}{ C_{t}}\\) the new value in memeory\n\n\nThe Output \\(\\color{orange}{ o_{t}}\\) gate:\nDecide which value to output to be multiply with the cell state value that was filter with tanh before output as new hidden state for the next layer.\nThe code for LSTM cell:\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h,c)\nRefactor to take advantage of the GPU by combine all small matrix into one big one to avoid moving too frequence data in and out of GPU and allow GPU to parallelize the task.\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) #split tensor into 4 then combine with input\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n\n\n\nTrain the LSTM\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)\n\n\nRegularizing an LSTM\nAlthough hard to train and prone to overfitting, there some trick such as using data augmentation by translate the same text into different language then retranslate back to generate more variance. And using various regularization method to alleviate the overfitting problem some of the techniques describe here:\n\n\nDropout\n Dropout is one of the regularization technique use to combat overfitting tendency of the model. The method usually apply at training time. This method is to randomly change some activations value to zero which temporary remove the neural nodes from the network.\nIt makes the neural less relie on the input from the source that the neural regularly receive the input from since these sources may not be there. It makes sure all neurons actively work toward the general concept rather than try to fit specify pattern in the current data.\nDropout has different behavior in training and validation mode which is why the flag training must set to True when traing and False when evaluating.\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p) # create probability of random value \n        return x * mask.div_(1-p)\n\n\nActivation Regularization (AR) and Temporal Activation Regularization (TAR)\ntwo regularization methods very similar to weight decay.\n\nAR\nThis approach is apply at the final activation from LSTM to reduce its size. AR is often applied on the dropped-out activations. The code is\nloss += alpha * activations.pow(2).mean()\n\n\nTAR\nThis approach is to encourage the model to output sensible value by adding a penalty to the loss to make the difference between two consecutive activations as small as possible. TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps) with activations tensor has a shape bs x sl x n_hid the code is:\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\nTo use these is required:\n\nthe proper output,\nthe activations of the LSTM pre-dropout, and\nthe activations of the LSTM post-dropout\n\nIn practive it’s often used a callback RNNRegularizer to apply the regularization.\n\n\n\nTraining AWD-LSTM: a Weight-Tied Regularized LSTM\nApply regularization can be combined together dropout, AR, TAR This method uses: - Embedding dropout (just after the embedding layer) - Input dropout (after the embedding layer) - Weight dropout (applied to the weights of the LSTM at each training step) - Hidden dropout (applied to the hidden state between two layers)\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\nAnother trick is weight-tying  by realize that input embedding is a mapping from English words to activation value. And output from hidden layer is a mapping from activations value to English words are the same thing. And assign the same weight matrix to these layersself.h_o.weight  self.i_h.weight\nThe final code tweak become:\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter,  # add callback\n                RNNRegularizer(alpha=2, beta=1)]) # add callback to learner\n\n# or use the TextLearner that will call add the callback\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)                       \n\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n\n\nGRU: Gate Recurrent Units architecture\n\nGRU is a simplify version of LSTM and work them same way."
  },
  {
    "objectID": "tutorial/nlps/rnn.html#rnn-application",
    "href": "tutorial/nlps/rnn.html#rnn-application",
    "title": "Recurrent Neural Network",
    "section": "RNN application",
    "text": "RNN application\n\nPOS\nNER\nDeidentification\nTranslation\nsequence-to-sequence\nchatbot\nquestion-answer\nsequence classification\nsentiment\n\n\nJargons:\n\nhidden state: The activations that are updated at each step of a recurrent neural network.\nA neural network that is defined using a loop like this is called a recurrent neural network (RNN)\nBack propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which “detaches” the history of computation steps in the hidden state every few time steps. In essence calculate the gradient and propagate backward as usual but don’t store them.\nThe bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p)\nalpha and beta are two hyperparameters to tune"
  },
  {
    "objectID": "tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "href": "tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "title": "Machine Vision",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "href": "tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "title": "Machine Vision",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tip/tips.html#quarto-execute-julia-code",
    "href": "tip/tips.html#quarto-execute-julia-code",
    "title": "Working with Tensors",
    "section": "quarto execute Julia code",
    "text": "quarto execute Julia code"
  },
  {
    "objectID": "tip/tips.html#parametric-plots",
    "href": "tip/tips.html#parametric-plots",
    "title": "Working with Tensors",
    "section": "Parametric Plots",
    "text": "Parametric Plots\nPlot function pair (x(u), y(u)). See ?@fig-parametric for an example.\n#| label: fig-parametric\n#| fig-cap: \"Parametric Plots\"\n\nusing Plots\n\nplot(sin, \n     x-&gt;sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))\n\nWorking with Tensors\nTensor is multi-dimension arrays\n\nVector is 1D array\nMatrix is 2D array\nTensor is nD array\n\nSince tensor is core operation of Neural Network. Understand how to manipulate Tensor efficiently is the diffenrence between success and failure. The majority of the tasks are transforming different shape and size of these arrays to match the expected shape and size of the API.\nMany of these operations involve:\n- Squeezing matrix into vector\n- Add dimension to vector to transform it into matrix\n- Convert tensor into matrix or vice versa\n- Convert tensor into vector or vice versa\nThe math rule that govern these operation are:\n\nDot-product\n\nMatrix multiplication\nMultiply scalar value to matrix\nMatrix addition\n\nPython has its own specific operation:\n\nVectorization\n\nElement-wise multiplication\nMatrix multiplication\n\nPytorch or Numpy specific:\n\nEinstein summation\n\n:bulb: Tips\nWhen working with these tensors always check the shape and size:\n\ndon’t forget to peek into the tensor as often as possible\n\ncheck the shape\ncheck the size\ncheck the content of the tensor\nverify that they are what they should be"
  },
  {
    "objectID": "tip/sample.html",
    "href": "tip/sample.html",
    "title": "MachineIntell",
    "section": "",
    "text": "%pip install numpy\n%pip install matplotlib"
  },
  {
    "objectID": "tip/sample.html#live-python-code",
    "href": "tip/sample.html#live-python-code",
    "title": "MachineIntell",
    "section": "Live Python code",
    "text": "Live Python code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "news/newsall.html",
    "href": "news/newsall.html",
    "title": "Latest News",
    "section": "",
    "text": "Solve age old differential equations help improve Machine Learning modeling\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "news/newsall.html#deep-learning-from-fastai-library",
    "href": "news/newsall.html#deep-learning-from-fastai-library",
    "title": "Latest News",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nnbdev version was release"
  },
  {
    "objectID": "news/news1.html#natural-language-processing",
    "href": "news/news1.html#natural-language-processing",
    "title": "News 1",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "news/news2.html#deep-learning-from-fastai-library",
    "href": "news/news2.html#deep-learning-from-fastai-library",
    "title": "News 2",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nUsing fastai library is the fastest way to get your Machine Learning working with less head ache"
  },
  {
    "objectID": "news/news2.html#natural-language-processing",
    "href": "news/news2.html#natural-language-processing",
    "title": "News 2",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "help/helps.html",
    "href": "help/helps.html",
    "title": "Helping Hand",
    "section": "",
    "text": "## Help from expert\nHere you can find helping hand when you need it\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources/resource.html#resources",
    "href": "resources/resource.html#resources",
    "title": "Resources",
    "section": "Resources",
    "text": "Resources\nSolve age old differential equations help improve Machine Learning modeling"
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "core",
    "section": "",
    "text": "core\n\nThe main module for machine learning and deep learning library\n\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nfrom nbdev.showdoc import *\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef foo(): pass\n:::\n::: {.cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\nimport nbdev; nbdev.nbdev_export()\n:::"
  },
  {
    "objectID": "Programming/prog.html",
    "href": "Programming/prog.html",
    "title": "Coding Tools",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "Programming/Languages/lang.html",
    "href": "Programming/Languages/lang.html",
    "title": "Coding Languages",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Vanh Phomsavanh",
    "section": "",
    "text": "I’m a professional developer working in Silicon Valley for over 20 years. I’ve been working with various High tech companies from big and small to startup and estasblihed.\nWorking with hardware devices such as IC packaging, embedded controllers, GUI frameworks on Windows an Mac. I’ve been fasinating with Linux long before Linus Torvalds show up at one of our SVLUG meetup in 1998 at Carl’Jr on First st and Zanker rd in San Jose.\nLinux is my fovorite OS and still is my daily drive today. I’m also find Artificial Intelligence quite fasninating along with Quantum Computing."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Vanh Phomsavanh",
    "section": "Education",
    "text": "Education\nSan Jose state University Computer Engineering 1989"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Vanh Phomsavanh",
    "section": "Experience",
    "text": "Experience\nfutureWei as Staff software engineer\nGeometric as Director of Software and System\nATT as Senior Network engineer\nSiemens as Framework engineer"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "",
    "text": "Originally posted on the fast.ai blog"
  },
  {
    "objectID": "blog/index.html#our-new-secret-weapon-for-productivity",
    "href": "blog/index.html#our-new-secret-weapon-for-productivity",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Our new secret weapon for productivity",
    "text": "Our new secret weapon for productivity\nToday we’re excited to announce that we’ve teamed up with Quarto to give nbdev superpowers. nbdev offers Python programmers a common set of tools for using Jupyter notebooks to:\n\nWrite & distribute software packages\nTest code, and\nAuthor documentation and technical articles\n\nAlthough notebooks are already widely used for once-off exploratory work, it’s less well-known that they are perfectly capable of writing quality software. In fact, we’ve used nbdev for a wide range of software projects over the last three years, including deep learning libraries, API clients, Python language extensions, terminal user interfaces, and more. We discovered that it is not only capable of writing great software but that it has also increased our productivity by 300% or more. With nbdev, developers simply write notebooks with lightweight markup and get high-quality documentation, tests, continuous integration, and packaging for free! Nbdev has allowed us to maintain and scale manyopen source projects. Pull requests are often accompanied by detailed documentation and tests–contributors simply write notebooks.\nThis is why we’re excited to share nbdev v2. It’s rewritten from the ground up, with much-anticipated features including:\n\nInteroperation with non-nbdev codebases for tasks like documentation\nSupport for any static site generator\nWide variety of output mediums such as blogs, papers, slides, and websites\nA faster Jupyter kernel, which also means faster tests\nCleaner and more extensible API, which supports custom directives, custom module exporters, and more"
  },
  {
    "objectID": "blog/index.html#nbdev-in-industry",
    "href": "blog/index.html#nbdev-in-industry",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "nbdev in industry",
    "text": "nbdev in industry\nWe have piloted nbdev at several companies. We were delighted to receive the following feedback, which fits our own experience using and developing nbdev:\n\n\n\nDavid Berg, on using nbdev for internal documentation at Netflix: “Prior to using nbdev, documentation was the most cumbersome aspect of our software development process… Using nbdev allows us to spend more time creating rich prose around the many code snippets guaranteeing the whole experience is robust. nbdev has turned what was once a chore into a natural extension of the notebook-based testing we were already doing.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nErik Gaasedelen, on using nbdev in production at Lyft: “I use this in production at my company. It’s an awesome tool… nbdev streamlines everything so I can write docs, tests, and code all in one place… The packaging is also really well thought out. From my point of view it is close to a Pareto improvement over traditional Python library development.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nHugo Bowne-Anderson, on using nbdev for Outerbounds: “nbdev has transformed the way we write documentation. Gone are the days of worrying about broken code examples when our API changes or [due to] human errors associated with copying & pasting code into markdown files. The authoring experience of nbdev… [allows] us to write prose and live code in a unified interface, which allows more experimentation… On top of this, nbdev allows us to include unit tests in our documentation which mitigates the burden of maintaining the docs over time.”\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoxanna Pourzand, on using nbdev for Transform: “We’re so excited about using nbdev. Our product is technical so our resulting documentation includes a lot of code-based examples. Before nbdev, we had no way of maintaining our code examples and ensuring that it was up-to-date for both command inputs and outputs. It was all manual. With nbdev, we now have this under control in a sustainable way. Since we’ve deployed these docs, we also had a situation where we were able to identify a bug in one of our interfaces, which we found by seeing the error that was output in the documentation.”"
  },
  {
    "objectID": "blog/index.html#whats-nbdev",
    "href": "blog/index.html#whats-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "What’s nbdev?",
    "text": "What’s nbdev?\nNbdev embraces the dynamic nature of python and REPL-driven development in ways that traditional IDEs and software development workflows cannot. We thoroughly discussed the motivation, history, and goals of nbdev in this initial launch post three years ago. The creator of Jupyter, Fernando Pérez, told us:\n\n[Nbdev] should be celebrated and used a lot more - I have kept a tab with your original nbdev blog post open for months in Chrome because of how often I refer to it and point others to this work\n\nIn short, nbdev embraces ideas from literate programming and exploratory programming. These paradigms have been revisited in platforms like XCode Playgrounds and languages like Smalltalk, LISP, and Mathematica. With nbdev, we sought to push these paradigms even further by enabling it for one of the most popular dynamic programming languages in the world: Python.\n\n\n\nState of the Octoverse 2021, GitHub\n\n\nEven though nbdev is most widely used in scientific computing communities due to its integration with Jupyter Notebooks, we’ve found that nbdev is well suited for a much wider range of software. We have used nbdev to write deep learning libraries, API clients, python language extensions,terminal user interfaces, and more!\nHamel: When I use nbdev, my colleagues are often astounded by how quickly I can create and distribute high-quality python packages. I consider nbdev to be a superpower that allows me to create tests and documentation without any additional friction, which makes all of my projects more maintainable. I also find writing software with nbdev to be more fun and productive as I can iterate very fast on ideas relative to more traditional software engineering workflows. Lastly, with nbdev I can also use traditional text-based IDEs if I want to, so I get the best of both worlds."
  },
  {
    "objectID": "blog/index.html#what-we-learned-after-three-years-of-using-nbdev",
    "href": "blog/index.html#what-we-learned-after-three-years-of-using-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "What we learned after three years of using nbdev",
    "text": "What we learned after three years of using nbdev\nWhile nbdev was originally developed to simplify the software development workflow for various fast.ai projects, we found that users wanted to extend nbdev to:\n\nWrite and publish blog posts, books, papers, and other types of documents with Jupyter Notebooks\nDocument existing codebases not written in nbdev\nAccommodate traditional Python conventions–for those constrained in how their code is organized and formatted\nPublish content using any static site generator\n\nWhile we created projects such as fastpages and fastdoc to accomplish some of these tasks, we realized that it would be better to have a single set of flexible tools to accomplish all of them. To this end, we were extremely excited to discover Quarto, an open-source technical publishing system built on pandoc.\nHamel: The more I used nbdev for creating Python modules, the more I wanted to use it for writing blogs and documenting existing codebases. The ability to customize the way notebooks are rendered (hiding vs. showing cells, stripping output, etc.), along with the facilities for including unit tests, made it my go-to authoring tool for all technical content. I’m excited that nbdev2 unlocks all of these possibilities for everyone!"
  },
  {
    "objectID": "blog/index.html#enter-quarto-a-pandoc-super-processor",
    "href": "blog/index.html#enter-quarto-a-pandoc-super-processor",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Enter Quarto: A pandoc super-processor",
    "text": "Enter Quarto: A pandoc super-processor\nQuarto is a project that enables technical publishing with support for Jupyter Notebook, VSCode, Observable, and plaintext editors. Furthermore, Quarto enables the publishing of high-quality articles, reports, websites, and blogs in HTML, PDF, ePub, PowerPoint slides, and more. Quarto is maintained by RStudio, a company with a long history of products supporting literate programming, such as RMarkdown and RStudio.\nQuarto is built on top of Pandoc, a universal document converter that supports nearly any format you can think of. Pandoc achieves this seemingly magical feat by representing documents in a common abstract syntax tree (AST) that serves as the medium through which different formats can be translated. By extension, Quarto allows you to generate content in almost any format you wish! You can use pandoc filters to modify the AST and the output format, which allows you to use any static site generator you want, and programmatically modify and generate content.\nQuarto allows you to compose pandoc filters in a processing pipeline and apply them to specific documents or entire projects. You can also distribute filters as Quarto extensions, which makes Quarto extremely customizable.\nWe also find Quarto compelling because user interfaces such as comment directives (comments that start with #|) correlate with nbdev. In fact, we even learned that nbdev inspired Quarto in this regard! In general, Quarto and nbdev share many goals, and the Quarto team has been incredibly responsive to our suggestions. For example, the ability to create notebook filters to modify notebooks before rendering. Below is a screenshot of a Jupyter notebook rendered with Quarto and nbdev.\n\n\n\n\nQuarto rendering a Jupyter notebook written with nbdev\n\n\n\nFinally, Quarto supports more programming languages than just Python and has been adding new features and fixing bugs at an impressive speed. This gives us confidence that we will be able to expand nbdev to support more use cases in the future. We discuss some of these future directions in the closing section."
  },
  {
    "objectID": "blog/index.html#a-blazing-fast-notebook-kernel-execnb",
    "href": "blog/index.html#a-blazing-fast-notebook-kernel-execnb",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "A blazing fast notebook kernel: execnb",
    "text": "A blazing fast notebook kernel: execnb\nA core component of nbdev is executing and testing notebooks programmatically. It is important that this notebook runner executes with minimal overhead to maintain our goal of providing a delightful developer experience. This is why we built execnb, a lightweight notebook runner for Python kernels, which executes notebooks blazingly fast. Furthermore, execnb allows parameterized execution of notebooks.\nHamel: I have been an enthusiastic user of tools like papermill that programmatically run notebooks for use-cases like creating dashboards or enabling new kinds of machine learning workflows. I believe execnb unlocks even more possibilities with its ability to inject arbitrary code at any place in a notebook, as well as the ability to pass callbacks that run before and/or after cells are executed. This opens up possibilities to create new types of workflows with notebooks that I am excited about exploring in the near future."
  },
  {
    "objectID": "blog/index.html#towards-a-dialect-of-python-that-embraces-its-dynamic-nature",
    "href": "blog/index.html#towards-a-dialect-of-python-that-embraces-its-dynamic-nature",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Towards a dialect of python that embraces its dynamic nature",
    "text": "Towards a dialect of python that embraces its dynamic nature\nOne way to understand nbdev is part of an ecosystem that is designed to embrace Python’s dynamic properties for REPL-driven software engineering. Similar to Clojure, our goal is to provide tools that remove all friction from using the REPL in your programming workflow. We believe that the REPL enhances developer workflows thanks to context-sensitive auto-completion, signature inspection, and documentation–all based on the actual state of your code, and none of which are available in IDEs that depend solely on static analysis. We have found that for this reason, nbdev, with its Jupyter notebook foundation, makes programming significantly more productive and enjoyable.\nOur efforts to support REPL-driven development and literate programming are not limited to nbdev. We maintain a number of libraries that extend python to bolster this programming experience. The most notable of these libraries is fastcore, which extends Python in terms of testing, documenting code, metaprogramming, attribute helpers, enhanced representations of objects, and notebook-friendly patching. This blog post offers a gentle introduction to fastcore. In addition to literate programming, fastcore encourages conventions such as brevity and efficient use of vertical space so you can accomplish more with significantly less code. For example, below is a simple decorator that enables notebook-friendly patching:\n\n\n\n@patch decorator from fastcore\n\n\nWe believe that this combination of a new developer workflow (nbdev), Python extensions (fastcore), and associated norms form a new dialect of Python that is centered on leveraging its dynamic nature–in contrast to an ever-growing trend toward static analysis. We suspect that this dialect of Python will be more productive for programmers in many scenarios. We are framing this ecosystem as a “dialect” as it is still very much Python and is approachable by anyone who is familiar with the language. Furthermore, despite nbdev’s notebook workflow, our tools generate plaintext modules that can be navigated and edited with text-based IDEs, allowing programmers to experience the best of both worlds, if they desire.\nHamel: I believe this framing of a Python dialect is key to properly understanding what nbdev is. While it may be tempting to get stuck on specific features or technical details of nbdev, it is useful to zoom out to understand the overall intent of creating a better workflow rather than conforming too rigidly to existing ones. A good analogy is TypeScript’s relationship with JavaScript: it is an extension of an existing programming language that supports a new way of programming. I encourage you to treat nbdev in a similar fashion: be willing to try new ways of programming and observe which tradeoffs resonate with you. At the very least, I believe nbdev is a fun way to experience a different way of writing software, which will broaden your horizons about programming in general, all without having to learn an entirely new programming language!"
  },
  {
    "objectID": "blog/index.html#the-future-of-nbdev",
    "href": "blog/index.html#the-future-of-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "The future of nbdev",
    "text": "The future of nbdev\nWhile we are excited about nbdev2, we believe we have only scratched the surface of what’s possible. We are considering the following features:\n\nSupporting more languages beyond Python, such as Julia, R and JavaScript\nOffering interfaces for executing parameterized notebooks that mimic Python scripts\nExtensions for more static site generators and filters\nSupporting alternate testing backends, such as pytest\nSupporting a greater number of docstring formats, such as Google-style docstrings\nMore options to use plain-text or human readable notebook backends other than JSON\n\nIf you have interesting ideas about how nbdev can be extended, please drop and chat with us on discord or post a message in the forums."
  },
  {
    "objectID": "blog/index.html#how-you-can-get-started-with-nbdev",
    "href": "blog/index.html#how-you-can-get-started-with-nbdev",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "How you can get started with nbdev",
    "text": "How you can get started with nbdev\nOur project’s website is at nbdev.fast.ai, where we will be posting tutorials, examples, and more documentation in the coming days."
  },
  {
    "objectID": "blog/index.html#thank-you",
    "href": "blog/index.html#thank-you",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "Thank You",
    "text": "Thank You\nThis new version of nbdev was a team effort by many wonderful people. We want to highlight two people who have made outstanding contributions:\n\nWasim Lorgat was instrumental across different areas, including significant contributions to fastcore, execnb, and nbdev, as well as the implementation of the new nbdev home page. With Wasim’s help, we were able to push nbdev to a new level of functionality and quality.\nJJ Allaire is not only the CEO of RStudio but also the steward of Quarto. JJ was incredibly responsive and eager to work with us on nbdev and added many features to Quarto specifically with nbdev in mind, such as notebook filters. We were also astounded by the attention to detail and the pace at which bugs are addressed. This new version of nbdev would not have been possible without JJ’s help, and we are excited to continue to work with him.\n\nWe also want to thank the amazing fastai community, notably Isaac Flath, Benjamin Warner and Zach Mueller for their tireless work on this project."
  },
  {
    "objectID": "blog/index.html#a-conversation-with-jj-allaire",
    "href": "blog/index.html#a-conversation-with-jj-allaire",
    "title": "nbdev+Quarto: A new secret weapon for coding productivity",
    "section": "A conversation with JJ Allaire",
    "text": "A conversation with JJ Allaire\nTo celebrate the launch of nbdev v2 and Quarto, Jeremy sat down with the CEO of Posit (previously known as RStudio, the company behind Quarto), JJ Allaire, to talk about software development, scientific publishing, R, Python, literate programming, and much more."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "MachineIntell",
    "section": "",
    "text": "import nbdev"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Intelligence",
    "section": "",
    "text": "Machine Intelligence and machine learning Technology"
  },
  {
    "objectID": "tutorial/math/diffEq/diffeq1.html",
    "href": "tutorial/math/diffEq/diffeq1.html",
    "title": "Calculus",
    "section": "",
    "text": "A Mathematic branch that try to understand the changing world by study things that are continuouly changing. Calculus has two branches:"
  },
  {
    "objectID": "tutorial/math/diffEq/diffeq1.html#differential-equation",
    "href": "tutorial/math/diffEq/diffeq1.html#differential-equation",
    "title": "Calculus",
    "section": "Differential Equation",
    "text": "Differential Equation\nA branch of Differential Calculus that work with problem involving how some thing change affected other related thing is called Differential Equation. It try to understand how an independent variable \\(\\color{lime}{x}\\) change induce a dependent variable \\(\\color{lime}{y}\\) to change and try to find the solution to it. It is used in Physic, Economy, Biology etc…\nIn Mathematic Differential means a proces to find the derivative or slope or in lay term the rate of change. Differential Equation means an equation involving with thing that changing which has the derivative as part of the equation.\nThere are many types of Differential Equations here are a few of the main one are:\n\nOrdinary Differential Equation ODE\n\nPartial Differential Equations PDE\nNon-linear Differential Equations NLDE\n\nThe ODE dealing with only one independent variable. While Partial Differential Equation dealing with multiple independent variables and Non-linear Differential Equation work with non linear system.\n\nOrdinary Differential Equation (ODE)\nODE dealing with how a dependent variable changed with respect to a single independent variable changed(derivative).\n\n\n\n\n\n\nTip\n\n\n\nODE general equation form: \\(y^{(n)}= F\\left (x,y,y',\\ldots, y^{(n-1)} \\right )\\)\n\n\n\nProblem: compute the compound Interest of a bank account\nHere an example of ODE problem: figure out the compound interest of a bank account. A bank account has interest which accrues when the interest get added to original balance. What would be the banlance at the end of one year, five years or any day, month, year?\n\n\n\n\n\n\nTip\n\n\n\nsimplify form: \\(\\frac{du}{dt} = pu = f(u,p,t)\\)"
  },
  {
    "objectID": "tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "href": "tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "title": "Linear Algebra",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nTo ask NN to analyze whetherthe picture is a dog or a cat these pictures must be digitized and feed to the NN. Machine Learning deal with large amount of data for example a picture of a dog taken with smart phone consist of thousand up to many million pixels. To feed this image to NN for analysis is must be transform into array of data that NN can use.\nIn Math the branch that work with array of data is called Linear Algebra and the field is most appropriate is matrices.\nArray of data may have multidimension. Name of array with different dimension:\n\nVector an array with one dimension or rank\nMatrix an array with two dimension or rank\nTensor an array with three or more dimension or rank\n\nOne of the most frequent uses operation in NN is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(C = A \\times B\\)\n\n\n\nFramework Provide tools\nAI framework such as Pytorch, TensorFlow, Flux have many of the math tools build in. Also many programming languages e.g. Python, Julia, R have support for Matrix operations build in to the language."
  },
  {
    "objectID": "tutorial/tutoring.html#linear-algebra-for-maching-learning",
    "href": "tutorial/tutoring.html#linear-algebra-for-maching-learning",
    "title": "Tutorials",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "tutorial/tutoring.html#differential-equation-in-machine-learning",
    "href": "tutorial/tutoring.html#differential-equation-in-machine-learning",
    "title": "Tutorials",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "tutorial/nlps/transformer.html",
    "href": "tutorial/nlps/transformer.html",
    "title": "Transformers",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "tutorial/nlps/attention.html",
    "href": "tutorial/nlps/attention.html",
    "title": "Attention Mechanism",
    "section": "",
    "text": "A technique to help improve the performance of the Encoder-Decoder by increase the flexibility to use the whole set of input data instead of just the data that are closer to it. By a weighted from all the input data vector plus the ability to assigned the most relevant data the higher weight. This allow the model to pick the most appropiate data for the task at hand.\nThis method also wisely uses in other discipline e.g. vision, not just in language translation.\nWhat are the components of Attention\nAttention is divided into three parts"
  },
  {
    "objectID": "tutorial/nlps/encodedecode.html",
    "href": "tutorial/nlps/encodedecode.html",
    "title": "Encoder-Decoder pair in NLP",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "tutorial/nlps/attention.html#computing-attention",
    "href": "tutorial/nlps/attention.html#computing-attention",
    "title": "Attention Mechanism",
    "section": "Computing attention",
    "text": "Computing attention\n The amount of attention that the network should be paying attention to is computed with\n\\(\\alpha^{<t,t'>} = \\text{amount of attention } y^{t} \\text{ should pay to }\\alpha^{t'}\\) \\(\\alpha^{<t,t'>} = \\frac{exp(e^{t,t'})}{\\sum_{t' = 1}^{T_x} exp(e^{<t,t'>}) }\\)\nwhere \\(\\alpha\\) is attention, \\(S\\) is state, \\(t\\) is time step, \\(t'\\) is time step of next input\ncompute \\(e^{<t,t'>}\\) using a small network.\nThe drawback of this algorithm is depended on the amount of \\(T_x\\) tokens it could run in quadratic time.\nVisualization of attention at work"
  },
  {
    "objectID": "tutorial/nlps/attention.html#seq2seq",
    "href": "tutorial/nlps/attention.html#seq2seq",
    "title": "Attention Mechanism",
    "section": "Seq2Seq",
    "text": "Seq2Seq\nThe general task of find item from a sequence to sequence can be compared to attention mechanism that have the three component as query \\(Q\\),keys \\(K\\), and value \\(V\\). As below:\n\n\\(Q\\) \\(\\implies\\) querying previous state decoder \\(S_{t-1}\\) output which each query a dabase of keys to compute the value using dot product \\[e_{q,k_i} = q\\cdot k_i\\]\nOnce the score is found then using softmax to compute the weights \\(V\\) \\(\\implies\\) value of encoder input \\(h_i\\) \\[\\alpha_{q,k_i} = softmax(e_{q,k_i})\\]\nCompute the attention with weight sum of value vector \\(V_{k_i}\\) \\(K\\) \\(\\implies\\) context vector \\(C_t\\)\n\n\\[attention(q,K,V) = \\sum_i^n \\alpha_{q,{k_i},V_{k_i}}\\]\nIn essence, when the generalized attention mechanism is presented with a sequence of words, it takes the query vector attributed to some specific word in the sequence and scores it against each key in the database. In doing so, it captures how the word under consideration relates to the others in the sequence. Then it scales the values according to the attention weights (computed from the scores) to retain focus on those words relevant to the query. In doing so, it produces an attention output for the word under consideration."
  },
  {
    "objectID": "tutorial/nlps/attention.html#general-attention-mechanism-with-seq2seq",
    "href": "tutorial/nlps/attention.html#general-attention-mechanism-with-seq2seq",
    "title": "Attention Mechanism",
    "section": "General Attention Mechanism with Seq2Seq",
    "text": "General Attention Mechanism with Seq2Seq\nThe general task of find item from a sequence to sequence can be compared to attention mechanism that have the three component as query \\(Q\\),keys \\(K\\), and value \\(V\\). As below:\n\n\\(Q\\) \\(\\implies\\) querying previous state decoder \\(S_{t-1}\\) output which each query a dabase of keys to compute the value using dot product \\[e_{q,k_i} = q\\cdot k_i\\]\nOnce the score is found then using softmax to compute the weights \\(V\\) \\(\\implies\\) value of encoder input \\(h_i\\) \\[\\alpha_{q,k_i} = softmax(e_{q,k_i})\\]\nCompute the attention with weight sum of value vector \\(V_{k_i}\\) \\(K\\) \\(\\implies\\) context vector \\(C_t\\)\n\n\\[attention(q,K,V) = \\sum_i^n \\alpha_{q,{k_i},V_{k_i}}\\]\nIn essence, when the generalized attention mechanism is presented with a sequence of words, it takes the query vector attributed to some specific word in the sequence and scores it against each key in the database. In doing so, it captures how the word under consideration relates to the others in the sequence. Then it scales the values according to the attention weights (computed from the scores) to retain focus on those words relevant to the query. In doing so, it produces an attention output for the word under consideration.\nExample of calculate attention by hand\n\ncalculate the attention for the first word in a sequence of four\n\nfrom numpy import array\nfrom numpy import random\nfrom numpy import dot\nfrom scipy.special import softmax\n\n# define encoder representations of four different words\nword_1 = array([1, 0, 0])\nword_2 = array([0, 1, 0])\nword_3 = array([1, 1, 0])\nword_4 = array([0, 0, 1])\n\n# stacking the word embeddings into a single array\nWords = array([word_1, word_2, word_3, word_4])\n\ngenerates the weight matrices to multiply to the word embeddings to generate the queries, keys, and values by hand instead of by training.\n\n# generating the weight matrices\nrandom.seed(42) # to allow us to reproduce the same attention values\nW_Q = random.randint(3, size=(3, 3))\nW_K = random.randint(3, size=(3, 3))\nW_V = random.randint(3, size=(3, 3))\n\ngenerate query,key, value vectors by multiply each word embedding with each weight matrices\n\n# generating the queries, keys and values\nQ = Words @ W_Q\nK = Words @ W_K\nV = Words @ W_V\n\ncreate scores the query vector using dot product\n\n# scoring the first query vector against all key vectors\nscores = Q @ K.transpose()\n\nGenerate the weight by send the scores value through softmax to avoid vanishing/exploding gradient divide the score with square root of dimension of the key vector\n\n# computing the weights by a softmax operation\nWeights = softmax(scores / K.shape[1] ** 0.5,axis=1)\n\nCalculate the attention by weighted sum of all value vectors\n\n# computing the attention by a weighted sum of the value vectors\nattention = Weights @ V\n \nprint(attention)"
  },
  {
    "objectID": "tutorial/nlps/word2vec.html",
    "href": "tutorial/nlps/word2vec.html",
    "title": "Word2vec",
    "section": "",
    "text": "a method to efficiently create word embeddings. It’s used to creating recommendation engines and making sense of sequential data even in commercial, non-language tasks.\n\n\nSince the computer is only capable of working on number. Any human language must be translate into number in order for the computer to understand and operated on them using mathematic method. Hence, the text is encoded into numbers after the computer operated on the text it is then decoded back into text that human can understand.\n\n\n\nA technique to find the similiarity of meaning of words to have similar representation. feature of the technique: \n\nThe meaning are learn from training from large amount of text\nUse a single learned vector to hold all values A word may have multiple meanings e.g. king mean power,man,… all these meanings are put into a vector represent “king”.\n\n[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n\nAnd different from one-hot encoding technique where mulitple dimensions matricies is required to represent word.\nrepresentation is based on usage e.g. similar way of usage result in similar representation in contrast with bag-of-word method which require explicit managing.\nBack by theory in Linguistic “distributional hypothesis” by Zellig Harris \n\n\n\nOne-Hot Encoding is one of the most widely used encoding methods in ML models. This technique is used to quantify categorical data where it compares each level of the categorical variable to a fixed reference level. It converts a single variable with n observations and x distinct values, to x binary variables with n observations where each observation denotes 1 as present and 0 as absent of the dichotomous binary variable.\n\n\n\n\nBefore a word can add to the vector it must be: - One-hot encodable so it can be mapped to the vector - Vector size is specified - Vector is randomly initialized\n\n\n\n\nAn architecture for creating word embeddings that uses \\(n\\) future words as well as \\(n\\) past words to create a word embedding. \\[J_0 = \\frac{1}{T}\\sum_{t=1}^{T}log\\space p(w_t|w_{t-n},...,w_{t-1},w_{t+1},...,w_{t+n})\\] The method is using the distributed representations of context (or surrounding words) are combined to predict the word in the middle of the sliding window.\nIf we have 4 context words used for predicting one target word the input layer will be in the form of four 1XW input vectors. These input vectors will be passed to the hidden layer where it is multiplied by a WXN matrix. Finally, the 1XN output from the hidden layer enters the sum layer where an element-wise summation is performed on the vectors before a final activation is performed and the output is obtained.\nIt’s a simple technique of using a single hidden layer to predict a target word at the output layer from a given context in the input layer. Two matrices, the input matrix I and output matrix O, are used to calculate the hidden layer H = C x I where C is the input vector,w is total words in the corpus,n is the dimension of hidden layer, and target words T = H x O. Finally, the softmax function is used to output the probability value between 0-1.\n\\[H_{1\\times n} = C_{1\\times w} \\times M_{w\\times n}\\] \\[T_{1\\times w} = H_{1\\times n} \\times O_{n\\times w}\\] \n**The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19.\nFurther, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission.\nThe reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.**\n\n\nimport numpy as np\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nimport gensim\ndata=open('/content/gdrive/My Drive/covid.txt','r')\ncorona_data = [text for text in data if text.count(' ') &gt;= 2]\nvectorize = Tokenizer()\nvectorize.fit_on_texts(corona_data)\ncorona_data = vectorize.texts_to_sequences(corona_data)\ntotal_vocab = sum(len(s) for s in corona_data)\nword_count = len(vectorize.word_index) + 1\nwindow_size = 2\n\ndef cbow_model(data, window_size, total_vocab):\n    total_length = window_size*2\n    for text in data:\n        text_len = len(text)\n        for idx, word in enumerate(text):\n            context_word = []\n            target   = []            \n            begin = idx - window_size\n            end = idx + window_size + 1\n            context_word.append([text[i] for i in range(begin, end) if 0 &lt;= i &lt; text_len and i != idx])\n            target.append(word)\n            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n            final_target = np_utils.to_categorical(target, total_vocab)\n            yield(contextual, final_target) \nFinally, it is time to build the neural network model that will train the CBOW on our sample data.\nmodel = Sequential()\nmodel.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\nmodel.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\nmodel.add(Dense(total_vocab, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nfor i in range(10):\n    cost = 0\n    for x, y in cbow_model(data, window_size, total_vocab):\n        cost += model.train_on_batch(contextual, final_target)\n    print(i, cost)\nafter training create a file that contains all the vectors.\ndimensions=100\nvect_file = open('/content/gdrive/My Drive/vectors.txt' ,'w')\nvect_file.write('{} {}\\n'.format(total_vocab,dimensions))\n\nweights = model.get_weights()[0]\nfor text, i in vectorize.word_index.items():\n    final_vec = ' '.join(map(str, list(weights[i, :])))\n    vect_file.write('{} {}\\n'.format(text, final_vec))\nvect_file.close()\nuse the vectors that were created and use them in the gensim model.\ncbow_output = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/vectors.txt', binary=False)\ncbow_output.most_similar(positive=['virus'])\n\n\n\n\n Predict the context word for a given target word the reverse of CBOW algorithm.\n\nIt uses a hidden layer to performs the dot product between the weight matrix and the input vector w(t). \nThere normally using 2 layers of neural network\nThese outputs is then become input to another dot product operation\nThe prediction error is used to modified the weights using backpropagation.\nThe final output is input to softmax function to output probability vector\nThere is no activation function involve at all.\n\n\n\n\nLet’s define some variables :\nV Number of unique words in our corpus of text ( Vocabulary ) x Input layer nx1 (One hot encoding of our input word ). N Number of neurons in the hidden layer of neural network W Weights matrix vxn between input layer and hidden layer W’ Weights matrix nxv between hidden layer and output layer y A softmax output vx1 layer having probabilities of every word in our vocabulary T the transpose h hidden vector nx1\n\\[softmax = \\frac{e^{(x_i - max(x_i))}}{\\sum_{i=1}^{n} e^{(x_i - max(x_i))}}\\] \\[h = W^T\\cdot x\\] \\[u = W^{\\prime T} \\cdot h\\]\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\nclass word2vec(object):\n    def __init__(self):\n        self.N = 10\n        self.X_train = []\n        self.y_train = []\n        self.window_size = 2\n        self.alpha = 0.001\n        self.words = []\n        self.word_index = {}\n\n    def initialize(self,V,data):\n        self.V = V\n        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n        self.W' = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n        \n        self.words = data\n        for i in range(len(data)):\n            self.word_index[data[i]] = i\n\n    def feed_forward(self,X):\n        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n        self.u = np.dot(self.W'.T,self.h)\n        #print(self.u)\n        self.y = softmax(self.u)\n        return self.y\n        \n    def backpropagate(self,x,t):\n        e = self.y - np.asarray(t).reshape(self.V,1)\n        # e.shape is V x 1\n        dLdW1 = np.dot(self.h,e.T)\n        X = np.array(x).reshape(self.V,1)\n        dLdW = np.dot(X, np.dot(self.W',e).T)\n        self.W' = self.W' - self.alpha*dLdW1\n        self.W = self.W - self.alpha*dLdW\n        \n    def train(self,epochs):\n        for x in range(1,epochs):   \n            self.loss = 0\n            for j in range(len(self.X_train)):\n                self.feed_forward(self.X_train[j])\n                self.backpropagate(self.X_train[j],self.y_train[j])\n                C = 0\n                for m in range(self.V):\n                    if(self.y_train[j][m]):\n                        self.loss += -1*self.u[m][0]\n                        C += 1\n                self.loss += C*np.log(np.sum(np.exp(self.u)))\n            print(\"epoch \",x, \" loss = \",self.loss)\n            self.alpha *= 1/( (1+self.alpha*x) )\n            \n    def predict(self,word,number_of_predictions):\n        if word in self.words:\n            index = self.word_index[word]\n            X = [0 for i in range(self.V)]\n            X[index] = 1\n            prediction = self.feed_forward(X)\n            output = {}\n            for i in range(self.V):\n                output[prediction[i][0]] = i\n            \n            top_context_words = []\n            for k in sorted(output,reverse=True):\n                top_context_words.append(self.words[output[k]])\n                if(len(top_context_words)&gt;=number_of_predictions):\n                    break\n    \n            return top_context_words\n        else:\n            print(\"Word not found in dictionary\")\n\ndef preprocessing(corpus):\n    stop_words = set(stopwords.words('english'))\n    training_data = []\n    sentences = corpus.split(\".\")\n    for i in range(len(sentences)):\n        sentences[i] = sentences[i].strip()\n        sentence = sentences[i].split()\n        x = [word.strip(string.punctuation) for word in sentence\n                                    if word not in stop_words]\n        x = [word.lower() for word in x]\n        training_data.append(x)\n    return training_data\n    \n\ndef prepare_data_for_training(sentences,w2v):\n    data = {}\n    for sentence in sentences:\n        for word in sentence:\n            if word not in data:\n                data[word] = 1\n            else:\n                data[word] += 1\n    V = len(data)\n    data = sorted(list(data.keys()))\n    vocab = {}\n    for i in range(len(data)):\n        vocab[data[i]] = i\n    \n    #for i in range(len(words)):\n    for sentence in sentences:\n        for i in range(len(sentence)):\n            center_word = [0 for x in range(V)]\n            center_word[vocab[sentence[i]]] = 1\n            context = [0 for x in range(V)]\n            \n            for j in range(i-w2v.window_size,i+w2v.window_size):\n                if i!=j and j&gt;=0 and j&lt;len(sentence):\n                    context[vocab[sentence[j]]] += 1\n            w2v.X_train.append(center_word)\n            w2v.y_train.append(context)\n    w2v.initialize(V,data)\n\n    return w2v.X_train,w2v.y_train\n\ncorpus = \"\"\ncorpus += \"The earth revolves around the sun. The moon revolves around the earth\"\nepochs = 1000\n\ntraining_data = preprocessing(corpus)\nw2v = word2vec()\n\nprepare_data_for_training(training_data,w2v)\nw2v.train(epochs)\n\nprint(w2v.predict(\"around\",3))\n\n\n\n\nA word embedding technique that is used to convert the words in the dataset to vectors so that the machine understands. Each unique word in your data is assigned to a vector and these vectors vary in dimensions depending on the length of the word.\nA statistical method for efficiently learning a standalone word embedding from a text corpus by analyze and math to exploit the “man-ness” from word “King” and “woman-ness” from word “Queen” to come up with “King is to queen as man to woman” to capture the semantic of the language. - The offset, angle between vector represent relationship between word e.g. “King-Man+woman = queen” - Allow others technique that focus on word usage location in predefind context sliding window of neighboring words to be used - Continous Bag-of-Words CBOW model e.g. predict current word based on context - Continous Skip-Gram model e.g. predict surrounding words based on current word\n - Have high efficiency of both space and time of computation allow the larger data set to be used\n\n\n\n\n\nAn approach that combine global statistics of matrix factorization (LSA Latent Semantic Analysis) with local context base learning for use with Word2vec. It uses the method of create explicit word-context matrix using statistic across the whole corpus.\n\n\n\n\nStandalone Train the model on embedding alone separately from other task and then use the model as part of other task\nJointly Train the model to learn embedding as part of other task which later used for only this model. ### Pretrain Embedding There are many freely available pretrained embedding model that can directly plug-in to your model\nStatic When a pretrained model has a good fit with your task it can be a direct component of your model\nUpdated When the pretrained embedding is seeded as parted of the model that will be updating during model training."
  },
  {
    "objectID": "tutorial/nlps/word2vec.html#text-conversion",
    "href": "tutorial/nlps/word2vec.html#text-conversion",
    "title": "Word2vec",
    "section": "",
    "text": "Since the computer is only capable of working on number. Any human language must be translate into number in order for the computer to understand and operated on them using mathematic method. Hence, the text is encoded into numbers after the computer operated on the text it is then decoded back into text that human can understand."
  },
  {
    "objectID": "tutorial/nlps/word2vec.html#embeddings-meaning",
    "href": "tutorial/nlps/word2vec.html#embeddings-meaning",
    "title": "Word2vec",
    "section": "",
    "text": "A technique to find the similiarity of meaning of words to have similar representation. feature of the technique: \n\nThe meaning are learn from training from large amount of text\nUse a single learned vector to hold all values A word may have multiple meanings e.g. king mean power,man,… all these meanings are put into a vector represent “king”.\n\n[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n\nAnd different from one-hot encoding technique where mulitple dimensions matricies is required to represent word.\nrepresentation is based on usage e.g. similar way of usage result in similar representation in contrast with bag-of-word method which require explicit managing.\nBack by theory in Linguistic “distributional hypothesis” by Zellig Harris \n\n\n\nOne-Hot Encoding is one of the most widely used encoding methods in ML models. This technique is used to quantify categorical data where it compares each level of the categorical variable to a fixed reference level. It converts a single variable with n observations and x distinct values, to x binary variables with n observations where each observation denotes 1 as present and 0 as absent of the dichotomous binary variable."
  },
  {
    "objectID": "tutorial/nlps/word2vec.html#embedding-layer",
    "href": "tutorial/nlps/word2vec.html#embedding-layer",
    "title": "Word2vec",
    "section": "",
    "text": "Before a word can add to the vector it must be: - One-hot encodable so it can be mapped to the vector - Vector size is specified - Vector is randomly initialized"
  },
  {
    "objectID": "tutorial/nlps/word2vec.html#cbow-continous-bag-of-words",
    "href": "tutorial/nlps/word2vec.html#cbow-continous-bag-of-words",
    "title": "Word2vec",
    "section": "",
    "text": "An architecture for creating word embeddings that uses \\(n\\) future words as well as \\(n\\) past words to create a word embedding. \\[J_0 = \\frac{1}{T}\\sum_{t=1}^{T}log\\space p(w_t|w_{t-n},...,w_{t-1},w_{t+1},...,w_{t+n})\\] The method is using the distributed representations of context (or surrounding words) are combined to predict the word in the middle of the sliding window.\nIf we have 4 context words used for predicting one target word the input layer will be in the form of four 1XW input vectors. These input vectors will be passed to the hidden layer where it is multiplied by a WXN matrix. Finally, the 1XN output from the hidden layer enters the sum layer where an element-wise summation is performed on the vectors before a final activation is performed and the output is obtained.\nIt’s a simple technique of using a single hidden layer to predict a target word at the output layer from a given context in the input layer. Two matrices, the input matrix I and output matrix O, are used to calculate the hidden layer H = C x I where C is the input vector,w is total words in the corpus,n is the dimension of hidden layer, and target words T = H x O. Finally, the softmax function is used to output the probability value between 0-1.\n\\[H_{1\\times n} = C_{1\\times w} \\times M_{w\\times n}\\] \\[T_{1\\times w} = H_{1\\times n} \\times O_{n\\times w}\\] \n**The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19.\nFurther, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission.\nThe reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.**\n\n\nimport numpy as np\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nimport gensim\ndata=open('/content/gdrive/My Drive/covid.txt','r')\ncorona_data = [text for text in data if text.count(' ') &gt;= 2]\nvectorize = Tokenizer()\nvectorize.fit_on_texts(corona_data)\ncorona_data = vectorize.texts_to_sequences(corona_data)\ntotal_vocab = sum(len(s) for s in corona_data)\nword_count = len(vectorize.word_index) + 1\nwindow_size = 2\n\ndef cbow_model(data, window_size, total_vocab):\n    total_length = window_size*2\n    for text in data:\n        text_len = len(text)\n        for idx, word in enumerate(text):\n            context_word = []\n            target   = []            \n            begin = idx - window_size\n            end = idx + window_size + 1\n            context_word.append([text[i] for i in range(begin, end) if 0 &lt;= i &lt; text_len and i != idx])\n            target.append(word)\n            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n            final_target = np_utils.to_categorical(target, total_vocab)\n            yield(contextual, final_target) \nFinally, it is time to build the neural network model that will train the CBOW on our sample data.\nmodel = Sequential()\nmodel.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\nmodel.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\nmodel.add(Dense(total_vocab, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nfor i in range(10):\n    cost = 0\n    for x, y in cbow_model(data, window_size, total_vocab):\n        cost += model.train_on_batch(contextual, final_target)\n    print(i, cost)\nafter training create a file that contains all the vectors.\ndimensions=100\nvect_file = open('/content/gdrive/My Drive/vectors.txt' ,'w')\nvect_file.write('{} {}\\n'.format(total_vocab,dimensions))\n\nweights = model.get_weights()[0]\nfor text, i in vectorize.word_index.items():\n    final_vec = ' '.join(map(str, list(weights[i, :])))\n    vect_file.write('{} {}\\n'.format(text, final_vec))\nvect_file.close()\nuse the vectors that were created and use them in the gensim model.\ncbow_output = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/vectors.txt', binary=False)\ncbow_output.most_similar(positive=['virus'])"
  },
  {
    "objectID": "tutorial/nlps/word2vec.html#skip-gram",
    "href": "tutorial/nlps/word2vec.html#skip-gram",
    "title": "Word2vec",
    "section": "",
    "text": "Predict the context word for a given target word the reverse of CBOW algorithm.\n\nIt uses a hidden layer to performs the dot product between the weight matrix and the input vector w(t). \nThere normally using 2 layers of neural network\nThese outputs is then become input to another dot product operation\nThe prediction error is used to modified the weights using backpropagation.\nThe final output is input to softmax function to output probability vector\nThere is no activation function involve at all.\n\n\n\n\nLet’s define some variables :\nV Number of unique words in our corpus of text ( Vocabulary ) x Input layer nx1 (One hot encoding of our input word ). N Number of neurons in the hidden layer of neural network W Weights matrix vxn between input layer and hidden layer W’ Weights matrix nxv between hidden layer and output layer y A softmax output vx1 layer having probabilities of every word in our vocabulary T the transpose h hidden vector nx1\n\\[softmax = \\frac{e^{(x_i - max(x_i))}}{\\sum_{i=1}^{n} e^{(x_i - max(x_i))}}\\] \\[h = W^T\\cdot x\\] \\[u = W^{\\prime T} \\cdot h\\]\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\nclass word2vec(object):\n    def __init__(self):\n        self.N = 10\n        self.X_train = []\n        self.y_train = []\n        self.window_size = 2\n        self.alpha = 0.001\n        self.words = []\n        self.word_index = {}\n\n    def initialize(self,V,data):\n        self.V = V\n        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n        self.W' = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n        \n        self.words = data\n        for i in range(len(data)):\n            self.word_index[data[i]] = i\n\n    def feed_forward(self,X):\n        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n        self.u = np.dot(self.W'.T,self.h)\n        #print(self.u)\n        self.y = softmax(self.u)\n        return self.y\n        \n    def backpropagate(self,x,t):\n        e = self.y - np.asarray(t).reshape(self.V,1)\n        # e.shape is V x 1\n        dLdW1 = np.dot(self.h,e.T)\n        X = np.array(x).reshape(self.V,1)\n        dLdW = np.dot(X, np.dot(self.W',e).T)\n        self.W' = self.W' - self.alpha*dLdW1\n        self.W = self.W - self.alpha*dLdW\n        \n    def train(self,epochs):\n        for x in range(1,epochs):   \n            self.loss = 0\n            for j in range(len(self.X_train)):\n                self.feed_forward(self.X_train[j])\n                self.backpropagate(self.X_train[j],self.y_train[j])\n                C = 0\n                for m in range(self.V):\n                    if(self.y_train[j][m]):\n                        self.loss += -1*self.u[m][0]\n                        C += 1\n                self.loss += C*np.log(np.sum(np.exp(self.u)))\n            print(\"epoch \",x, \" loss = \",self.loss)\n            self.alpha *= 1/( (1+self.alpha*x) )\n            \n    def predict(self,word,number_of_predictions):\n        if word in self.words:\n            index = self.word_index[word]\n            X = [0 for i in range(self.V)]\n            X[index] = 1\n            prediction = self.feed_forward(X)\n            output = {}\n            for i in range(self.V):\n                output[prediction[i][0]] = i\n            \n            top_context_words = []\n            for k in sorted(output,reverse=True):\n                top_context_words.append(self.words[output[k]])\n                if(len(top_context_words)&gt;=number_of_predictions):\n                    break\n    \n            return top_context_words\n        else:\n            print(\"Word not found in dictionary\")\n\ndef preprocessing(corpus):\n    stop_words = set(stopwords.words('english'))\n    training_data = []\n    sentences = corpus.split(\".\")\n    for i in range(len(sentences)):\n        sentences[i] = sentences[i].strip()\n        sentence = sentences[i].split()\n        x = [word.strip(string.punctuation) for word in sentence\n                                    if word not in stop_words]\n        x = [word.lower() for word in x]\n        training_data.append(x)\n    return training_data\n    \n\ndef prepare_data_for_training(sentences,w2v):\n    data = {}\n    for sentence in sentences:\n        for word in sentence:\n            if word not in data:\n                data[word] = 1\n            else:\n                data[word] += 1\n    V = len(data)\n    data = sorted(list(data.keys()))\n    vocab = {}\n    for i in range(len(data)):\n        vocab[data[i]] = i\n    \n    #for i in range(len(words)):\n    for sentence in sentences:\n        for i in range(len(sentence)):\n            center_word = [0 for x in range(V)]\n            center_word[vocab[sentence[i]]] = 1\n            context = [0 for x in range(V)]\n            \n            for j in range(i-w2v.window_size,i+w2v.window_size):\n                if i!=j and j&gt;=0 and j&lt;len(sentence):\n                    context[vocab[sentence[j]]] += 1\n            w2v.X_train.append(center_word)\n            w2v.y_train.append(context)\n    w2v.initialize(V,data)\n\n    return w2v.X_train,w2v.y_train\n\ncorpus = \"\"\ncorpus += \"The earth revolves around the sun. The moon revolves around the earth\"\nepochs = 1000\n\ntraining_data = preprocessing(corpus)\nw2v = word2vec()\n\nprepare_data_for_training(training_data,w2v)\nw2v.train(epochs)\n\nprint(w2v.predict(\"around\",3))"
  },
  {
    "objectID": "tutorial/nlps/word2vec.html#word2vec-1",
    "href": "tutorial/nlps/word2vec.html#word2vec-1",
    "title": "Word2vec",
    "section": "",
    "text": "A word embedding technique that is used to convert the words in the dataset to vectors so that the machine understands. Each unique word in your data is assigned to a vector and these vectors vary in dimensions depending on the length of the word.\nA statistical method for efficiently learning a standalone word embedding from a text corpus by analyze and math to exploit the “man-ness” from word “King” and “woman-ness” from word “Queen” to come up with “King is to queen as man to woman” to capture the semantic of the language. - The offset, angle between vector represent relationship between word e.g. “King-Man+woman = queen” - Allow others technique that focus on word usage location in predefind context sliding window of neighboring words to be used - Continous Bag-of-Words CBOW model e.g. predict current word based on context - Continous Skip-Gram model e.g. predict surrounding words based on current word\n - Have high efficiency of both space and time of computation allow the larger data set to be used"
  },
  {
    "objectID": "tutorial/nlps/word2vec.html#algorithm",
    "href": "tutorial/nlps/word2vec.html#algorithm",
    "title": "Word2vec",
    "section": "",
    "text": "An approach that combine global statistics of matrix factorization (LSA Latent Semantic Analysis) with local context base learning for use with Word2vec. It uses the method of create explicit word-context matrix using statistic across the whole corpus.\n\n\n\n\nStandalone Train the model on embedding alone separately from other task and then use the model as part of other task\nJointly Train the model to learn embedding as part of other task which later used for only this model. ### Pretrain Embedding There are many freely available pretrained embedding model that can directly plug-in to your model\nStatic When a pretrained model has a good fit with your task it can be a direct component of your model\nUpdated When the pretrained embedding is seeded as parted of the model that will be updating during model training."
  },
  {
    "objectID": "about.html#programming",
    "href": "about.html#programming",
    "title": "Vanh Phomsavanh",
    "section": "Programming",
    "text": "Programming\nI’ve been working as software engineer for two decades. And I’ve been using various programming languages starting from:\nC/C++\nC#\nJava\nKotlin\nJavascript\nRust\nJulia\nSwift\nPython\nI’m quite familiar with many OS and Frameworks such as"
  },
  {
    "objectID": "about.html#operating-system",
    "href": "about.html#operating-system",
    "title": "Vanh Phomsavanh",
    "section": "Operating System:",
    "text": "Operating System:\nWindows\nMacOS\nLinux\n## Frameworks:\n.Net\nAndroid"
  },
  {
    "objectID": "getting_started.html#beginner-start-here",
    "href": "getting_started.html#beginner-start-here",
    "title": "MachineIntell",
    "section": "",
    "text": "import nbdev"
  },
  {
    "objectID": "index.html#deep-learning-from-fastai-library",
    "href": "index.html#deep-learning-from-fastai-library",
    "title": "Machine intelligence",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nnbdev version was release"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Machine intelligence",
    "section": "",
    "text": "Solve age old differential equations help improve Machine Learning modeling\n\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\n\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\n\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\n\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\n\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\n\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "index.html#l",
    "href": "index.html#l",
    "title": "Machine intelligence",
    "section": "L",
    "text": "L\nSolve age old differential equations help improve Machine Learning modeling\n\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\n\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\n\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\n\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\n\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\n\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "index.html#lastest",
    "href": "index.html#lastest",
    "title": "Machine intelligence",
    "section": "Lastest",
    "text": "Lastest\nSolve age old differential equations help improve Machine Learning modeling\n\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\n\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\n\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\n\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\n\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\n\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "index.html#lastest-news",
    "href": "index.html#lastest-news",
    "title": "Machine Intelligence",
    "section": "Lastest news",
    "text": "Lastest news\nSolve age old differential equations help improve Machine Learning modeling\n\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\n\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\n\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\n\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\n\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\n\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "AI/news1.html#natural-language-processing",
    "href": "AI/news1.html#natural-language-processing",
    "title": "News 1",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "AI/news2.html#deep-learning-from-fastai-library",
    "href": "AI/news2.html#deep-learning-from-fastai-library",
    "title": "News 2",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nUsing fastai library is the fastest way to get your Machine Learning working with less head ache"
  },
  {
    "objectID": "AI/news2.html#natural-language-processing",
    "href": "AI/news2.html#natural-language-processing",
    "title": "News 2",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "AI/newsall.html",
    "href": "AI/newsall.html",
    "title": "Latest News",
    "section": "",
    "text": "Solve age old differential equations help improve Machine Learning modeling\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "AI/newsall.html#deep-learning-from-fastai-library",
    "href": "AI/newsall.html#deep-learning-from-fastai-library",
    "title": "Latest News",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nnbdev version was release"
  },
  {
    "objectID": "Tutorial/tutoring.html#linear-algebra-for-maching-learning",
    "href": "Tutorial/tutoring.html#linear-algebra-for-maching-learning",
    "title": "Tutorials",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "Tutorial/tutoring.html#differential-equation-in-machine-learning",
    "href": "Tutorial/tutoring.html#differential-equation-in-machine-learning",
    "title": "Tutorials",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "Tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "href": "Tutorial/nlps/nlp.html#linear-algebra-for-maching-learning",
    "title": "NLP",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "Tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "href": "Tutorial/nlps/nlp.html#differential-equation-in-machine-learning",
    "title": "NLP",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "Tutorial/math/diffEq/compoundint.html#ordinary-differential-equation",
    "href": "Tutorial/math/diffEq/compoundint.html#ordinary-differential-equation",
    "title": "ODE",
    "section": "Ordinary Differential Equation",
    "text": "Ordinary Differential Equation"
  },
  {
    "objectID": "Tutorial/math/diffEq/compoundint.html#julia-plots",
    "href": "Tutorial/math/diffEq/compoundint.html#julia-plots",
    "title": "ODE",
    "section": "Julia Plots",
    "text": "Julia Plots"
  },
  {
    "objectID": "Tutorial/math/diffEq/diffeq1.html",
    "href": "Tutorial/math/diffEq/diffeq1.html",
    "title": "Calculus",
    "section": "",
    "text": "A Mathematic branch that try to understand the changing world by study things that are continuouly changing. Calculus has two branches:"
  },
  {
    "objectID": "Tutorial/math/diffEq/diffeq1.html#differential-equation",
    "href": "Tutorial/math/diffEq/diffeq1.html#differential-equation",
    "title": "Calculus",
    "section": "Differential Equation",
    "text": "Differential Equation\nA branch of Differential Calculus that work with problem involving how some thing change affected other related thing is called Differential Equation. It try to understand how an independent variable \\(\\color{lime}{x}\\) change induce a dependent variable \\(\\color{lime}{y}\\) to change and try to find the solution to it. It is used in Physic, Economy, Biology etc…\nIn Mathematic Differential means a proces to find the derivative or slope or in lay term the rate of change. Differential Equation means an equation involving with thing that changing which has the derivative as part of the equation.\nThere are many types of Differential Equations here are a few of the main one are:\n\nOrdinary Differential Equation ODE\n\nPartial Differential Equations PDE\nNon-linear Differential Equations NLDE\n\nThe ODE dealing with only one independent variable. While Partial Differential Equation dealing with multiple independent variables and Non-linear Differential Equation work with non linear system.\n\nOrdinary Differential Equation (ODE)\nODE dealing with how a dependent variable changed with respect to a single independent variable changed(derivative).\n\n\n\n\n\n\nTip\n\n\n\nODE general equation form: \\(y^{(n)}= F\\left (x,y,y',\\ldots, y^{(n-1)} \\right )\\)\n\n\n\nProblem: compute the compound Interest of a bank account\nHere an example of ODE problem: figure out the compound interest of a bank account. A bank account has interest which accrues when the interest get added to original balance. What would be the banlance at the end of one year, five years or any day, month, year?\n\n\n\n\n\n\nTip\n\n\n\nsimplify form: \\(\\frac{du}{dt} = pu = f(u,p,t)\\)"
  },
  {
    "objectID": "Tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "href": "Tutorial/math/linearalgebra/lna1.html#linear-algebra-for-maching-learning",
    "title": "Linear Algebra",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nTo ask NN to analyze whetherthe picture is a dog or a cat these pictures must be digitized and feed to the NN. Machine Learning deal with large amount of data for example a picture of a dog taken with smart phone consist of thousand up to many million pixels. To feed this image to NN for analysis is must be transform into array of data that NN can use.\nIn Math the branch that work with array of data is called Linear Algebra and the field is most appropriate is matrices.\nArray of data may have multidimension. Name of array with different dimension:\n\nVector an array with one dimension or rank\nMatrix an array with two dimension or rank\nTensor an array with three or more dimension or rank\n\nOne of the most frequent uses operation in NN is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(C = A \\times B\\)\n\n\n\nFramework Provide tools\nAI framework such as Pytorch, TensorFlow, Flux have many of the math tools build in. Also many programming languages e.g. Python, Julia, R have support for Matrix operations build in to the language."
  },
  {
    "objectID": "Tutorial/math/nnMath/loss/loss.html#loss-functions",
    "href": "Tutorial/math/nnMath/loss/loss.html#loss-functions",
    "title": "Loss or Cost functions",
    "section": "Loss Functions",
    "text": "Loss Functions\nLoss functions are important in Neural Network. It’s at the heart of how the machine can learn by trial and error. The value calculate by this function is used to adjust the weight of the input data to nudge the NN to lean toward output the correct answer.\n\n\n\n\n\n\nTip\n\n\n\nLoss function refer a single value calculation, while cost function refers to whole or group of value combine together\n\n\n\nThere are many loss functions that work well for particular problem. So choosing the appropriate one is critical to get the machine the work and archieve your goal.\n\nCost functions\nThe cost function refer to the sum of all loss functions\n\n\nType of Loss function\nThere are many type of Loss functions. Each one is approriate for certain tasks understand how each of them work is essential to reach your result.\n\n\nCross Entropy Loss\nThe cross entropy loss for some target \\(\\color{orange}{ x}\\) and some prediction \\(\\color{orange}{ p(x)}\\) is given by:\n\\[ \\color{orange}{ H(x) = -\\sum x\\, \\log p(x) }\\]\nBut since our \\(\\color{orange}{x}\\) are 1-hot encoded, this can be rewritten as \\(\\color{orange}{ -\\log(p_{i})}\\) where \\(\\color{orange}{i}\\) is the index of the desired target.\nThis can be done using numpy-style integer array indexing. Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link.\n\n\nLogarithm\nUsing logarithm is important to get stable compute in practic. Since in actual compute the machine using floating point number to do caculation. And exact precision is depend on the number of bit uses. If the number of bit is too small the value compute is underflow or the carry bit is large the value will be overflow, on the other hand, if the number of bit is too much the bit are waste.\nLog can help speed up the compute by convert the multiplication and division which is slow into addition and substraction which is fast. Not only that in some case it may help avoid the overflow and underflow problem.\nNote that the formula\n\\[\\color{orange}{ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)}\\]\ngives a simplification when we compute the log softmax, which was previously defined as \\(\\color{orange}{ (x.exp()/(x.exp().sum(-1,keepdim=True))).log()}\\)\ndef log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n\ndef log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()\nThen, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula:\n\\[\\color{orange}{ \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )}\\]\nwhere \\(\\color{orange}{ a }\\) is the maximum of the \\(\\color{orange}{ x_{j}}\\).\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n    \ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n\nNegative Log Likelihood (NLL)\nWhen using logarithm to compute value that less than \\(\\color{chocolate}{1}\\) (probability) the result is negative value.\n\nlog 1 = 0\nlog .5 = -0.301\nlog .9 = -0.045\nlog .1 = -1\n\n\\[\\color{orange}{ \\log(p_{i}) = -p_{i}\\ \\ \\text{when}\\ i \\lt 1 }\\]\nTherefore the minus sign is used to convert it to positive value"
  },
  {
    "objectID": "Tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "href": "Tutorial/math/nnMath/nnmath.html#math-use-in-maching-learning",
    "title": "Neural Network Math",
    "section": "Math use in Maching Learning",
    "text": "Math use in Maching Learning\nHere are some essential math use in Machine Learning. It’s important to understand them in order to get insight into the inner working of the Neural Network. And how it come about the result.\n\nHere is a list of Math that is used in Neural Network:\n\nVector\nMatrix\nLoss functions\nCross-entropy loss\nsigmoid function\nsoftmax\nargmax function\nPartial derivative\ndifferential equation\n\n\n\n\nSoftmax (softargmax)\n\nSoftmax is one of the useful function in Neural Network computation. It’s allowed the data that output from the Neural Network which may not relate to one another be grouped into a single group and relate to each other as a posibility.\n\nIn mathematically term it’s a function that take the vector as input value and convert them to vector of output value and organized them as a probability value that sum to 1. The input value may be zero, negative, positive.\n\n\nSometime it is called multi-class logistic regression function. Since it’s used as final output for them. Many Neural Network output value that are not suitable for output so they must be convert using softmax.\n\nSoftmax equation is defined by:\n\\(\\color{coral}{ \\sigma : \\mathbb{R}^K\\to (0,1)^K}\\) is defined when \\(\\color{orange}{ K \\ge 1 }\\) by the formula\n\\[\\color{orange}{ \\hbox{softmax(x)}_{i} = \\sigma(x)_i   = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}} }\\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf x=(x_1,\\dotsc,x_K) \\in R^K  \\]\nor more concisely:\n\\[\\color{orange}{\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}} }\\] The input vector \\(\\color{orange}{x}\\) values are normalized by dividing each value in the vector by the sum of all values; this normalization ensures that the sum of all the components of the output vector \\(\\color{orange}{ \\sigma(x)}\\) is \\(\\color{orange}{= 1}\\).\n\n\nSoftmax and Sigmoid\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a generalization version of Sigmoid function and the graph is identical\n\n\nSoftmax is the generalize version of sigmoid function since softmax take vector as input and output a vector while sigmoid takes a scalar value and output a scalar. \\[\\color{orange}{\\hbox{sigmoid S(x)} = \\frac{1}{1 + e^{-x}} }\\] Sigmoid can have two possibility that must sum to \\(\\color{orange}{1}\\). When softmax has only two possibility then it is equal to sigmoid function.\n\n\n\nArgmax function\n\nThe argmax function convert all the value in the input vector to zero except the maximum value in that vector which it’s convert to one. The resulting vector contain mostly 0 except the max value that is one.\n\nThe argmax function can be considered as one-hot or look-up table representation of the output (assuming there is a unique maximum arg):\n\n\\[\\color{orange}{\\operatorname{arg\\,max}(x_1, \\dots, x_n) = (y_1, \\dots, y_n) = (0, \\dots, 0, 1, 0, \\dots, 0),}\\]\n\n\n\nSoftmax vs Argmax\nThe softmax can be considered as a smoother version of the arg max where the value in the output vector are either \\(\\color{orange}{0}\\) or \\(\\color{orange}{1}\\).\n\n\n\n\n\n\n\nTip\n\n\n\nsoftmax is a smoother verion of argmax\n\n\nHere is both softmax and argmax in one picture. The vector \\(\\color{orange}{v}\\) is softmax and vector \\(\\color{orange}{y}\\) is argmax\n\ncode\ncode in Python\nimport numpy as np\na = [1.0,2.0,3.0,4.0,1.0,3.0]\nnp.exp(a) / np.sum(np.exp(a))\ncode in Julia\nA = [1.0,2.0,3.0,4.0,1.0,3.0]\nexp.(A) ./ sum(exp.(A))"
  },
  {
    "objectID": "Tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "href": "Tutorial/math/maths.html#linear-algebra-for-maching-learning",
    "title": "Mathematic",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "Tutorial/math/maths.html#differential-equation-in-machine-learning",
    "href": "Tutorial/math/maths.html#differential-equation-in-machine-learning",
    "title": "Mathematic",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "Tutorial/nlps/rnn.html",
    "href": "Tutorial/nlps/rnn.html",
    "title": "Recurrent Neural Network",
    "section": "",
    "text": "The Markov model weakness is that it’s limit to context window(scope) that was choosen. Using the info that was stored to predict prior data then feed that info to model while it was considering current tag."
  },
  {
    "objectID": "Tutorial/nlps/rnn.html#bidirectional-rnn",
    "href": "Tutorial/nlps/rnn.html#bidirectional-rnn",
    "title": "Recurrent Neural Network",
    "section": "Bidirectional RNN",
    "text": "Bidirectional RNN\nA technique to train two independent RNN where one process from start to end the other process from end to start then combine the output from both into single one"
  },
  {
    "objectID": "Tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "href": "Tutorial/nlps/rnn.html#long-short-term-memory-lstm",
    "title": "Recurrent Neural Network",
    "section": "Long short term memory (LSTM)",
    "text": "Long short term memory (LSTM)\n\nAn RNN that has the capability to forget the info that is not relevant to the current task.\n\n\nLSTM\n\nforget gate to delete info of non relevant from current context\nadd gate to select new info into current context with tanh activation that indicate the direction of info(should care about) and a sigmoid to indicate the scaling(how much should be care about) factor of the info to be add to forget gate to produce state context\nout gate with sigmoid combine with state context to output result"
  },
  {
    "objectID": "Tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "href": "Tutorial/nlps/rnn.html#creating-a-language-model-from-scratch",
    "title": "Recurrent Neural Network",
    "section": "Creating a Language Model from Scratch",
    "text": "Creating a Language Model from Scratch\nA language model is a model that predict the next word in the sentence.\n\nFirst Language Model\nBuild a model to predict each word based on the previous three words by create a list of every sequence of three words as independent variables, and the next word after each sequence as the dependent variable.\nThe model takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. It use the standard three linear layers, but with two tweaks.\n\nThe first linear layer will use only the first word’s embedding as activations,\n\nThe second layer will use the second word’s embedding plus the first layer’s output activations, and\nThe third layer will use the third word’s embedding plus the second layer’s output activations.\nThe key effect of this is that every word is interpreted in the information context of any words preceding it.\n\nEach of these three layers will use the same weight matrix\n\nThe way that one word impacts the activations from previous words should not change depending on the position of a word.\nActivation values will change as data moves through the layers, but the layer’s weights themselves will not change from layer to layer.\nSo, a layer does not learn one sequence position; it must learn to handle all positions."
  },
  {
    "objectID": "Tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "href": "Tutorial/nlps/rnn.html#the-architect-of-the-first-model",
    "title": "Recurrent Neural Network",
    "section": "The architect of the first model",
    "text": "The architect of the first model\n\nHere the figure the model. Where word is the input, FC is fully connected layer and triangular is output prediction\n\n3 layers model code\nThe first cut of the code for 3 layers model use:\n\nThe embedding layer (input2_hidden, for input to hidden)\nThe linear layer to create the activations for the next word (hidden2_hidden, for hidden to hidden)\nA final linear layer to predict the fourth word (hidden2_output, for hidden to output)\n\nThey all use the same embedding since they come from same data\nclass LMModel1(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n  # input2_hidden is embedding layer\n  # hidden2_hidden is linear layer\n  # hidden2_output is linear layer   \n    def forward(self, x):\n        h = F.relu(self.hidden2_hidden(self.input2_hidden(x[:,0])))\n        h = h + self.input2_hidden(x[:,1])\n        h = F.relu(self.hidden2_hidden(h))\n        h = h + self.input2_hidden(x[:,2])\n        h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\n\nL((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))\n# tensor of numericalized value for model\nseqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\nseqs\nbs = 64\ncut = int(len(seqs) * 0.8)\n# create batch\ndls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n\nlearn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)   \ncompare to the simplest model which always predict the next word which is ‘thousand’ to see how it performs:\n# a simplest model that always predict 'thousand' on each input sentence\nc = Counter(tokens[cut:])\nmc = c.most_common(5)\nmc\nmc[0][1] / len(tokens[cut:])\n\nn,counts = 0,torch.zeros(len(vocab))\nfor x,y in dls.valid:\n    n += y.shape[0]\n    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n#index of the most common words ('thousand')\nidx = torch.argmax(counts)\nidx, vocab[idx.item()], counts[idx].item()/n\n\nRefactor to use loop:\nThe RNN \nRewrite the code to use loop this is look closer to RNN\nclass LMModel2(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n        \n    # refactor to use for loop the RNN!\n    def forward(self, x):\n        h = 0.               # using broascast\n        for i in range(3):\n            h = h + self.input2_hidden(x[:,i])\n            h = F.relu(self.hidden2_hidden(h))\n        return self.hidden2_output(h)\n\nlearn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 1e-3)\n\n\n\nRefactor to add memory to RNN\nAdd the ability to retain previous word instead of start up new every time \nclass LMModel3(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.input2_hidden = nn.Embedding(vocab_sz, n_hidden)  \n        self.hidden2_hidden = nn.Linear(n_hidden, n_hidden)     \n        self.hidden2_output = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0.  # using broascast\n        \n    # refactor to use for loop the RNN!\n    def forward(self, x):\n                   \n        for i in range(3):\n            self.h = self.h + self.input2_hidden(x[:,i])\n            self.h = F.relu(self.hidden2_hidden(h))\n            \n        out = self.hidden2_output(self.h)\n        self.h = self.h.detach() # do bptt\n        return out\n    def reset(self): self.h = 0.\n\n\n\nBPTT Backpropagation through time\nThis model hidden state now can remember previous activation from previous batch and the gradient will be calculate on sequence length token of the past not instead of recalculate with each new stream this is call BPTT\n# rearrange data so model see in particular sequence\nm = len(seqs)//bs\nm,bs,len(seqs)\n\n# reindex model see as contiguous batch with each epoch\ndef group_chunks(ds, bs):\n    m = len(ds) // bs\n    new_ds = L()\n    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n    return new_ds\n\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(\n    group_chunks(seqs[:cut], bs), \n    group_chunks(seqs[cut:], bs), \n    bs=bs, drop_last=True, # drop last batch that have diff shape\n    shuffle=False) # maintain sequence\n\nlearn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n                metrics=accuracy, \n                cbs=ModelResetter) # callback to reset each epoch\nlearn.fit_one_cycle(10, 3e-3)\nAdd More signal: keep the output\nThe model no longer throw away the output from previous run but add them as input to current run\nsl = 16\nseqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n         for i in range(0,len(nums)-sl-1,sl))\ncut = int(len(seqs) * 0.8)\ndls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n                             group_chunks(seqs[cut:], bs),\n                             bs=bs, drop_last=True, shuffle=False)\n\n# check if it still offset by 1\n[L(vocab[o] for o in s) for s in seqs[0]]\nRewrite the model to now output every word instead of every 3 words in order to feed this into next run\nclass LMModel4(Module):\n    def __init__(self, vocab_sz, n_hidden):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n        self.h_h = nn.Linear(n_hidden, n_hidden)     \n        self.h_o = nn.Linear(n_hidden,vocab_sz)\n        self.h = 0\n        \n    def forward(self, x):\n        outs = []\n        for i in range(sl):\n            self.h = self.h + self.i_h(x[:,i])\n            self.h = F.relu(self.h_h(self.h))\n            outs.append(self.h_o(self.h))\n        self.h = self.h.detach()\n        return torch.stack(outs, dim=1)\n    \n    def reset(self): self.h = 0\n\n    def loss_func(inp, targ):\n        return F.cross_entropy(inp.view(-1, len(vocab)), # flatten out to match bs x sl x vocab_sz from model\n        targ.view(-1)) # flatten out to match bs x sl x vocab_sz from model\n\nlearn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)     \n\n```python\n\n### Recurrent Neural Network: RNN\n![](./image/rnn-pic1.jpeg)\nRNN feed the output activation value back into hidden layer to help the hidden layers retain info about previous run therefore have some *memory* of the past.\n\n### Multi-Layer RNN\nsince the current model use the same weight matrix for each hidden layer which mean there no new info to be learn from. One way to \nTo improve the model further is to stack more layers by feed the output from one layer into the next layer so on.\n\n\nLook at it in unrolling way\n![stack-layer](./image/stacklayer_rnn2.png)\n\nRefactoring to use PyTorch\n\nThe model now has too deep layers this could lead to problem of *vanishing* or *exploding* gradient\n\n```python\nclass LMModel5(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = torch.zeros(n_layers, bs, n_hidden)\n\n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = h.detach()\n        return self.h_o(res)\n\n    def reset(self): self.h.zero_()\n\nlearn = Learner(dls, LMModel5(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 3e-3)\n\n\nExploding or Disappearing Activations\nThe problem stem from the gradient value calcutated from the output layer won’t propagated back to the earlier layer. This is because the network is too deep.\nVanishing Gradient:\nAs the gradient value travel backward the especially the small value is diminishing as the floating point value get computed and recomputed many time each time it get round off closer and closer to 0 and finally become 0.\nExploding Gradient:\nThis the opposite of vanishing gradient. This phenomena happen espcially when the large value get larger with each computation it get exponentially large until it get large closer to infinity and become useless. \n\n\nThe floating point problem\nOne problem with doing floating point math with computer is that computer has limit amount of storage and floating point number is imprecise e.g. 1.0000012 or 1.00000000000000000000000012 is these two numbers are the same value? The answer is it’s depend on what do you want to do with it. On some task the first one is good enough but on other the second is what require. To store the second number require more storage.\nThe impreciseness is rather harder to understand. Float number get denser and denser when value get closer and closer to 0. In practice how do you represent this to the computer so it do some useful work. The IEEE organization come up with a standard which require how much storage should be devoted to certain type of floating point value e.g. 8-bit folating point,16-bit, 32-bit which mean the storage depend on the precision the user requirement not the actual value it may case the value get round-off read more from here. This mean do more calculation may lead less precision. This is what happen when doing lot and lot of gradient calculations the value may end up at 0 which is call vanishing gradient or the value get larger and larger exponentially or explode to infinite.\nThese problems are the main reason why RNN model is hard to train than CNN model,however research is very active to try new way to reduce or avoid these problems.\n\n\nNeural Network that have Memory\nIn Machine Learning two approach to handle Explode and Vanishing gradient is to use Neural network that can retain previous activation and also can figure out what value should be retain and which should be removed. The two popular approch are LSTM and GRU. GRU is a simplify version of LSTM\n\n\n\nComparison LSTM and GRU\n\n\n\nLSTM architecture\nInternally LSTM comprise with little neural net that decise how much gradient is needed and which one to update or delete. \nsigmoid equation:\n\\(\\color{orange}{ f(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1} = \\frac12 + \\frac12 \\tanh\\left(\\frac{x}{2}\\right)}\\)\nSigmoid only let positive value between 0 and 1 pass through\ntanh equation:\n\\(\\color{orange}{ f(x) = \\tanh x = \\frac{e^x-e^{-x}}{e^x+e^{-x}}}\\)\nTanh only let value between -1 and 1 pass through\n\n\nAnother look at LSTM internal:\n\nThe little NN is compose of gates call forget gate,input gate, cell gate, output gate. These gates work together to provide LSTM the capability to remember activation value that is important and forget the unneccessary activation value\n\nThe forget \\(\\color{orange}{ f_{t}}\\) gate:\nTake input \\(\\color{orange}{ x}\\), hidden state \\(\\color{orange}{ h_{t-1}}\\) then gated them via the sigmoid \\(\\color{orange}{ \\sigma}\\) activation to get only positive value then multiply them with previous cell state(memory) \\(\\color{orange}{ C_{t-1}}\\). It decides should the value be kept or discarded. If result from \\(\\color{orange}{ \\sigma}\\) value closer to 1 the value is kept else the value is discarded.\n\n\nThe input \\(\\color{orange}{ i_{t}}\\)gate:\nTogether with cell gate to update the cell state this gate also decide should the value be kept or discard depend on the value of the sigmoid\n\n\nThe cell \\(\\color{orange}{ \\tilde{C_{t}}}\\) gate:\nDecide what value to update from the range of -1 to 1 output from tanh function the value then add with previou cell state \\(\\color{orange}{ C_{t-1}}\\) value to get \\(\\color{orange}{ C_{t}}\\) the new value in memeory\n\n\nThe Output \\(\\color{orange}{ o_{t}}\\) gate:\nDecide which value to output to be multiply with the cell state value that was filter with tanh before output as new hidden state for the next layer.\nThe code for LSTM cell:\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.forget_gate = nn.Linear(ni + nh, nh)\n        self.input_gate  = nn.Linear(ni + nh, nh)\n        self.cell_gate   = nn.Linear(ni + nh, nh)\n        self.output_gate = nn.Linear(ni + nh, nh)\n\n    def forward(self, input, state):\n        h,c = state\n        h = torch.stack([h, input], dim=1)\n        forget = torch.sigmoid(self.forget_gate(h))\n        c = c * forget\n        inp = torch.sigmoid(self.input_gate(h))\n        cell = torch.tanh(self.cell_gate(h))\n        c = c + inp * cell\n        out = torch.sigmoid(self.output_gate(h))\n        h = outgate * torch.tanh(c)\n        return h, (h,c)\nRefactor to take advantage of the GPU by combine all small matrix into one big one to avoid moving too frequence data in and out of GPU and allow GPU to parallelize the task.\nclass LSTMCell(Module):\n    def __init__(self, ni, nh):\n        self.ih = nn.Linear(ni,4*nh)\n        self.hh = nn.Linear(nh,4*nh)\n\n    def forward(self, input, state):\n        h,c = state\n        # One big multiplication for all the gates is better than 4 smaller ones\n        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) #split tensor into 4 then combine with input\n        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n        cellgate = gates[3].tanh()\n\n        c = (forgetgate*c) + (ingate*cellgate)\n        h = outgate * c.tanh()\n        return h, (h,c)\n\n\n\nTrain the LSTM\nclass LMModel6(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        res,h = self.rnn(self.i_h(x), self.h)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(res)\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = Learner(dls, LMModel6(len(vocab), 64, 2), \n                loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy, cbs=ModelResetter)\nlearn.fit_one_cycle(15, 1e-2)\n\n\nRegularizing an LSTM\nAlthough hard to train and prone to overfitting, there some trick such as using data augmentation by translate the same text into different language then retranslate back to generate more variance. And using various regularization method to alleviate the overfitting problem some of the techniques describe here:\n\n\nDropout\n Dropout is one of the regularization technique use to combat overfitting tendency of the model. The method usually apply at training time. This method is to randomly change some activations value to zero which temporary remove the neural nodes from the network.\nIt makes the neural less relie on the input from the source that the neural regularly receive the input from since these sources may not be there. It makes sure all neurons actively work toward the general concept rather than try to fit specify pattern in the current data.\nDropout has different behavior in training and validation mode which is why the flag training must set to True when traing and False when evaluating.\nclass Dropout(Module):\n    def __init__(self, p): self.p = p\n    def forward(self, x):\n        if not self.training: return x\n        mask = x.new(*x.shape).bernoulli_(1-p) # create probability of random value \n        return x * mask.div_(1-p)\n\n\nActivation Regularization (AR) and Temporal Activation Regularization (TAR)\ntwo regularization methods very similar to weight decay.\n\nAR\nThis approach is apply at the final activation from LSTM to reduce its size. AR is often applied on the dropped-out activations. The code is\nloss += alpha * activations.pow(2).mean()\n\n\nTAR\nThis approach is to encourage the model to output sensible value by adding a penalty to the loss to make the difference between two consecutive activations as small as possible. TAR is applied on the non-dropped-out activations (because those zeros create big differences between two consecutive time steps) with activations tensor has a shape bs x sl x n_hid the code is:\nloss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\nTo use these is required:\n\nthe proper output,\nthe activations of the LSTM pre-dropout, and\nthe activations of the LSTM post-dropout\n\nIn practive it’s often used a callback RNNRegularizer to apply the regularization.\n\n\n\nTraining AWD-LSTM: a Weight-Tied Regularized LSTM\nApply regularization can be combined together dropout, AR, TAR This method uses: - Embedding dropout (just after the embedding layer) - Input dropout (after the embedding layer) - Weight dropout (applied to the weights of the LSTM at each training step) - Hidden dropout (applied to the hidden state between two layers)\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\nAnother trick is weight-tying  by realize that input embedding is a mapping from English words to activation value. And output from hidden layer is a mapping from activations value to English words are the same thing. And assign the same weight matrix to these layersself.h_o.weight  self.i_h.weight\nThe final code tweak become:\nclass LMModel7(Module):\n    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n        self.drop = nn.Dropout(p)\n        self.h_o = nn.Linear(n_hidden, vocab_sz)\n        self.h_o.weight = self.i_h.weight\n        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n        \n    def forward(self, x):\n        raw,h = self.rnn(self.i_h(x), self.h)\n        out = self.drop(raw)\n        self.h = [h_.detach() for h_ in h]\n        return self.h_o(out),raw,out\n    \n    def reset(self): \n        for h in self.h: h.zero_()\n\nlearn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n                cbs=[ModelResetter,  # add callback\n                RNNRegularizer(alpha=2, beta=1)]) # add callback to learner\n\n# or use the TextLearner that will call add the callback\nlearn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)                       \n\nlearn.fit_one_cycle(15, 1e-2, wd=0.1)\n\n\nGRU: Gate Recurrent Units architecture\n\nGRU is a simplify version of LSTM and work them same way."
  },
  {
    "objectID": "Tutorial/nlps/rnn.html#rnn-application",
    "href": "Tutorial/nlps/rnn.html#rnn-application",
    "title": "Recurrent Neural Network",
    "section": "RNN application",
    "text": "RNN application\n\nPOS\nNER\nDeidentification\nTranslation\nsequence-to-sequence\nchatbot\nquestion-answer\nsequence classification\nsentiment\n\n\nJargons:\n\nhidden state: The activations that are updated at each step of a recurrent neural network.\nA neural network that is defined using a loop like this is called a recurrent neural network (RNN)\nBack propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which “detaches” the history of computation steps in the hidden state every few time steps. In essence calculate the gradient and propagate backward as usual but don’t store them.\nThe bernoulli_ method is creating a tensor of random zeros (with probability p) and ones (with probability 1-p)\nalpha and beta are two hyperparameters to tune"
  },
  {
    "objectID": "Tutorial/nlps/transformer.html",
    "href": "Tutorial/nlps/transformer.html",
    "title": "Transformers",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "Tutorial/nlps/attention.html",
    "href": "Tutorial/nlps/attention.html",
    "title": "Attention Mechanism",
    "section": "",
    "text": "A technique to help improve the performance of the Encoder-Decoder by increase the flexibility to use the whole set of input data instead of just the data that are closer to it. By a weighted from all the input data vector plus the ability to assigned the most relevant data the higher weight. This allow the model to pick the most appropiate data for the task at hand.\nThis method also wisely uses in other discipline e.g. vision, not just in language translation.\nWhat are the components of Attention\nAttention is divided into three parts"
  },
  {
    "objectID": "Tutorial/nlps/attention.html#computing-attention",
    "href": "Tutorial/nlps/attention.html#computing-attention",
    "title": "Attention Mechanism",
    "section": "Computing attention",
    "text": "Computing attention\n The amount of attention that the network should be paying attention to is computed with\n\\(\\alpha^{&lt;t,t'&gt;} = \\text{amount of attention } y^{t} \\text{ should pay to }\\alpha^{t'}\\) \\(\\alpha^{&lt;t,t'&gt;} = \\frac{exp(e^{t,t'})}{\\sum_{t' = 1}^{T_x} exp(e^{&lt;t,t'&gt;}) }\\)\nwhere \\(\\alpha\\) is attention, \\(S\\) is state, \\(t\\) is time step, \\(t'\\) is time step of next input\ncompute \\(e^{&lt;t,t'&gt;}\\) using a small network.\nThe drawback of this algorithm is depended on the amount of \\(T_x\\) tokens it could run in quadratic time.\nVisualization of attention at work"
  },
  {
    "objectID": "Tutorial/nlps/attention.html#general-attention-mechanism-with-seq2seq",
    "href": "Tutorial/nlps/attention.html#general-attention-mechanism-with-seq2seq",
    "title": "Attention Mechanism",
    "section": "General Attention Mechanism with Seq2Seq",
    "text": "General Attention Mechanism with Seq2Seq\nThe general task of find item from a sequence to sequence can be compared to attention mechanism that have the three component as query \\(Q\\),keys \\(K\\), and value \\(V\\). As below:\n\n\\(Q\\) \\(\\implies\\) querying previous state decoder \\(S_{t-1}\\) output which each query a dabase of keys to compute the value using dot product \\[e_{q,k_i} = q\\cdot k_i\\]\nOnce the score is found then using softmax to compute the weights \\(V\\) \\(\\implies\\) value of encoder input \\(h_i\\) \\[\\alpha_{q,k_i} = softmax(e_{q,k_i})\\]\nCompute the attention with weight sum of value vector \\(V_{k_i}\\) \\(K\\) \\(\\implies\\) context vector \\(C_t\\)\n\n\\[attention(q,K,V) = \\sum_i^n \\alpha_{q,{k_i},V_{k_i}}\\]\nIn essence, when the generalized attention mechanism is presented with a sequence of words, it takes the query vector attributed to some specific word in the sequence and scores it against each key in the database. In doing so, it captures how the word under consideration relates to the others in the sequence. Then it scales the values according to the attention weights (computed from the scores) to retain focus on those words relevant to the query. In doing so, it produces an attention output for the word under consideration.\nExample of calculate attention by hand\n\ncalculate the attention for the first word in a sequence of four\n\nfrom numpy import array\nfrom numpy import random\nfrom numpy import dot\nfrom scipy.special import softmax\n\n# define encoder representations of four different words\nword_1 = array([1, 0, 0])\nword_2 = array([0, 1, 0])\nword_3 = array([1, 1, 0])\nword_4 = array([0, 0, 1])\n\n# stacking the word embeddings into a single array\nWords = array([word_1, word_2, word_3, word_4])\n\ngenerates the weight matrices to multiply to the word embeddings to generate the queries, keys, and values by hand instead of by training.\n\n# generating the weight matrices\nrandom.seed(42) # to allow us to reproduce the same attention values\nW_Q = random.randint(3, size=(3, 3))\nW_K = random.randint(3, size=(3, 3))\nW_V = random.randint(3, size=(3, 3))\n\ngenerate query,key, value vectors by multiply each word embedding with each weight matrices\n\n# generating the queries, keys and values\nQ = Words @ W_Q\nK = Words @ W_K\nV = Words @ W_V\n\ncreate scores the query vector using dot product\n\n# scoring the first query vector against all key vectors\nscores = Q @ K.transpose()\n\nGenerate the weight by send the scores value through softmax to avoid vanishing/exploding gradient divide the score with square root of dimension of the key vector\n\n# computing the weights by a softmax operation\nWeights = softmax(scores / K.shape[1] ** 0.5,axis=1)\n\nCalculate the attention by weighted sum of all value vectors\n\n# computing the attention by a weighted sum of the value vectors\nattention = Weights @ V\n \nprint(attention)"
  },
  {
    "objectID": "Tutorial/nlps/encodedecode.html",
    "href": "Tutorial/nlps/encodedecode.html",
    "title": "Encoder-Decoder pair in NLP",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html",
    "href": "Tutorial/nlps/word2vec.html",
    "title": "Word2vec",
    "section": "",
    "text": "a method to efficiently create word embeddings. It’s used to creating recommendation engines and making sense of sequential data even in commercial, non-language tasks.\n\n\nSince the computer is only capable of working on number. Any human language must be translate into number in order for the computer to understand and operated on them using mathematic method. Hence, the text is encoded into numbers after the computer operated on the text it is then decoded back into text that human can understand.\n\n\n\nA technique to find the similiarity of meaning of words to have similar representation. feature of the technique: \n\nThe meaning are learn from training from large amount of text\nUse a single learned vector to hold all values A word may have multiple meanings e.g. king mean power,man,… all these meanings are put into a vector represent “king”.\n\n[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n\nAnd different from one-hot encoding technique where mulitple dimensions matricies is required to represent word.\nrepresentation is based on usage e.g. similar way of usage result in similar representation in contrast with bag-of-word method which require explicit managing.\nBack by theory in Linguistic “distributional hypothesis” by Zellig Harris \n\n\n\nOne-Hot Encoding is one of the most widely used encoding methods in ML models. This technique is used to quantify categorical data where it compares each level of the categorical variable to a fixed reference level. It converts a single variable with n observations and x distinct values, to x binary variables with n observations where each observation denotes 1 as present and 0 as absent of the dichotomous binary variable.\n\n\n\n\nBefore a word can add to the vector it must be: - One-hot encodable so it can be mapped to the vector - Vector size is specified - Vector is randomly initialized\n\n\n\n\nAn architecture for creating word embeddings that uses \\(n\\) future words as well as \\(n\\) past words to create a word embedding. \\[J_0 = \\frac{1}{T}\\sum_{t=1}^{T}log\\space p(w_t|w_{t-n},...,w_{t-1},w_{t+1},...,w_{t+n})\\] The method is using the distributed representations of context (or surrounding words) are combined to predict the word in the middle of the sliding window.\nIf we have 4 context words used for predicting one target word the input layer will be in the form of four 1XW input vectors. These input vectors will be passed to the hidden layer where it is multiplied by a WXN matrix. Finally, the 1XN output from the hidden layer enters the sum layer where an element-wise summation is performed on the vectors before a final activation is performed and the output is obtained.\nIt’s a simple technique of using a single hidden layer to predict a target word at the output layer from a given context in the input layer. Two matrices, the input matrix I and output matrix O, are used to calculate the hidden layer H = C x I where C is the input vector,w is total words in the corpus,n is the dimension of hidden layer, and target words T = H x O. Finally, the softmax function is used to output the probability value between 0-1.\n\\[H_{1\\times n} = C_{1\\times w} \\times M_{w\\times n}\\] \\[T_{1\\times w} = H_{1\\times n} \\times O_{n\\times w}\\] \n**The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19.\nFurther, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission.\nThe reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.**\n\n\nimport numpy as np\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nimport gensim\ndata=open('/content/gdrive/My Drive/covid.txt','r')\ncorona_data = [text for text in data if text.count(' ') &gt;= 2]\nvectorize = Tokenizer()\nvectorize.fit_on_texts(corona_data)\ncorona_data = vectorize.texts_to_sequences(corona_data)\ntotal_vocab = sum(len(s) for s in corona_data)\nword_count = len(vectorize.word_index) + 1\nwindow_size = 2\n\ndef cbow_model(data, window_size, total_vocab):\n    total_length = window_size*2\n    for text in data:\n        text_len = len(text)\n        for idx, word in enumerate(text):\n            context_word = []\n            target   = []            \n            begin = idx - window_size\n            end = idx + window_size + 1\n            context_word.append([text[i] for i in range(begin, end) if 0 &lt;= i &lt; text_len and i != idx])\n            target.append(word)\n            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n            final_target = np_utils.to_categorical(target, total_vocab)\n            yield(contextual, final_target) \nFinally, it is time to build the neural network model that will train the CBOW on our sample data.\nmodel = Sequential()\nmodel.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\nmodel.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\nmodel.add(Dense(total_vocab, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nfor i in range(10):\n    cost = 0\n    for x, y in cbow_model(data, window_size, total_vocab):\n        cost += model.train_on_batch(contextual, final_target)\n    print(i, cost)\nafter training create a file that contains all the vectors.\ndimensions=100\nvect_file = open('/content/gdrive/My Drive/vectors.txt' ,'w')\nvect_file.write('{} {}\\n'.format(total_vocab,dimensions))\n\nweights = model.get_weights()[0]\nfor text, i in vectorize.word_index.items():\n    final_vec = ' '.join(map(str, list(weights[i, :])))\n    vect_file.write('{} {}\\n'.format(text, final_vec))\nvect_file.close()\nuse the vectors that were created and use them in the gensim model.\ncbow_output = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/vectors.txt', binary=False)\ncbow_output.most_similar(positive=['virus'])\n\n\n\n\n Predict the context word for a given target word the reverse of CBOW algorithm.\n\nIt uses a hidden layer to performs the dot product between the weight matrix and the input vector w(t). \nThere normally using 2 layers of neural network\nThese outputs is then become input to another dot product operation\nThe prediction error is used to modified the weights using backpropagation.\nThe final output is input to softmax function to output probability vector\nThere is no activation function involve at all.\n\n\n\n\nLet’s define some variables :\nV Number of unique words in our corpus of text ( Vocabulary ) x Input layer nx1 (One hot encoding of our input word ). N Number of neurons in the hidden layer of neural network W Weights matrix vxn between input layer and hidden layer W’ Weights matrix nxv between hidden layer and output layer y A softmax output vx1 layer having probabilities of every word in our vocabulary T the transpose h hidden vector nx1\n\\[softmax = \\frac{e^{(x_i - max(x_i))}}{\\sum_{i=1}^{n} e^{(x_i - max(x_i))}}\\] \\[h = W^T\\cdot x\\] \\[u = W^{\\prime T} \\cdot h\\]\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\nclass word2vec(object):\n    def __init__(self):\n        self.N = 10\n        self.X_train = []\n        self.y_train = []\n        self.window_size = 2\n        self.alpha = 0.001\n        self.words = []\n        self.word_index = {}\n\n    def initialize(self,V,data):\n        self.V = V\n        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n        self.W' = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n        \n        self.words = data\n        for i in range(len(data)):\n            self.word_index[data[i]] = i\n\n    def feed_forward(self,X):\n        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n        self.u = np.dot(self.W'.T,self.h)\n        #print(self.u)\n        self.y = softmax(self.u)\n        return self.y\n        \n    def backpropagate(self,x,t):\n        e = self.y - np.asarray(t).reshape(self.V,1)\n        # e.shape is V x 1\n        dLdW1 = np.dot(self.h,e.T)\n        X = np.array(x).reshape(self.V,1)\n        dLdW = np.dot(X, np.dot(self.W',e).T)\n        self.W' = self.W' - self.alpha*dLdW1\n        self.W = self.W - self.alpha*dLdW\n        \n    def train(self,epochs):\n        for x in range(1,epochs):   \n            self.loss = 0\n            for j in range(len(self.X_train)):\n                self.feed_forward(self.X_train[j])\n                self.backpropagate(self.X_train[j],self.y_train[j])\n                C = 0\n                for m in range(self.V):\n                    if(self.y_train[j][m]):\n                        self.loss += -1*self.u[m][0]\n                        C += 1\n                self.loss += C*np.log(np.sum(np.exp(self.u)))\n            print(\"epoch \",x, \" loss = \",self.loss)\n            self.alpha *= 1/( (1+self.alpha*x) )\n            \n    def predict(self,word,number_of_predictions):\n        if word in self.words:\n            index = self.word_index[word]\n            X = [0 for i in range(self.V)]\n            X[index] = 1\n            prediction = self.feed_forward(X)\n            output = {}\n            for i in range(self.V):\n                output[prediction[i][0]] = i\n            \n            top_context_words = []\n            for k in sorted(output,reverse=True):\n                top_context_words.append(self.words[output[k]])\n                if(len(top_context_words)&gt;=number_of_predictions):\n                    break\n    \n            return top_context_words\n        else:\n            print(\"Word not found in dictionary\")\n\ndef preprocessing(corpus):\n    stop_words = set(stopwords.words('english'))\n    training_data = []\n    sentences = corpus.split(\".\")\n    for i in range(len(sentences)):\n        sentences[i] = sentences[i].strip()\n        sentence = sentences[i].split()\n        x = [word.strip(string.punctuation) for word in sentence\n                                    if word not in stop_words]\n        x = [word.lower() for word in x]\n        training_data.append(x)\n    return training_data\n    \n\ndef prepare_data_for_training(sentences,w2v):\n    data = {}\n    for sentence in sentences:\n        for word in sentence:\n            if word not in data:\n                data[word] = 1\n            else:\n                data[word] += 1\n    V = len(data)\n    data = sorted(list(data.keys()))\n    vocab = {}\n    for i in range(len(data)):\n        vocab[data[i]] = i\n    \n    #for i in range(len(words)):\n    for sentence in sentences:\n        for i in range(len(sentence)):\n            center_word = [0 for x in range(V)]\n            center_word[vocab[sentence[i]]] = 1\n            context = [0 for x in range(V)]\n            \n            for j in range(i-w2v.window_size,i+w2v.window_size):\n                if i!=j and j&gt;=0 and j&lt;len(sentence):\n                    context[vocab[sentence[j]]] += 1\n            w2v.X_train.append(center_word)\n            w2v.y_train.append(context)\n    w2v.initialize(V,data)\n\n    return w2v.X_train,w2v.y_train\n\ncorpus = \"\"\ncorpus += \"The earth revolves around the sun. The moon revolves around the earth\"\nepochs = 1000\n\ntraining_data = preprocessing(corpus)\nw2v = word2vec()\n\nprepare_data_for_training(training_data,w2v)\nw2v.train(epochs)\n\nprint(w2v.predict(\"around\",3))\n\n\n\n\nA word embedding technique that is used to convert the words in the dataset to vectors so that the machine understands. Each unique word in your data is assigned to a vector and these vectors vary in dimensions depending on the length of the word.\nA statistical method for efficiently learning a standalone word embedding from a text corpus by analyze and math to exploit the “man-ness” from word “King” and “woman-ness” from word “Queen” to come up with “King is to queen as man to woman” to capture the semantic of the language. - The offset, angle between vector represent relationship between word e.g. “King-Man+woman = queen” - Allow others technique that focus on word usage location in predefind context sliding window of neighboring words to be used - Continous Bag-of-Words CBOW model e.g. predict current word based on context - Continous Skip-Gram model e.g. predict surrounding words based on current word\n - Have high efficiency of both space and time of computation allow the larger data set to be used\n\n\n\n\n\nAn approach that combine global statistics of matrix factorization (LSA Latent Semantic Analysis) with local context base learning for use with Word2vec. It uses the method of create explicit word-context matrix using statistic across the whole corpus.\n\n\n\n\nStandalone Train the model on embedding alone separately from other task and then use the model as part of other task\nJointly Train the model to learn embedding as part of other task which later used for only this model. ### Pretrain Embedding There are many freely available pretrained embedding model that can directly plug-in to your model\nStatic When a pretrained model has a good fit with your task it can be a direct component of your model\nUpdated When the pretrained embedding is seeded as parted of the model that will be updating during model training."
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html#text-conversion",
    "href": "Tutorial/nlps/word2vec.html#text-conversion",
    "title": "Word2vec",
    "section": "",
    "text": "Since the computer is only capable of working on number. Any human language must be translate into number in order for the computer to understand and operated on them using mathematic method. Hence, the text is encoded into numbers after the computer operated on the text it is then decoded back into text that human can understand."
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html#embeddings-meaning",
    "href": "Tutorial/nlps/word2vec.html#embeddings-meaning",
    "title": "Word2vec",
    "section": "",
    "text": "A technique to find the similiarity of meaning of words to have similar representation. feature of the technique: \n\nThe meaning are learn from training from large amount of text\nUse a single learned vector to hold all values A word may have multiple meanings e.g. king mean power,man,… all these meanings are put into a vector represent “king”.\n\n[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]\n\nAnd different from one-hot encoding technique where mulitple dimensions matricies is required to represent word.\nrepresentation is based on usage e.g. similar way of usage result in similar representation in contrast with bag-of-word method which require explicit managing.\nBack by theory in Linguistic “distributional hypothesis” by Zellig Harris \n\n\n\nOne-Hot Encoding is one of the most widely used encoding methods in ML models. This technique is used to quantify categorical data where it compares each level of the categorical variable to a fixed reference level. It converts a single variable with n observations and x distinct values, to x binary variables with n observations where each observation denotes 1 as present and 0 as absent of the dichotomous binary variable."
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html#embedding-layer",
    "href": "Tutorial/nlps/word2vec.html#embedding-layer",
    "title": "Word2vec",
    "section": "",
    "text": "Before a word can add to the vector it must be: - One-hot encodable so it can be mapped to the vector - Vector size is specified - Vector is randomly initialized"
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html#cbow-continous-bag-of-words",
    "href": "Tutorial/nlps/word2vec.html#cbow-continous-bag-of-words",
    "title": "Word2vec",
    "section": "",
    "text": "An architecture for creating word embeddings that uses \\(n\\) future words as well as \\(n\\) past words to create a word embedding. \\[J_0 = \\frac{1}{T}\\sum_{t=1}^{T}log\\space p(w_t|w_{t-n},...,w_{t-1},w_{t+1},...,w_{t+n})\\] The method is using the distributed representations of context (or surrounding words) are combined to predict the word in the middle of the sliding window.\nIf we have 4 context words used for predicting one target word the input layer will be in the form of four 1XW input vectors. These input vectors will be passed to the hidden layer where it is multiplied by a WXN matrix. Finally, the 1XN output from the hidden layer enters the sum layer where an element-wise summation is performed on the vectors before a final activation is performed and the output is obtained.\nIt’s a simple technique of using a single hidden layer to predict a target word at the output layer from a given context in the input layer. Two matrices, the input matrix I and output matrix O, are used to calculate the hidden layer H = C x I where C is the input vector,w is total words in the corpus,n is the dimension of hidden layer, and target words T = H x O. Finally, the softmax function is used to output the probability value between 0-1.\n\\[H_{1\\times n} = C_{1\\times w} \\times M_{w\\times n}\\] \\[T_{1\\times w} = H_{1\\times n} \\times O_{n\\times w}\\] \n**The speed of transmission is an important point of difference between the two viruses. Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19.\nFurther, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission.\nThe reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.**\n\n\nimport numpy as np\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nimport gensim\ndata=open('/content/gdrive/My Drive/covid.txt','r')\ncorona_data = [text for text in data if text.count(' ') &gt;= 2]\nvectorize = Tokenizer()\nvectorize.fit_on_texts(corona_data)\ncorona_data = vectorize.texts_to_sequences(corona_data)\ntotal_vocab = sum(len(s) for s in corona_data)\nword_count = len(vectorize.word_index) + 1\nwindow_size = 2\n\ndef cbow_model(data, window_size, total_vocab):\n    total_length = window_size*2\n    for text in data:\n        text_len = len(text)\n        for idx, word in enumerate(text):\n            context_word = []\n            target   = []            \n            begin = idx - window_size\n            end = idx + window_size + 1\n            context_word.append([text[i] for i in range(begin, end) if 0 &lt;= i &lt; text_len and i != idx])\n            target.append(word)\n            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n            final_target = np_utils.to_categorical(target, total_vocab)\n            yield(contextual, final_target) \nFinally, it is time to build the neural network model that will train the CBOW on our sample data.\nmodel = Sequential()\nmodel.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\nmodel.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\nmodel.add(Dense(total_vocab, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nfor i in range(10):\n    cost = 0\n    for x, y in cbow_model(data, window_size, total_vocab):\n        cost += model.train_on_batch(contextual, final_target)\n    print(i, cost)\nafter training create a file that contains all the vectors.\ndimensions=100\nvect_file = open('/content/gdrive/My Drive/vectors.txt' ,'w')\nvect_file.write('{} {}\\n'.format(total_vocab,dimensions))\n\nweights = model.get_weights()[0]\nfor text, i in vectorize.word_index.items():\n    final_vec = ' '.join(map(str, list(weights[i, :])))\n    vect_file.write('{} {}\\n'.format(text, final_vec))\nvect_file.close()\nuse the vectors that were created and use them in the gensim model.\ncbow_output = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/vectors.txt', binary=False)\ncbow_output.most_similar(positive=['virus'])"
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html#skip-gram",
    "href": "Tutorial/nlps/word2vec.html#skip-gram",
    "title": "Word2vec",
    "section": "",
    "text": "Predict the context word for a given target word the reverse of CBOW algorithm.\n\nIt uses a hidden layer to performs the dot product between the weight matrix and the input vector w(t). \nThere normally using 2 layers of neural network\nThese outputs is then become input to another dot product operation\nThe prediction error is used to modified the weights using backpropagation.\nThe final output is input to softmax function to output probability vector\nThere is no activation function involve at all.\n\n\n\n\nLet’s define some variables :\nV Number of unique words in our corpus of text ( Vocabulary ) x Input layer nx1 (One hot encoding of our input word ). N Number of neurons in the hidden layer of neural network W Weights matrix vxn between input layer and hidden layer W’ Weights matrix nxv between hidden layer and output layer y A softmax output vx1 layer having probabilities of every word in our vocabulary T the transpose h hidden vector nx1\n\\[softmax = \\frac{e^{(x_i - max(x_i))}}{\\sum_{i=1}^{n} e^{(x_i - max(x_i))}}\\] \\[h = W^T\\cdot x\\] \\[u = W^{\\prime T} \\cdot h\\]\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\nclass word2vec(object):\n    def __init__(self):\n        self.N = 10\n        self.X_train = []\n        self.y_train = []\n        self.window_size = 2\n        self.alpha = 0.001\n        self.words = []\n        self.word_index = {}\n\n    def initialize(self,V,data):\n        self.V = V\n        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n        self.W' = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n        \n        self.words = data\n        for i in range(len(data)):\n            self.word_index[data[i]] = i\n\n    def feed_forward(self,X):\n        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n        self.u = np.dot(self.W'.T,self.h)\n        #print(self.u)\n        self.y = softmax(self.u)\n        return self.y\n        \n    def backpropagate(self,x,t):\n        e = self.y - np.asarray(t).reshape(self.V,1)\n        # e.shape is V x 1\n        dLdW1 = np.dot(self.h,e.T)\n        X = np.array(x).reshape(self.V,1)\n        dLdW = np.dot(X, np.dot(self.W',e).T)\n        self.W' = self.W' - self.alpha*dLdW1\n        self.W = self.W - self.alpha*dLdW\n        \n    def train(self,epochs):\n        for x in range(1,epochs):   \n            self.loss = 0\n            for j in range(len(self.X_train)):\n                self.feed_forward(self.X_train[j])\n                self.backpropagate(self.X_train[j],self.y_train[j])\n                C = 0\n                for m in range(self.V):\n                    if(self.y_train[j][m]):\n                        self.loss += -1*self.u[m][0]\n                        C += 1\n                self.loss += C*np.log(np.sum(np.exp(self.u)))\n            print(\"epoch \",x, \" loss = \",self.loss)\n            self.alpha *= 1/( (1+self.alpha*x) )\n            \n    def predict(self,word,number_of_predictions):\n        if word in self.words:\n            index = self.word_index[word]\n            X = [0 for i in range(self.V)]\n            X[index] = 1\n            prediction = self.feed_forward(X)\n            output = {}\n            for i in range(self.V):\n                output[prediction[i][0]] = i\n            \n            top_context_words = []\n            for k in sorted(output,reverse=True):\n                top_context_words.append(self.words[output[k]])\n                if(len(top_context_words)&gt;=number_of_predictions):\n                    break\n    \n            return top_context_words\n        else:\n            print(\"Word not found in dictionary\")\n\ndef preprocessing(corpus):\n    stop_words = set(stopwords.words('english'))\n    training_data = []\n    sentences = corpus.split(\".\")\n    for i in range(len(sentences)):\n        sentences[i] = sentences[i].strip()\n        sentence = sentences[i].split()\n        x = [word.strip(string.punctuation) for word in sentence\n                                    if word not in stop_words]\n        x = [word.lower() for word in x]\n        training_data.append(x)\n    return training_data\n    \n\ndef prepare_data_for_training(sentences,w2v):\n    data = {}\n    for sentence in sentences:\n        for word in sentence:\n            if word not in data:\n                data[word] = 1\n            else:\n                data[word] += 1\n    V = len(data)\n    data = sorted(list(data.keys()))\n    vocab = {}\n    for i in range(len(data)):\n        vocab[data[i]] = i\n    \n    #for i in range(len(words)):\n    for sentence in sentences:\n        for i in range(len(sentence)):\n            center_word = [0 for x in range(V)]\n            center_word[vocab[sentence[i]]] = 1\n            context = [0 for x in range(V)]\n            \n            for j in range(i-w2v.window_size,i+w2v.window_size):\n                if i!=j and j&gt;=0 and j&lt;len(sentence):\n                    context[vocab[sentence[j]]] += 1\n            w2v.X_train.append(center_word)\n            w2v.y_train.append(context)\n    w2v.initialize(V,data)\n\n    return w2v.X_train,w2v.y_train\n\ncorpus = \"\"\ncorpus += \"The earth revolves around the sun. The moon revolves around the earth\"\nepochs = 1000\n\ntraining_data = preprocessing(corpus)\nw2v = word2vec()\n\nprepare_data_for_training(training_data,w2v)\nw2v.train(epochs)\n\nprint(w2v.predict(\"around\",3))"
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html#word2vec-1",
    "href": "Tutorial/nlps/word2vec.html#word2vec-1",
    "title": "Word2vec",
    "section": "",
    "text": "A word embedding technique that is used to convert the words in the dataset to vectors so that the machine understands. Each unique word in your data is assigned to a vector and these vectors vary in dimensions depending on the length of the word.\nA statistical method for efficiently learning a standalone word embedding from a text corpus by analyze and math to exploit the “man-ness” from word “King” and “woman-ness” from word “Queen” to come up with “King is to queen as man to woman” to capture the semantic of the language. - The offset, angle between vector represent relationship between word e.g. “King-Man+woman = queen” - Allow others technique that focus on word usage location in predefind context sliding window of neighboring words to be used - Continous Bag-of-Words CBOW model e.g. predict current word based on context - Continous Skip-Gram model e.g. predict surrounding words based on current word\n - Have high efficiency of both space and time of computation allow the larger data set to be used"
  },
  {
    "objectID": "Tutorial/nlps/word2vec.html#algorithm",
    "href": "Tutorial/nlps/word2vec.html#algorithm",
    "title": "Word2vec",
    "section": "",
    "text": "An approach that combine global statistics of matrix factorization (LSA Latent Semantic Analysis) with local context base learning for use with Word2vec. It uses the method of create explicit word-context matrix using statistic across the whole corpus.\n\n\n\n\nStandalone Train the model on embedding alone separately from other task and then use the model as part of other task\nJointly Train the model to learn embedding as part of other task which later used for only this model. ### Pretrain Embedding There are many freely available pretrained embedding model that can directly plug-in to your model\nStatic When a pretrained model has a good fit with your task it can be a direct component of your model\nUpdated When the pretrained embedding is seeded as parted of the model that will be updating during model training."
  },
  {
    "objectID": "Tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "href": "Tutorial/visions/vision.html#linear-algebra-for-maching-learning",
    "title": "Machine Vision",
    "section": "Linear Algebra for Maching Learning",
    "text": "Linear Algebra for Maching Learning\nUnderstanding Linear Algebra is essential in Machine Learning. It’s a tool that the main compute that the machine use to calulate the value it finally presented as the result.\nSince input data must be convert to matrices form for the machine to operate on using Matix operation rules. One of the most frequent uses operation is matrix multiplication or MatMul for short and Python event have it own symbol for this “@” e.g. \\[ A@B \\]\n\n\n\n\n\n\nTip\n\n\n\nmatrix multiplication \\(AxB\\)\n\n\n\nVector\nMatrix"
  },
  {
    "objectID": "Tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "href": "Tutorial/visions/vision.html#differential-equation-in-machine-learning",
    "title": "Machine Vision",
    "section": "Differential Equation In Machine Learning",
    "text": "Differential Equation In Machine Learning\nOn the other hand, Diffential Equation is used in calculate back propragation. It’s used inconjunction with chain-rule to adjust the input in order to getting closer to actual value\n\nDifferential Equation\n\\[\\frac{du}{dt} = pu  \\]"
  },
  {
    "objectID": "AI/sdiffusion.html#natural-language-processing",
    "href": "AI/sdiffusion.html#natural-language-processing",
    "title": "Stable Diffusion",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "AI/gpt.html#deep-learning-from-fastai-library",
    "href": "AI/gpt.html#deep-learning-from-fastai-library",
    "title": "Generative AI",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nUsing fastai library is the fastest way to get your Machine Learning working with less head ache"
  },
  {
    "objectID": "AI/gpt.html#natural-language-processing",
    "href": "AI/gpt.html#natural-language-processing",
    "title": "Generative AI",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing"
  },
  {
    "objectID": "AI/gpt.html#generative-ai",
    "href": "AI/gpt.html#generative-ai",
    "title": "Generative AI",
    "section": "Generative AI",
    "text": "Generative AI\nA term that use to distinquist the Large Language Model from vanilla Language Model that have very large set of parameters.\nGAI has a characteristic of the ability to generate language response similiar to human generate content. It’s base on Transformer architect ans was trained on very large set unlabelled data set.\nGAI is classified as unsupervised or self-supervised machine learning since they are trained with large corpus of data with minimal or none of labelled.\nIt unique ability is to response to input prompt to regenerate content. It may be response to only text to generate text output unimodal or able to response to more than text multi-modal e.g. text, source code and images then generate either text, source code or image or all\nGenerative AI Model that response to prompt and\nTrain on text, source codes data:\n\nlarge corpus of text\n\nwikipedia\nbookCorpus\nGithub\n[others}(https://en.wikipedia.org/wiki/List_of_text_corpora)\n\n\nto generate text or source code:\n\nChatGPT\nBingChat\nChatbot\nBard\nOpenAI Codex\n\nTrain on pictures with/without text captions\n\nInternet images\nLAION\nEleutherAI\n\nto generate images:\n\nStable Diffusion\nMidjourney\nDALL-E\nImagen\n\ngenerate molecules:\nTrain on amino acids sequences:\n\nSMILES\nvarious biological dataset\n\nTo generate protein structure:\n\nAlphaFold\n\ngenerate music:\nTrain on recorded audo waveforms:\n\nrecorded music\n\nto generate new music\n\nMusicLM\n\nGenerate video\nTrain on record video\n\nrecorded video\n\nto generate video clip\n\nGen1\nRunwayML"
  },
  {
    "objectID": "AI/attention.html",
    "href": "AI/attention.html",
    "title": "Attention",
    "section": "",
    "text": "## Transformer An Architecture That have attention at the heart of the system. It consists of the encoder-decoder unit without recurrence or convolution. It requires that the position of the token in the input be provided since its cannot capture this information on its own.\n\n\nThese positions information are generated with the sine and cosine function with different frequencies then sum with the embedding vector before pass on as the input to the Attention layer\n\n\n### embeddings (capturing semantic)\n\n\nA process that convert the sequence of tokens e.g. text which has low dimension(1D) into higher dimension and store on the vector while also capture the meaning in the input tokens. Tokens that have similar meaning are grouping closer to each orther while tokens that have opposite meaning are placing further away. These vectors are suitable to be passed on to neural network model to learn on.\n\n\n### Components Each layer have 2 sublayers with residual connections:\n\n\n1. Implement multi-heads attentions for parallel processing 2. Implement fully connected feed forward network with 2 linear units with ReLU activation functon sandwich in between \\[ffn_{x} = ReLU(W_{1x} + b_1)W_2 + b_2\\]\n\n\nThe output from this unit then normalize by the layernorm layer that also take the value directly from the input \\(x\\) \\[layernorm( x + sublayer(x))\\]\n\n\n\n\n\n## The Attention A mechanism that let model extract information about previous tokens from stream of input plus its own state to generate information about the current token.\n\n\n- It also assign weight and relevancy to the token so - it can assert the priority of the token as how much attention that it should payed to the token.\n\n\nType of Attention\n\n\n- Self Attention - Cross Attention - Multi-head Attention\n\n\n### How does it work? Each attention unit comprise with three matrices that contains the weight of relevant the query \\(W_Q\\) which represent the query for information, the key \\(W_K\\) the holder of interested information, and the value \\(W_V\\) the actual requested information.\n\n\nWhen a token is looking for information to assess its current state it generate a query vector \\(q_i = x_iW_Q\\) to search for potential relevant information holder vector \\(k_i = x_iW_K\\) of required information \\(v_i = x_iW_v\\)\n\n\nTo determine if the information match the requested a dot product is perform between the two vectors \\(q_i\\) and \\(k_j\\) if the resultant value is large there is a match otherwise there’s no match.\n\n\nTo calculate the attention value of all tokens together use the equation \\[ Attention(Q,K,V) = softmax \\bigg(\\frac{QK^T}{\\sqrt{d_k}} \\bigg)V\\] where Q is the query matrix K is the key matrix V is the value matrix T is the time step \\(\\sqrt{d_k}\\) is the stability gradient factor to prevent fluctuation during training computation\n\n\n#### Attention head A set of \\(W_Q,W_K,W_V\\) is called a head. Each head may be assigned to process tokens and tasks that are relevant to a token. Each layer of the model may have many heads which’s called multi-head Attention this increase the capability of the model process many different tokens in parallel.\n\n\n#### Feed-Forward Unit The output from these processing may be passed on to the feed-forward unit for additional process. This unit is another neural network that comprise with normalization unit,and residual unit.\n\n\n### Encoder The encoder unit job is to map all the input tokens (word) to sequence of attented tokens to be later feed to the decoder. These inputs tokens has been through embedding process and ready for the layer to extract information from.\n\n\n### Decoder The decoder unit role is to extract information only from all the tokens that preceeding the token that the model is expect to predict. Hence, the model doesn’t know before hand what token(word) it’s supposed to be.\n\n\nOperation\n\n\n- First layer - Sublayer 1 - implement muli-head attention then take input from: - embedding with position info - previous decoder if available - implement mask to extract relevance info - pass data on to sublayer 2 - Sublayer 2 - implement normalizaton using layernorm then take input from: - sublayer 1 - directly from input - combine info from both sources - do normalization - pass data on to Layer 2\n\n\n- Second layer - Sublayer 1 - implement muli-head attention then take input from: - Query from first layer - Key and value from encoder if implement - pass data on to sublayer 2 - Sublayer 2 - implement normalizaton using layernorm then take input from: - sublayer 1 - directly from Layer 1 - combine info from both sources - do normalization - pass data on to Layer 3\n\n\n- Third layer - Sublayer 1 - implement fully connected feed forward network then take input from: - Layer 2 - pass data on to sublayer 2 - Sublayer 2 - implement normalizaton using layernorm then take input from: - sublayer 1 - directly from Layer 2 - pass data on to Output layer\n\n\nOutput layer - implement Linear layer - implement softmax\n\n\n\nBut it can combines the input from previous token’s state with the input from encoder state plus the input from the feed-forward unit then generate the output\n\nFor multi-head attention\n\nIt’s require a mask to extract only the relevance value and suppress the rest by convert them to \\(- \\infty\\) from the scaled dot-product of \\(Q\\) and \\(K\\) The mask also force the decoder to only search one way backward.\n\\[mask(QK^T) = mask \\Bigg ( \\begin{bmatrix}\n   a_{11} & a_{11} & ... & a_{1n} \\\\\n   a_{21} & a_{22} &... & a_{2n} \\\\\n  \\vdots & \\vdots &\\ddots & \\vdots \\\\\n   a_{m1} & a_{m2} & ... & a_{mn}\n\\end{bmatrix}\\Bigg ) = \\begin{bmatrix}\n   a_{11} & - \\infty & ... & - \\infty \\\\\n   a_{21} & a_{22} &... & - \\infty\\\\\n  \\vdots & \\vdots &\\ddots & \\vdots \\\\\n   a_{m1} & a_{m2} & ... & a_{mn}\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "AI/attention.html#deep-learning-from-fastai-library",
    "href": "AI/attention.html#deep-learning-from-fastai-library",
    "title": "Attention",
    "section": "Deep learning from fastai library",
    "text": "Deep learning from fastai library\nnbdev version was release"
  },
  {
    "objectID": "AI/attention.html#transformer",
    "href": "AI/attention.html#transformer",
    "title": "Attention",
    "section": "Transformer",
    "text": "Transformer\nAn Architecture That uses attention at the heart of the system. It consists of the encoder-decoder unit without reliant on recurrence (RNN) or convolution (CNN) neural network. However, it requires that the position of the token in the input be provided since its cannot capture this information on its own.\nThese positions information are crucial to assess the priority or relevancy of a word in the sequence. It’s usually generated with the sine and cosine function with different frequencies then sum with the embedding vector before pass on as the input to the Attention layer.\nThe task of the transformer as it apply in Language Model is to predict the next word in the sequence.\n\n\n\n\n\n\nTip\n\n\n\nIn this context I use the token in the same meaning as a “word” in English. However, the token can also be used to represent a letter e.g. “a”,“b”,“c” or digit e.g. “1”,“2”,“3” or symbol e.g. “+”,“-”,…\n\n\n\nEmbeddings (capturing semantic)\nA process that convert the sequence of tokens e.g. text which has low dimension(1D) into higher dimension and store on the vector while also capture the meaning in the input tokens.\nTokens that have similar meaning are grouping closer to each orther while tokens that have opposite meaning are placing further away.\nThese vectors are suitable to be passed on to neural network model to learn on.\n\n\n\n\n\n\nTip\n\n\n\nA word in this context refers to “A”, “a”, ” “,”-” as well as “sky”, “temperature”, “multi-head”\n\n\n\n\nComponents\nEach layer have 2 sublayers with residual connections:\n\nImplement multi-heads attentions for parallel processing\n\nImplement fully connected feed forward network with 2 linear units with ReLU activation functon sandwich in between \\[ffn_{x} = ReLU(W_{1x} + b_1)W_2 + b_2\\]\n\nThe output from this unit then normalize by the layernorm layer that also take the value directly from the input \\(x\\) \\[layernorm( x + sublayer(x))\\]"
  },
  {
    "objectID": "AI/attention.html#section",
    "href": "AI/attention.html#section",
    "title": "Attention",
    "section": "",
    "text": "A mechanism that let model extract information about previous tokens from stream of input plus its own state to generate information about the current token.\n- It also assign weight and relevancy to the token so\n- it can assert the priority of the token as how much attention that it should payed to the token.\n\nHow does it work?\nEach attention unit comprise with three matrices that contains the weight of relevant the query \\(W_Q\\) which represent the query for information, the key \\(W_K\\) the holder of interested information, and the value \\(W_V\\) the actual requested information.\nWhen a token is looking for information to assess its current state it generate a query vector \\(q_i = x_iW_Q\\) to search for potential relevant information holder vector \\(k_i = x_iW_K\\) of required information \\(v_i = x_iW_v\\)\nTo determine if the information match the requested a dot product is perform between the two vectors \\(q_i\\) and \\(k_j\\) if the resultant value is large there is a match otherwise there’s no match.\nTo calculate the attention value of all tokens together use the equation \\[ Attention(Q,K,V) = softmax \\bigg(\\frac{QK^T}{\\sqrt{d_k}} \\bigg)V\\]\nwhere Q is the query matrix\nK is the key matrix\nV is the value matrix\n\\(\\sqrt{d_k}\\) is the stability gradient factor to prevent fluctuation during training computation\n\nAttention head\nA set of \\(W_Q,W_K,W_V\\) is called a head. Each head may be assigned to process tokens and tasks that are relevant to a token. Each layer of the model may have many heads which’s called multi-head Attention this increase the capability of the model process many different tokens in parallel.\n\n\nFeed-Forward Unit\nThe output from these processing may be passed on to the feed-forward unit for additional process.\nThis unit is another neural network that comprise with normalization unit,and residual unit."
  },
  {
    "objectID": "AI/attention.html#t",
    "href": "AI/attention.html#t",
    "title": "Attention",
    "section": "T",
    "text": "T\nA mechanism that let model extract information about previous tokens from stream of input plus its own state to generate information about the current token.\n- It also assign weight and relevancy to the token so\n- it can assert the priority of the token as how much attention that it should payed to the token.\n\nHow does it work?\nEach attention unit comprise with three matrices that contains the weight of relevant the query \\(W_Q\\) which represent the query for information, the key \\(W_K\\) the holder of interested information, and the value \\(W_V\\) the actual requested information.\nWhen a token is looking for information to assess its current state it generate a query vector \\(q_i = x_iW_Q\\) to search for potential relevant information holder vector \\(k_i = x_iW_K\\) of required information \\(v_i = x_iW_v\\)\nTo determine if the information match the requested a dot product is perform between the two vectors \\(q_i\\) and \\(k_j\\) if the resultant value is large there is a match otherwise there’s no match.\nTo calculate the attention value of all tokens together use the equation \\[ Attention(Q,K,V) = softmax \\bigg(\\frac{QK^T}{\\sqrt{d_k}} \\bigg)V\\]\nwhere Q is the query matrix\nK is the key matrix\nV is the value matrix\n\\(\\sqrt{d_k}\\) is the stability gradient factor to prevent fluctuation during training computation\n\nAttention head\nA set of \\(W_Q,W_K,W_V\\) is called a head. Each head may be assigned to process tokens and tasks that are relevant to a token. Each layer of the model may have many heads which’s called multi-head Attention this increase the capability of the model process many different tokens in parallel.\n\n\nFeed-Forward Unit\nThe output from these processing may be passed on to the feed-forward unit for additional process.\nThis unit is another neural network that comprise with normalization unit,and residual unit."
  },
  {
    "objectID": "AI/attention.html#the",
    "href": "AI/attention.html#the",
    "title": "Attention",
    "section": "The",
    "text": "The\nA mechanism that let model extract information about previous tokens from stream of input plus its own state to generate information about the current token.\n- It also assign weight and relevancy to the token so\n- it can assert the priority of the token as how much attention that it should payed to the token.\n\nHow does it work?\nEach attention unit comprise with three matrices that contains the weight of relevant the query \\(W_Q\\) which represent the query for information, the key \\(W_K\\) the holder of interested information, and the value \\(W_V\\) the actual requested information.\nWhen a token is looking for information to assess its current state it generate a query vector \\(q_i = x_iW_Q\\) to search for potential relevant information holder vector \\(k_i = x_iW_K\\) of required information \\(v_i = x_iW_v\\)\nTo determine if the information match the requested a dot product is perform between the two vectors \\(q_i\\) and \\(k_j\\) if the resultant value is large there is a match otherwise there’s no match.\nTo calculate the attention value of all tokens together use the equation \\[ Attention(Q,K,V) = softmax \\bigg(\\frac{QK^T}{\\sqrt{d_k}} \\bigg)V\\]\nwhere Q is the query matrix\nK is the key matrix\nV is the value matrix\n\\(\\sqrt{d_k}\\) is the stability gradient factor to prevent fluctuation during training computation\n\nAttention head\nA set of \\(W_Q,W_K,W_V\\) is called a head. Each head may be assigned to process tokens and tasks that are relevant to a token. Each layer of the model may have many heads which’s called multi-head Attention this increase the capability of the model process many different tokens in parallel.\n\n\nFeed-Forward Unit\nThe output from these processing may be passed on to the feed-forward unit for additional process.\nThis unit is another neural network that comprise with normalization unit,and residual unit."
  },
  {
    "objectID": "AI/attention.html#the-attention",
    "href": "AI/attention.html#the-attention",
    "title": "Attention",
    "section": "The Attention",
    "text": "The Attention\nA mechanism that let model search and extract information from stream of input and use it to predict the next word. It start search for token that have the closet relevance meaning to what it’s think the next word should be from input sequence. Once it found the match it’s then incorporate this data as part of the predict of the next word.\n\nIt also assign weight and relevancy to the token so\n\nit can assert the priority of the token as how much attention that it should payed to the token.\n\n\nType of Attention\n\nSelf Attention\n\nA layer only focus on the data that being process inside the unit e.g. encoder unit, decoder unit\n\nCross Attention\n\nThe layer have access to data from the outside of its own unit e.g. the decoder take input from the encoder\n\nMulti-head Attention\n\nThe implementation of single-head unit in parallel to increase the capability of the model and also taking advantage of multicore, multiprocessing capability of modern CPU and GPU\n\n\n\n\nHow does it work?\nEach attention unit comprise with three matrices that contains the weight of relevant the query \\(W_Q\\) which represent the query for information, the key \\(W_K\\) the holder of interested information, and the value \\(W_V\\) the actual requested information.\nWhen a token is looking for information to assess its current state it generate a query vector \\(q_i = x_iW_Q\\) to search for potential relevant information holder vector \\(k_i = x_iW_K\\) of required information \\(v_i = x_iW_v\\)\nTo determine if the information match the requested a dot product is perform between the two vectors \\(q_i\\) and \\(k_j\\) if the resultant value is large there is a match otherwise there’s no match.\nTo calculate the attention value of all tokens together use the equation \\[ Attention(Q,K,V) = softmax \\bigg(\\frac{QK^T}{\\sqrt{d_k}} \\bigg)V\\]\nwhere Q is the query matrix\nK is the key matrix\nV is the value matrix\nT is the time step \\(\\sqrt{d_k}\\) is the stability gradient factor to prevent fluctuation during training computation\n\nFor multi-head attention\n\nIt’s require a mask to extract only the relevance value and suppress the rest by convert them to \\(- \\infty\\) from the scaled dot-product of \\(Q\\) and \\(K\\) The mask also force the decoder to only search one way backward.\n\n\nAttention head\nA set of \\(W_Q,W_K,W_V\\) is called a head. Each head may be assigned to process tokens and tasks that are relevant to a token. Each layer of the model may have many heads which’s called multi-head Attention this increase the capability of the model process many different tokens in parallel.\n\n\nFeed-Forward Unit\nThe output from these processing may be passed on to the feed-forward unit for additional process.\nThis unit is another neural network that comprise with normalization unit,and residual unit.\n\n\nMasking\nA technique to force the Attention to search for relevance token from the previouly seen stream of tokens only. Since technically it doesn’t know the future word would be since it has not appear yet.\n\\[mask(QK^T) = mask \\Bigg ( \\begin{bmatrix}\n   a_{11} & a_{11} & ... & a_{1n} \\\\\n   a_{21} & a_{22} &... & a_{2n} \\\\\n  \\vdots & \\vdots &\\ddots & \\vdots \\\\\n   a_{m1} & a_{m2} & ... & a_{mn}\n\\end{bmatrix}\\Bigg ) = \\begin{bmatrix}\n   a_{11} & - \\infty & ... & - \\infty \\\\\n   a_{21} & a_{22} &... & - \\infty\\\\\n  \\vdots & \\vdots &\\ddots & \\vdots \\\\\n   a_{m1} & a_{m2} & ... & a_{mn}\n\\end{bmatrix}\\]\n\n\n\nEncoder\nThe encoder unit job is to map all the input tokens (word) to sequence of attented tokens to be later feed to the decoder.\nThese inputs tokens has been through embedding process and ready for the layer to extract information from.\nThe encoder is a bidirectional search method since it perform both forward (suceeding) and backward (preceeding) search the input sequence for the token\n\n\nDecoder\nThe decoder unit role is to extract information only from all the tokens that preceeding the token that the model is expect to predict. Hence, the model doesn’t know before hand what token(word) it’s supposed to be.\nThe decoder start out with unidirectional of backward (preceeding) search due to the restriction of the masking that force it to only able to search the token sequence that already presented. However, subsequence layer namely the second layer may be able to perform bidirectional search since it has access to data from encoder which is an unrestic data.\nOperation\n\nFirst layer\n\nSublayer 1\n\nimplement muli-head attention then take input from:\n\nembedding with position info\nprevious decoder if available\n\nimplement mask to extract relevance info\npass data on to sublayer 2\n\nSublayer 2\n\nimplement normalizaton using layernorm then take input from:\n\nsublayer 1\ndirectly from input (residual)\n\ncombine info from both sources\ndo normalization\npass data on to Layer 2\n\n\nSecond layer\n\nSublayer 1\n\nimplement muli-head attention then take input from:\n\nQuery from first layer\nKey and value from encoder if implement\n\npass data on to sublayer 2\n\nSublayer 2\n\nimplement normalizaton using layernorm then take input from:\n\nsublayer 1\ndirectly from Layer 1 (residual)\n\ncombine info from both sources\ndo normalization\npass data on to Layer 3\n\n\nThird layer\n\nSublayer 1\n\nimplement fully connected feed forward network then take input from:\n\nLayer 2\n\npass data on to sublayer 2\n\nSublayer 2\n\nimplement normalizaton using layernorm then take input from:\n\nsublayer 1\ndirectly from Layer 2 (residual)\n\npass data on to Output layer\n\n\nOutput layer\n\nimplement Linear layer\nimplement softmax\noutput the prediction as probabilities\n\n\nBut it can combines the input from previous token’s state with the input from encoder state plus the input from the feed-forward unit then generate the output"
  },
  {
    "objectID": "index.html#ews",
    "href": "index.html#ews",
    "title": "Machine Intelligence",
    "section": "ews",
    "text": "ews\nSolve age old differential equations help improve Machine Learning modeling\n\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\n\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\n\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\n\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\n\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\n\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Machine Intelligence",
    "section": "News",
    "text": "News\nSolve age old differential equations help improve Machine Learning modeling\n\nStable Diffusion creator Stability AI accelerates open-source AI, raises $101M ￼VentureBeat|2 days ago Stability AI has raised $101M for its open-source Stable Diffusion model, an AI art generator that enables people to instantly create art.\n\nAI Knows How Much You’re Willing to Pay for Flights Before You Do ￼Bloomberg on MSN.com|2 hours ago Armed with mountains of data, artificial intelligence is emerging as an important tool for airlines to find the ideal fares to charge passengers, helping them squeeze out as much revenue as possible as the industry emerges from its biggest crisis.\n\nUS court rules, once again, that AI software can’t hold a patent ￼Ars Technica|1 hour ago The legal challenge came from Dr. Stephen Thaler, who filed two patent applications naming an AI program called “DABUS” as the inventor in 2019. The US Patent and Trademark Office (USPTO) denied the patents,\n\nHow AI Can Improve Pharmaceutical Commercial Operations ￼Forbes|10 hours ago While AI can uncover precious insights, for companies that have not worked with AI previously, this can mean a complete reorganization of their data foundation.\n\nAI-controlled robotic laser can target and kill cockroaches ￼New Scientist|13 hours ago A laser controlled by two cameras and a small computer running an AI model can be trained to target certain types of insect\n\nU.S. scientist hits another dead end in patent case over AI ‘inventor’ ￼Reuters|4 hours ago A U.S. computer scientist on Thursday lost his latest bid to have an artificial intelligence program he created be named “inventor” on a pair of patents he is seeking to obtain.\n\n￼ Generally Intelligent secures cash from OpenAI vets to build capable AI systems ￼TechCrunch|6 hours ago Generally Intelligent, a research startup with backing from OpenAI vets, is aiming to develop general-purpose AI systems."
  },
  {
    "objectID": "Programming/Languages/Python/python.html#eisum",
    "href": "Programming/Languages/Python/python.html#eisum",
    "title": "Python",
    "section": "Eisum",
    "text": "Eisum"
  },
  {
    "objectID": "Programming/Languages/Python/python.html#eisum-in-python",
    "href": "Programming/Languages/Python/python.html#eisum-in-python",
    "title": "Python",
    "section": "Eisum in Python",
    "text": "Eisum in Python\nEisum come from Eistein summation with was invented by Albert Einstein"
  },
  {
    "objectID": "Programming/Languages/Python/python.html#einsum-in-python",
    "href": "Programming/Languages/Python/python.html#einsum-in-python",
    "title": "Python",
    "section": "Einsum in Python",
    "text": "Einsum in Python\nEinsum stand for Einstein summation convention with was invented by Albert Einstein. A hidden gem that less popular but would be an asset for those who discovers it. It’s especially useful in Machine learning field where Tensor operation is at the core of Neural Network.\nThe convention help simplify the calculation of Tensor indices thus speedup the computation. Write more concise and efficient code especially in Python which not known for speed. It has more impact on Framework such as Pytorch that also generate GPU code. as describe by Tim Rocktaschel"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html",
    "href": "Programming/Languages/Python/einsum.html",
    "title": "My Notes",
    "section": "",
    "text": "These papers are stored at the Arxiv website, they are mostly have not been through peer reviews because they are so new. On the other hand, they are follow up and performed by others researchers and interested users on the internet, if there are problems these will be report almost instantly on the social network.\ndiffedit\nProgressive Distillation for Fast Sampling of Diffusion Models\nOn Distillation of Guided Diffusion Models\nImagic: Text-Based Real Image Editing with Diffusion Models\n\n\n\nA technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHow does it work?\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nimport numpy as np\n\n\nA = np.random.rand(3,5)\nB = np.random.rand(5,2)\nM = np.empty((3,2))\n\nA.shape,B.shape,M.shape\n\n((3, 5), (5, 2), (3, 2))\n\n\n\nA,B,M\n\n(array([[6.82759644e-01, 3.32325362e-01, 5.10214353e-01, 8.76891766e-01,\n         1.72299180e-01],\n        [5.33438050e-01, 7.25523710e-01, 8.54886776e-01, 3.96140734e-01,\n         3.56604238e-01],\n        [5.45349948e-05, 8.97919640e-01, 1.86133084e-01, 8.73382052e-01,\n         1.11479305e-01]]),\n array([[0.18449366, 0.69112411],\n        [0.68456084, 0.92427429],\n        [0.39745901, 0.59819685],\n        [0.53124429, 0.82624474],\n        [0.37620928, 0.55749668]]),\n array([[1.13621302e-313, 0.00000000e+000],\n        [6.92770534e-310, 6.92770729e-310],\n        [6.92770292e-310, 6.92770291e-310]]))\n\n\n\nnp.set_printoptions(precision=2, linewidth=140)\n\n\n\nCoding as loop of the matrix multiplication above\nwhere the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\n\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\n\nthe matrix is: [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\nUsing the Eisum method\n\n\n# the i and j are free index\n# the k is sum index since it will be summed away after the operation\nM1 = np.einsum('ik,kj-&gt;ij',A,B)\n\nprint(f'the matix is : {M1}')\n\nthe matix is : [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\n\n\n\nVector or matrix multiplicaton \\(u \\cdot v\\) that result of scalar value\n\n\n\nvector multiply by vector that result in a matrix \\(u \\otimes v\\)\nExample 2 - Using the free index in the output\n- No summation index\n\n# example 2\nD = np.random.rand(5)\nE = np.random.rand(3)\nout = np.einsum('i,j-&gt;ij',D,E)\nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]\n\n\n\n# loop version\nfor i in range(5):\n    for j in range(3):\n        total = 0\n        total += D[i] * E[j]\n        out[i,j] = total\n        \nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]\n\n\n\n\n\n\nThe free indices: - The index that specify the output\nThe Summation index\n- All other indices that appear in the input argument but not show up in the output\nThe General rules: 1. Same index in the a different input argument indicate that these indices will be multiplied and the product are outputed\n    M = np.einsum('ik,kj-&gt;ij',A,B)\n\nOmitting index indicate the index will be summed together\n\n    X = np.ones(3)\n    Y = np.einsum('i-&gt;',X)\n\nThe unsummed indices may return in any order\n\n    D = np.ones((5,4,3))\n    E = np.einsum('ijk-&gt;kji',D)\nOperation that benefit from Einsum 1. Permutation of Tensors 2. Summation 3. Column sum 4. Row sum 5. Matrix-Vector multiplication 6. Matrix-Matrix multiplication 7. Dot Product the first row with first row of a matrix 8. Dot product with matrix (multiplication and add) 9. Element-wise multiplication (Hadamard Product) (multiplication no add) 10. Outer Product 11. Batch matrix multiplicaton e.g. a = 3,2,6 and b = 3,6,3 - want to multiply the matrix of 2x6 with 6x3 matrix - these matrix must follow the multiplication rule - the first number is the batch size they must match, but not count as index - the torch.bmm function will do the same thing 12. Matrix diagonal\n- return the only the diagonal value of the matrix 13. Matrix Trace - summing the value of the diagonal of a matrix\n14. Tensor contration\n- shrinking the dimension of tensor\n15. Bilinear transformation\n\n# !pip install torch\n\n\nimport torch\nimport numpy as np\n\ntorch.set_printoptions(precision=2, linewidth=140)\n\nX = torch.rand((2,3))\nX\n\ntensor([[0.25, 0.74, 0.66],\n        [0.96, 0.80, 0.44]])\n\n\n\nTranspose Flipping the matrix or vector by switching the index of a matrix or vector\n\n\n\nCode\nc_ntp = np.transpose(X)\nc_tp = torch.transpose(X,0,1)\ncein = torch.einsum('ij-&gt;ji',X)\nprint(f'numpy: {c_ntp} \\npytorch: {c_tp}\\n \\neinsum: {cein}')\n\n\nnumpy: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]]) \npytorch: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n \neinsum: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n\n\n\nMatrix summation\nSumming all value in the matrix that result in a scalar value\n\n\n#2. Summation\ncma = torch.sum(X)\ncein = torch.einsum('ij-&gt;',X)\n\nprint(f'regular: {cma} \\neinsum: {cein}')\n\nregular: 3.8526394367218018 \neinsum: 3.8526394367218018\n\n\n\nX\n\ntensor([[0.25, 0.74, 0.66],\n        [0.96, 0.80, 0.44]])\n\n\nRow sum (Left to right)\nAdd all values from each column together along the row\n\n#4 row summation\n# sum by columns\nrows = torch.sum(X,dim=0)\ncein = torch.einsum('ij-&gt;i',X)\n\nprint(f'regular: {rows} \\neinsum: {cein}')\n\nregular: tensor([1.21, 1.54, 1.10]) \neinsum: tensor([1.65, 2.20])\n\n\nColumn sum (Top down)\nAdd all value from each row together along the column\n\n#3 Column summation\n# sum by rows\nc_col = torch.sum(X,dim=1)\ncein = torch.einsum('ij-&gt;j',X)\n\nprint(f'regular: {c_col} \\neinsum: {cein}')\n\nregular: tensor([1.65, 2.20]) \neinsum: tensor([1.21, 1.54, 1.10])\n\n\n5 matrix-vector multiplication\nThis a non equal dimension multiplication which in Python use broadcasting to padded (duplicate) the smaller vector to have equal size with the larger matrix before do multiplication\n\n#5 matrix-vector multiplication\nL = torch.rand((1,3))\nM = torch.rand((3,))\n\ncmm = torch.matmul(L,M)\ncein = torch.einsum('ij,j-&gt;i',L,M)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\nregular: tensor([0.76]) \neinsum: tensor([0.76])\n\n\n6 matrix-matrix multiplication\nThis standard matrix to matrix multiplication which result in another matrix\n\n#6 matrix-matrix multiplication\n# torch.einsum('ij,kj-&gt;ik',M,M)\n\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\nregular: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]]) \neinsum: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n\n\nN = torch.rand((3,3))\nM = torch.rand((2,3))\ntorch.einsum('ij,kj-&gt;ik',N,M)\n\ntensor([[0.98, 0.72],\n        [0.64, 0.60],\n        [1.58, 1.42]])\n\n\nDot product\nThis a matrix multiplication that result in a scalar value. It’s usually called multiply add.\nsince after multiply the row to the column then the sum operation is carry out resulting as a scalar value\n:bulb:\n\n# dot product of a matrix (multiply + add)\n#torch.einsum('ij,ij-&gt;',N,N)\n\n# c = torch.tensor([2,3])\n# d = torch.tensor([2,1])\nc = torch.rand((3))\nd = torch.rand((3))\n\nc_dot = torch.dot(c,d)\ncein = torch.einsum('i,i-&gt;',c,d)\n\nprint(f'c: {c}, c: {c.shape}')\nprint(f'c_dot: {c_dot}')\nprint(f'regular: {c_dot} \\n  einsum: {cein}')\n\nc: tensor([0.38, 0.88, 0.10]), c: torch.Size([3])\nc_dot: 0.3021569550037384\nregular: 0.3021569550037384 \n  einsum: 0.3021569550037384\n\n\n\n# dot product of only the first row of a matrix with first row of a matrix\ntorch.einsum('i,i-&gt;',N[0],N[0])\n\ntensor(0.89)\n\n\nHadamard Product Element wise multiplication (multiply only)\nThis is a normal matrix multiplication which different from multiply add or dot product\n\n# element wise multiplication (multiply only)\n# torch.einsum('ij,ij-&gt;ij',N,N)\n\nc = torch.randn((3,2))\nd = torch.randn((3,2))\ncmm = c * d\ncein = torch.einsum('ij,ij-&gt;ij',c,d)\nprint(f'regular: {cmm} \\n  einsum: {cein}')\n\nregular: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]]) \n  einsum: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]])\n\n\nOuter Product\nMultiply vector of different size to get a matrix as output In eisum must use different letter for index to represent size different\n\n# outer product (inner product)\nx = torch.rand(3)\ny = torch.rand(5)\nprint(f'x: {x}, x: {x.shape}')\nprint(f'y: {y}, y: {y.shape}')\n\nc_outer = torch.outer(x,y)\ncein = torch.einsum('i,j-&gt;ij',x,y)\nprint(f'regular: {c_outer} \\n  einsum: {cein}')\n\nx: tensor([0.91, 0.93, 0.21]), x: torch.Size([3])\ny: tensor([0.98, 0.62, 0.25, 0.90, 0.21]), y: torch.Size([5])\nregular: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]]) \n  einsum: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]])\n\n\nBatch matrix multiplication\nMultiply matrix by the set of n, where n is batch size\nwant to multiply 3 set of the matrix of 2x6 with 6x3 matrix\nthe first number is the batch size must match but not count as index so i is ignore\nthe mxn * nxp must match with n\n\n# batch matrix multiplicaton\n# want to multiply 3 set of the matrix of 2x6 with 6x3 matrix\n# the first number is the batch size must match but not count as index so i is ignore\n# the mxn * nxp must match with n\nR = torch.rand(3,2,6)\nS = torch.rand(3,6,3)\ncmn = np.matmul(R,S)\ncmm = torch.matmul(R,S)\n\ncein = torch.einsum('ijk,ikl-&gt;ijl',R,S)\n\nprint(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n\nregular: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n numpy: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]]) \n  einsum: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n\n\nDiagonal Matrix return the vector of value along the diagonal of a matrix\n\n# Diagonal matrix return only the diagonal value of a matrix\n\nT = torch.rand(3,3)\n\ncein = torch.einsum('ii-&gt;i',T)\nprint(f'T: {T} \\nT shape: {T.shape}')\nc_diag = torch.diag(T)\n\nprint(f'regular: {c_diag} \\n  einsum: {cein}')\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]]) \nT shape: torch.Size([3, 3])\nregular: tensor([0.33, 0.82, 0.18]) \n  einsum: tensor([0.33, 0.82, 0.18])\n\n\n\n# torch.einsum('ii-&gt;i',T)\n\nTrace\nTake the sum of all values along the diagonal axix of a matrix\n\n# matrix trace\n\nc_trace = torch.trace(T)\ncein = torch.einsum('ii-&gt;',T)\nprint(f'T: {T}')\nprint(f'regular: {c_trace} \\n  einsum: {cein}')\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]])\nregular: 1.3291736841201782 \n  einsum: 1.3291736841201782\n\n\nTensor Contraction\nShrinking the dimension of the tensor\nmust provide the dimension to be ignored\n\no = torch.rand((3,4,2))\np = torch.rand((4,3,6))\nprint(f'value: {o.shape} value2: {p.shape}')\n\nc_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\ncein = torch.einsum('ijk,jil-&gt;kl',o,p)\nprint(f'regular: {c_tdot} \\n  einsum: {cein}')\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nBilinear transformation\n\na = torch.rand(2,3)\nb = torch.rand(5,3,7)\nc = torch.rand(2,7)\n\ntorch.einsum('ik,jkl,il-&gt;ij',[a,b,c])\n\ntensor([[3.12, 3.11, 2.71, 3.34, 2.55],\n        [3.39, 2.72, 2.68, 3.09, 2.96]])"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html#research-papers-of-interest",
    "href": "Programming/Languages/Python/einsum.html#research-papers-of-interest",
    "title": "My Notes",
    "section": "",
    "text": "These papers are stored at the Arxiv website, they are mostly have not been through peer reviews because they are so new. On the other hand, they are follow up and performed by others researchers and interested users on the internet, if there are problems these will be report almost instantly on the social network.\ndiffedit\nProgressive Distillation for Fast Sampling of Diffusion Models\nOn Distillation of Guided Diffusion Models\nImagic: Text-Based Real Image Editing with Diffusion Models"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html#eistein-summation",
    "href": "Programming/Languages/Python/einsum.html#eistein-summation",
    "title": "My Notes",
    "section": "",
    "text": "A technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHow does it work?\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nimport numpy as np\n\n\nA = np.random.rand(3,5)\nB = np.random.rand(5,2)\nM = np.empty((3,2))\n\nA.shape,B.shape,M.shape\n\n((3, 5), (5, 2), (3, 2))\n\n\n\nA,B,M\n\n(array([[6.82759644e-01, 3.32325362e-01, 5.10214353e-01, 8.76891766e-01,\n         1.72299180e-01],\n        [5.33438050e-01, 7.25523710e-01, 8.54886776e-01, 3.96140734e-01,\n         3.56604238e-01],\n        [5.45349948e-05, 8.97919640e-01, 1.86133084e-01, 8.73382052e-01,\n         1.11479305e-01]]),\n array([[0.18449366, 0.69112411],\n        [0.68456084, 0.92427429],\n        [0.39745901, 0.59819685],\n        [0.53124429, 0.82624474],\n        [0.37620928, 0.55749668]]),\n array([[1.13621302e-313, 0.00000000e+000],\n        [6.92770534e-310, 6.92770729e-310],\n        [6.92770292e-310, 6.92770291e-310]]))\n\n\n\nnp.set_printoptions(precision=2, linewidth=140)\n\n\n\nCoding as loop of the matrix multiplication above\nwhere the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\n\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\n\nthe matrix is: [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\nUsing the Eisum method\n\n\n# the i and j are free index\n# the k is sum index since it will be summed away after the operation\nM1 = np.einsum('ik,kj-&gt;ij',A,B)\n\nprint(f'the matix is : {M1}')\n\nthe matix is : [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\n\n\n\nVector or matrix multiplicaton \\(u \\cdot v\\) that result of scalar value\n\n\n\nvector multiply by vector that result in a matrix \\(u \\otimes v\\)\nExample 2 - Using the free index in the output\n- No summation index\n\n# example 2\nD = np.random.rand(5)\nE = np.random.rand(3)\nout = np.einsum('i,j-&gt;ij',D,E)\nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]\n\n\n\n# loop version\nfor i in range(5):\n    for j in range(3):\n        total = 0\n        total += D[i] * E[j]\n        out[i,j] = total\n        \nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html#the-eisum-rules",
    "href": "Programming/Languages/Python/einsum.html#the-eisum-rules",
    "title": "My Notes",
    "section": "",
    "text": "The free indices: - The index that specify the output\nThe Summation index\n- All other indices that appear in the input argument but not show up in the output\nThe General rules: 1. Same index in the a different input argument indicate that these indices will be multiplied and the product are outputed\n    M = np.einsum('ik,kj-&gt;ij',A,B)\n\nOmitting index indicate the index will be summed together\n\n    X = np.ones(3)\n    Y = np.einsum('i-&gt;',X)\n\nThe unsummed indices may return in any order\n\n    D = np.ones((5,4,3))\n    E = np.einsum('ijk-&gt;kji',D)\nOperation that benefit from Einsum 1. Permutation of Tensors 2. Summation 3. Column sum 4. Row sum 5. Matrix-Vector multiplication 6. Matrix-Matrix multiplication 7. Dot Product the first row with first row of a matrix 8. Dot product with matrix (multiplication and add) 9. Element-wise multiplication (Hadamard Product) (multiplication no add) 10. Outer Product 11. Batch matrix multiplicaton e.g. a = 3,2,6 and b = 3,6,3 - want to multiply the matrix of 2x6 with 6x3 matrix - these matrix must follow the multiplication rule - the first number is the batch size they must match, but not count as index - the torch.bmm function will do the same thing 12. Matrix diagonal\n- return the only the diagonal value of the matrix 13. Matrix Trace - summing the value of the diagonal of a matrix\n14. Tensor contration\n- shrinking the dimension of tensor\n15. Bilinear transformation\n\n# !pip install torch\n\n\nimport torch\nimport numpy as np\n\ntorch.set_printoptions(precision=2, linewidth=140)\n\nX = torch.rand((2,3))\nX\n\ntensor([[0.25, 0.74, 0.66],\n        [0.96, 0.80, 0.44]])\n\n\n\nTranspose Flipping the matrix or vector by switching the index of a matrix or vector\n\n\n\nCode\nc_ntp = np.transpose(X)\nc_tp = torch.transpose(X,0,1)\ncein = torch.einsum('ij-&gt;ji',X)\nprint(f'numpy: {c_ntp} \\npytorch: {c_tp}\\n \\neinsum: {cein}')\n\n\nnumpy: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]]) \npytorch: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n \neinsum: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n\n\n\nMatrix summation\nSumming all value in the matrix that result in a scalar value\n\n\n#2. Summation\ncma = torch.sum(X)\ncein = torch.einsum('ij-&gt;',X)\n\nprint(f'regular: {cma} \\neinsum: {cein}')\n\nregular: 3.8526394367218018 \neinsum: 3.8526394367218018\n\n\n\nX\n\ntensor([[0.25, 0.74, 0.66],\n        [0.96, 0.80, 0.44]])\n\n\nRow sum (Left to right)\nAdd all values from each column together along the row\n\n#4 row summation\n# sum by columns\nrows = torch.sum(X,dim=0)\ncein = torch.einsum('ij-&gt;i',X)\n\nprint(f'regular: {rows} \\neinsum: {cein}')\n\nregular: tensor([1.21, 1.54, 1.10]) \neinsum: tensor([1.65, 2.20])\n\n\nColumn sum (Top down)\nAdd all value from each row together along the column\n\n#3 Column summation\n# sum by rows\nc_col = torch.sum(X,dim=1)\ncein = torch.einsum('ij-&gt;j',X)\n\nprint(f'regular: {c_col} \\neinsum: {cein}')\n\nregular: tensor([1.65, 2.20]) \neinsum: tensor([1.21, 1.54, 1.10])\n\n\n5 matrix-vector multiplication\nThis a non equal dimension multiplication which in Python use broadcasting to padded (duplicate) the smaller vector to have equal size with the larger matrix before do multiplication\n\n#5 matrix-vector multiplication\nL = torch.rand((1,3))\nM = torch.rand((3,))\n\ncmm = torch.matmul(L,M)\ncein = torch.einsum('ij,j-&gt;i',L,M)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\nregular: tensor([0.76]) \neinsum: tensor([0.76])\n\n\n6 matrix-matrix multiplication\nThis standard matrix to matrix multiplication which result in another matrix\n\n#6 matrix-matrix multiplication\n# torch.einsum('ij,kj-&gt;ik',M,M)\n\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\nregular: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]]) \neinsum: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n\n\nN = torch.rand((3,3))\nM = torch.rand((2,3))\ntorch.einsum('ij,kj-&gt;ik',N,M)\n\ntensor([[0.98, 0.72],\n        [0.64, 0.60],\n        [1.58, 1.42]])\n\n\nDot product\nThis a matrix multiplication that result in a scalar value. It’s usually called multiply add.\nsince after multiply the row to the column then the sum operation is carry out resulting as a scalar value\n:bulb:\n\n# dot product of a matrix (multiply + add)\n#torch.einsum('ij,ij-&gt;',N,N)\n\n# c = torch.tensor([2,3])\n# d = torch.tensor([2,1])\nc = torch.rand((3))\nd = torch.rand((3))\n\nc_dot = torch.dot(c,d)\ncein = torch.einsum('i,i-&gt;',c,d)\n\nprint(f'c: {c}, c: {c.shape}')\nprint(f'c_dot: {c_dot}')\nprint(f'regular: {c_dot} \\n  einsum: {cein}')\n\nc: tensor([0.38, 0.88, 0.10]), c: torch.Size([3])\nc_dot: 0.3021569550037384\nregular: 0.3021569550037384 \n  einsum: 0.3021569550037384\n\n\n\n# dot product of only the first row of a matrix with first row of a matrix\ntorch.einsum('i,i-&gt;',N[0],N[0])\n\ntensor(0.89)\n\n\nHadamard Product Element wise multiplication (multiply only)\nThis is a normal matrix multiplication which different from multiply add or dot product\n\n# element wise multiplication (multiply only)\n# torch.einsum('ij,ij-&gt;ij',N,N)\n\nc = torch.randn((3,2))\nd = torch.randn((3,2))\ncmm = c * d\ncein = torch.einsum('ij,ij-&gt;ij',c,d)\nprint(f'regular: {cmm} \\n  einsum: {cein}')\n\nregular: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]]) \n  einsum: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]])\n\n\nOuter Product\nMultiply vector of different size to get a matrix as output In eisum must use different letter for index to represent size different\n\n# outer product (inner product)\nx = torch.rand(3)\ny = torch.rand(5)\nprint(f'x: {x}, x: {x.shape}')\nprint(f'y: {y}, y: {y.shape}')\n\nc_outer = torch.outer(x,y)\ncein = torch.einsum('i,j-&gt;ij',x,y)\nprint(f'regular: {c_outer} \\n  einsum: {cein}')\n\nx: tensor([0.91, 0.93, 0.21]), x: torch.Size([3])\ny: tensor([0.98, 0.62, 0.25, 0.90, 0.21]), y: torch.Size([5])\nregular: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]]) \n  einsum: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]])\n\n\nBatch matrix multiplication\nMultiply matrix by the set of n, where n is batch size\nwant to multiply 3 set of the matrix of 2x6 with 6x3 matrix\nthe first number is the batch size must match but not count as index so i is ignore\nthe mxn * nxp must match with n\n\n# batch matrix multiplicaton\n# want to multiply 3 set of the matrix of 2x6 with 6x3 matrix\n# the first number is the batch size must match but not count as index so i is ignore\n# the mxn * nxp must match with n\nR = torch.rand(3,2,6)\nS = torch.rand(3,6,3)\ncmn = np.matmul(R,S)\ncmm = torch.matmul(R,S)\n\ncein = torch.einsum('ijk,ikl-&gt;ijl',R,S)\n\nprint(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n\nregular: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n numpy: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]]) \n  einsum: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n\n\nDiagonal Matrix return the vector of value along the diagonal of a matrix\n\n# Diagonal matrix return only the diagonal value of a matrix\n\nT = torch.rand(3,3)\n\ncein = torch.einsum('ii-&gt;i',T)\nprint(f'T: {T} \\nT shape: {T.shape}')\nc_diag = torch.diag(T)\n\nprint(f'regular: {c_diag} \\n  einsum: {cein}')\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]]) \nT shape: torch.Size([3, 3])\nregular: tensor([0.33, 0.82, 0.18]) \n  einsum: tensor([0.33, 0.82, 0.18])\n\n\n\n# torch.einsum('ii-&gt;i',T)\n\nTrace\nTake the sum of all values along the diagonal axix of a matrix\n\n# matrix trace\n\nc_trace = torch.trace(T)\ncein = torch.einsum('ii-&gt;',T)\nprint(f'T: {T}')\nprint(f'regular: {c_trace} \\n  einsum: {cein}')\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]])\nregular: 1.3291736841201782 \n  einsum: 1.3291736841201782\n\n\nTensor Contraction\nShrinking the dimension of the tensor\nmust provide the dimension to be ignored\n\no = torch.rand((3,4,2))\np = torch.rand((4,3,6))\nprint(f'value: {o.shape} value2: {p.shape}')\n\nc_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\ncein = torch.einsum('ijk,jil-&gt;kl',o,p)\nprint(f'regular: {c_tdot} \\n  einsum: {cein}')\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nBilinear transformation\n\na = torch.rand(2,3)\nb = torch.rand(5,3,7)\nc = torch.rand(2,7)\n\ntorch.einsum('ik,jkl,il-&gt;ij',[a,b,c])\n\ntensor([[3.12, 3.11, 2.71, 3.34, 2.55],\n        [3.39, 2.72, 2.68, 3.09, 2.96]])"
  },
  {
    "objectID": "Programming/Languages/Python/python.html",
    "href": "Programming/Languages/Python/python.html",
    "title": "Python",
    "section": "",
    "text": "\\[b = \\sum_i{}\\sum_j{A_{ij}} = A_{ij}\\]\n\\[b_j = \\sum_i{A_{ij}}\\]\n\\[b_i = \\sum_{j}{Aij}\\]\n\\[c_i = \\sum_k{A_{ik}}b_k\\]\nvector: \\[c = \\sum_{i}{a_i}{b_i}\\] matrix: \\[d = \\sum_i{}\\sum_j{A_{ij} B_{ij}}\\]\n\\[C_{ij} = A_{ij}B_{ij}\\]\n\\[C_{ij} = a_{i}b_{j}\\]\n\\[C_{ijl}= \\sum_k{}A_{ijk}{B_{ikl}}\\]\n\\[C_{ilmno} = \\sum_j{}\\sum_k{A_{ijkl}{B_{mnjok}}}\\]\n\\[C_{im} = \\sum_j{}\\sum_o{A_{ij}{B_{mjo}}C_{io}}\\]"
  },
  {
    "objectID": "Programming/Languages/Python/python.html#research-papers-of-interest",
    "href": "Programming/Languages/Python/python.html#research-papers-of-interest",
    "title": "Python",
    "section": "Research papers of interest",
    "text": "Research papers of interest\nThese papers are stored at the Arxiv website, they are mostly have not been through peer reviews because they are so new. On the other hand, they are follow up and performed by others researchers and interested users on the internet, if there are problems these will be report almost instantly on the social network.\ndiffedit\nProgressive Distillation for Fast Sampling of Diffusion Models\nOn Distillation of Guided Diffusion Models\nImagic: Text-Based Real Image Editing with Diffusion Models"
  },
  {
    "objectID": "Programming/Languages/Python/python.html#eistein-summation",
    "href": "Programming/Languages/Python/python.html#eistein-summation",
    "title": "Python",
    "section": "Eistein Summation",
    "text": "Eistein Summation\nA technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nCode: The standard way\nThe standard way to coding the matrix multiplication is using a loop this is simple but very inefficient with \\(O(n^2)\\). where the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\n\n\nThe Eisum Rules\nThe free indices: - The index that specify the output\nThe Summation index\n- All other indices that appear in the input argument but not show up in the output\nThe General rules:\n1. Same index in the a different input argument indicate that these indices will be multiplied and the product are outputed\n    M = np.einsum('ik,kj-&gt;ij',A,B)\n\nOmitting index indicate the index will be summed together\n\n    X = np.ones(3)\n    Y = np.einsum('i-&gt;',X)\n\nThe unsummed indices may return in any order\n\n    D = np.ones((5,4,3))\n    E = np.einsum('ijk-&gt;kji',D)\n\n\nOperation that benefit from Einsum\n\nPermutation of Tensors\n\nSummation\n\nColumn sum\n\nRow sum\n\nMatrix-Vector multiplication\n\nMatrix-Matrix multiplication\n\nDot Product the first row with first row of a matrix\n\nDot product with matrix (multiplication and add)\n\nElement-wise multiplication (Hadamard Product) (multiplication no add)\n\nOuter Product\n\nBatch matrix multiplicaton e.g. a = 3,2,6 and b = 3,6,3\n\nwant to multiply the matrix of 2x6 with 6x3 matrix\n\nthese matrix must follow the multiplication rule\n\nthe first number is the batch size they must match, but not count as index\n\nthe torch.bmm function will do the same thing\n\n\nMatrix diagonal\n\nreturn the only the diagonal value of the matrix\n\n\nMatrix Trace\n\nsumming the value of the diagonal of a matrix\n\n\n\n\nEinsum in action\nHere sample codes for that use Numpy, Pytorch or einsum to calculate Tensor\n\n1. Transpose\n\\[A_{ij} = B_{ji}\\]\nc_ntp = np.transpose(X)\nc_tp = torch.transpose(X,0,1)\ncein = torch.einsum('ij-&gt;ji',X)\nprint(f'numpy: {c_ntp}\\n pytorch: {c_tp}\\n \\neinsum: {cein}')\n\n\n\nnumpy: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]]) \npytorch: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\neinsum: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n\n\nSource: einsum.ipynb\n\n\n2. Summation of matrix\n\\[b = \\sum_i{}\\sum_j{A_{ij}} = A_{ij}\\]\ncma = torch.sum(X)\ncein = torch.einsum('ij-&gt;',X)\n\nprint(f'regular: {cma} \\neinsum: {cein}')\n\n\n\nregular: 3.8526394367218018 \neinsum: 3.8526394367218018\n\n\nSource: einsum.ipynb\n\n\n3. Row summation (1xn)\n\\[b_i = \\sum_{j}{Aij}\\]\nrows = torch.sum(X,dim=0)\ncein = torch.einsum('ij-&gt;i',X)\n\nprint(f'regular: {rows} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.21, 1.54, 1.10]) \neinsum: tensor([1.65, 2.20])\n\n\nSource: einsum.ipynb\n\n\n4. Comlumn summation (mx1)\n\\[b_j = \\sum_i{A_{ij}}\\]\nc_col = torch.sum(X,dim=1)\ncein = torch.einsum('ij-&gt;j',X)\n\nprint(f'regular: {c_col} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.65, 2.20]) \neinsum: tensor([1.21, 1.54, 1.10])\n\n\nSource: einsum.ipynb\n\n\n5. Matrix-vector multiplication\n\\[c_i = \\sum_k{A_{ik}}b_k\\]\nL = torch.rand((1,3))\nM = torch.rand((3,))\n\ncmm = torch.matmul(L,M)\ncein = torch.einsum('ij,j-&gt;i',L,M)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([0.76]) \neinsum: tensor([0.76])\n\n\nSource: einsum.ipynb\n\n\n6. Matrix-Matrix multiplication\n\\[C_{ij}= \\sum_k{}A_{ik}{B_{kj}}\\]\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]]) \neinsum: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n\nSource: einsum.ipynb\n\n\n7. Dot product (inner product)\nvector: \\[c = \\sum_{i}{a_i}{b_i}\\] matrix: \\[d = \\sum_i{}\\sum_j{A_{ij} B_{ij}}\\]\nc = torch.rand((3))\nd = torch.rand((3))\n\nc_dot = torch.dot(c,d)\ncein = torch.einsum('i,i-&gt;',c,d)\n\nprint(f'c: {c}, c: {c.shape}')\nprint(f'c_dot: {c_dot}')\nprint(f'regular: {c_dot} \\n  einsum: {cein}')\n\n\n\nc: tensor([0.38, 0.88, 0.10]), c: torch.Size([3])\nc_dot: 0.3021569550037384\nregular: 0.3021569550037384 \n  einsum: 0.3021569550037384\n\n\nSource: einsum.ipynb\n\n\n8. Hadamard Product (elementwise multiplication without add)\n\\[C_{ij} = A_{ij}B_{ij}\\]\nc = torch.randn((3,2))\nd = torch.randn((3,2))\ncmm = c * d\ncein = torch.einsum('ij,ij-&gt;ij',c,d)\nprint(f'regular: {cmm} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]]) \n  einsum: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]])\n\n\nSource: einsum.ipynb\n\n\n9. Outer Product (vector multiply vector)\n\\[C_{ij} = a_{i}b_{j}\\]\nx = torch.rand(3)\ny = torch.rand(5)\nprint(f'x: {x}, x: {x.shape}')\nprint(f'y: {y}, y: {y.shape}')\n\nc_outer = torch.outer(x,y)\ncein = torch.einsum('i,j-&gt;ij',x,y)\nprint(f'regular: {c_outer} \\n  einsum: {cein}')\n\n\n\nx: tensor([0.91, 0.93, 0.21]), x: torch.Size([3])\ny: tensor([0.98, 0.62, 0.25, 0.90, 0.21]), y: torch.Size([5])\nregular: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]]) \n  einsum: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]])\n\n\nSource: einsum.ipynb\n\n\n10. batch matrix multiplication\n\\[C_{ijl}= \\sum_k{}A_{ijk}{B_{ikl}}\\]\nR = torch.rand(3,2,6)\nS = torch.rand(3,6,3)\ncmn = np.matmul(R,S)\ncmm = torch.matmul(R,S)\n\ncein = torch.einsum('ijk,ikl-&gt;ijl',R,S)\n\nprint(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n numpy: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]]) \n  einsum: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n\n\nSource: einsum.ipynb\n\n\n11. Diagonal Matrix (return only the diagonal value of a matrix where all other value are 0)\n\\[\n\\forall i,j \\in \\{1, 2, \\ldots, n\\}, i \\ne j \\implies d_{i,j} = 0\n\\]\nT = torch.rand(3,3)\n\ncein = torch.einsum('ii-&gt;i',T)\nprint(f'T: {T} \\nT shape: {T.shape}')\nc_diag = torch.diag(T)\n\nprint(f'regular: {c_diag} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]]) \nT shape: torch.Size([3, 3])\nregular: tensor([0.33, 0.82, 0.18]) \n  einsum: tensor([0.33, 0.82, 0.18])\n\n\nSource: einsum.ipynb\n\n\n12. Trace (take sum along diagonal axis; square matrix only)\n\\[tr(A)= \\sum_i{a_{ii}}\\]\nc_trace = torch.trace(T)\ncein = torch.einsum('ii-&gt;',T)\nprint(f'T: {T}')\nprint(f'regular: {c_trace} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]])\nregular: 1.3291736841201782 \n  einsum: 1.3291736841201782\n\n\nSource: einsum.ipynb\n\n\n13. Tensor contraction\n\\[C_{ilmno} = \\sum_j{}\\sum_k{A_{ijkl}{B_{mnjok}}}\\]\no = torch.rand((3,4,2))\np = torch.rand((4,3,6))\nprint(f'value: {o.shape} value2: {p.shape}')\n\nc_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\ncein = torch.einsum('ijk,jil-&gt;kl',o,p)\nprint(f'regular: {c_tdot} \\n  einsum: {cein}')\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb\n\n\n14. Bilinear Transformation\n\\[C_{im} = \\sum_j{}\\sum_o{A_{ij}{B_{mjo}}C_{io}}\\]\na = torch.rand(2,3)\nb = torch.rand(5,3,7)\nc = torch.rand(2,7)\n\ntorch.einsum('ik,jkl,il-&gt;ij',[a,b,c])\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb"
  },
  {
    "objectID": "Programming/Languages/Python/python.html#the-eisum-rules",
    "href": "Programming/Languages/Python/python.html#the-eisum-rules",
    "title": "Python",
    "section": "The Eisum Rules",
    "text": "The Eisum Rules\nThe free indices: - The index that specify the output\nThe Summation index\n- All other indices that appear in the input argument but not show up in the output\nThe General rules:\n1. Same index in the a different input argument indicate that these indices will be multiplied and the product are outputed\n    M = np.einsum('ik,kj-&gt;ij',A,B)\n\nOmitting index indicate the index will be summed together\n\n    X = np.ones(3)\n    Y = np.einsum('i-&gt;',X)\n\nThe unsummed indices may return in any order\n\n    D = np.ones((5,4,3))\n    E = np.einsum('ijk-&gt;kji',D)\n\nEinsum in action\nHere sample codes for that use Numpy, Pytorch or einsum to calculate Tensor\n\n1. Transpose\n\\[A_{ij} = B_{ji}\\]\nc_ntp = np.transpose(X)\nc_tp = torch.transpose(X,0,1)\ncein = torch.einsum('ij-&gt;ji',X)\nprint(f'numpy: {c_ntp}\\n pytorch: {c_tp}\\n \\neinsum: {cein}')\n\n\n\nnumpy: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]]) \npytorch: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\neinsum: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n\n\nSource: einsum.ipynb\n\n\n2. Summation of matrix\n\\[b = \\sum_i{}\\sum_j{A_{ij}} = A_{ij}\\]\ncma = torch.sum(X)\ncein = torch.einsum('ij-&gt;',X)\n\nprint(f'regular: {cma} \\neinsum: {cein}')\n\n\n\nregular: 3.8526394367218018 \neinsum: 3.8526394367218018\n\n\nSource: einsum.ipynb\n\n\n3. Row summation (1xn)\n\\[b_i = \\sum_{j}{Aij}\\]\nrows = torch.sum(X,dim=0)\ncein = torch.einsum('ij-&gt;i',X)\n\nprint(f'regular: {rows} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.21, 1.54, 1.10]) \neinsum: tensor([1.65, 2.20])\n\n\nSource: einsum.ipynb\n\n\n4. Comlumn summation (mx1)\n\\[b_j = \\sum_i{A_{ij}}\\]\nc_col = torch.sum(X,dim=1)\ncein = torch.einsum('ij-&gt;j',X)\n\nprint(f'regular: {c_col} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.65, 2.20]) \neinsum: tensor([1.21, 1.54, 1.10])\n\n\nSource: einsum.ipynb\n\n\n5. Matrix-vector multiplication\n\\[c_i = \\sum_k{A_{ik}}b_k\\]\nL = torch.rand((1,3))\nM = torch.rand((3,))\n\ncmm = torch.matmul(L,M)\ncein = torch.einsum('ij,j-&gt;i',L,M)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([0.76]) \neinsum: tensor([0.76])\n\n\nSource: einsum.ipynb\n\n\n6. Matrix-Matrix multiplication\n\\[C_{ij}= \\sum_k{}A_{ik}{B_{kj}}\\]\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]]) \neinsum: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n\nSource: einsum.ipynb\n\n\n7. Dot product (inner product)\nvector: \\[c = \\sum_{i}{a_i}{b_i}\\] matrix: \\[d = \\sum_i{}\\sum_j{A_{ij} B_{ij}}\\]\nc = torch.rand((3))\nd = torch.rand((3))\n\nc_dot = torch.dot(c,d)\ncein = torch.einsum('i,i-&gt;',c,d)\n\nprint(f'c: {c}, c: {c.shape}')\nprint(f'c_dot: {c_dot}')\nprint(f'regular: {c_dot} \\n  einsum: {cein}')\n\n\n\nc: tensor([0.38, 0.88, 0.10]), c: torch.Size([3])\nc_dot: 0.3021569550037384\nregular: 0.3021569550037384 \n  einsum: 0.3021569550037384\n\n\nSource: einsum.ipynb\n\n\n8. Hadamard Product (elementwise multiplication without add)\n\\[C_{ij} = A_{ij}B_{ij}\\]\nc = torch.randn((3,2))\nd = torch.randn((3,2))\ncmm = c * d\ncein = torch.einsum('ij,ij-&gt;ij',c,d)\nprint(f'regular: {cmm} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]]) \n  einsum: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]])\n\n\nSource: einsum.ipynb\n\n\n9. Outer Product (vector multiply vector)\n\\[C_{ij} = a_{i}b_{j}\\]\nx = torch.rand(3)\ny = torch.rand(5)\nprint(f'x: {x}, x: {x.shape}')\nprint(f'y: {y}, y: {y.shape}')\n\nc_outer = torch.outer(x,y)\ncein = torch.einsum('i,j-&gt;ij',x,y)\nprint(f'regular: {c_outer} \\n  einsum: {cein}')\n\n\n\nx: tensor([0.91, 0.93, 0.21]), x: torch.Size([3])\ny: tensor([0.98, 0.62, 0.25, 0.90, 0.21]), y: torch.Size([5])\nregular: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]]) \n  einsum: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]])\n\n\nSource: einsum.ipynb\n\n\n10. batch matrix multiplication\n\\[C_{ijl}= \\sum_k{}A_{ijk}{B_{ikl}}\\]\nR = torch.rand(3,2,6)\nS = torch.rand(3,6,3)\ncmn = np.matmul(R,S)\ncmm = torch.matmul(R,S)\n\ncein = torch.einsum('ijk,ikl-&gt;ijl',R,S)\n\nprint(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n numpy: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]]) \n  einsum: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n\n\nSource: einsum.ipynb\n\n\n11. Diagonal Matrix (return only the diagonal value of a matrix where all other value are 0)\n\\[\n\\forall i,j \\in \\{1, 2, \\ldots, n\\}, i \\ne j \\implies d_{i,j} = 0\n\\]\nT = torch.rand(3,3)\n\ncein = torch.einsum('ii-&gt;i',T)\nprint(f'T: {T} \\nT shape: {T.shape}')\nc_diag = torch.diag(T)\n\nprint(f'regular: {c_diag} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]]) \nT shape: torch.Size([3, 3])\nregular: tensor([0.33, 0.82, 0.18]) \n  einsum: tensor([0.33, 0.82, 0.18])\n\n\nSource: einsum.ipynb\n\n\n12. Trace (take sum along diagonal axis; square matrix only)\n\\[tr(A)= \\sum_i{a_{ii}}\\]\nc_trace = torch.trace(T)\ncein = torch.einsum('ii-&gt;',T)\nprint(f'T: {T}')\nprint(f'regular: {c_trace} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]])\nregular: 1.3291736841201782 \n  einsum: 1.3291736841201782\n\n\nSource: einsum.ipynb\n\n\n13. Tensor contraction\n\\[C_{ilmno} = \\sum_j{}\\sum_k{A_{ijkl}{B_{mnjok}}}\\]\no = torch.rand((3,4,2))\np = torch.rand((4,3,6))\nprint(f'value: {o.shape} value2: {p.shape}')\n\nc_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\ncein = torch.einsum('ijk,jil-&gt;kl',o,p)\nprint(f'regular: {c_tdot} \\n  einsum: {cein}')\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb\n\n\n14. Bilinear Transformation\n\\[C_{im} = \\sum_j{}\\sum_o{A_{ij}{B_{mjo}}C_{io}}\\]\na = torch.rand(2,3)\nb = torch.rand(5,3,7)\nc = torch.rand(2,7)\n\ntorch.einsum('ik,jkl,il-&gt;ij',[a,b,c])\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb"
  },
  {
    "objectID": "Programming/Languages/Python/python.html#einstein-summation",
    "href": "Programming/Languages/Python/python.html#einstein-summation",
    "title": "Python",
    "section": "Einstein Summation",
    "text": "Einstein Summation\nA technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nCode:\nThe standard way to code the matrix multiplication is using a loop although this is simple but very inefficient with \\(O(n^2)\\) where the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\nHere the einsum version just as simple but much more efficient in term of memory usage and computation speed up.\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\nThe Eisum Rules\nHere some of the convention for einsum\nThe free indices:\n\nThe index that specify the output\n\nThe Summation index\n\nAll other indices that appear in the input argument but not show up in the output\n\nThe General rules:\n\nSame index in the a different input argument indicate that these indices will be multiplied and the product are outputed\n\n    M = np.einsum('ik,kj-&gt;ij',A,B)\n\nOmitting index indicate the index will be summed together\n\n    X = np.ones(3)\n    Y = np.einsum('i-&gt;',X)\n\nThe unsummed indices may return in any order\n\n    D = np.ones((5,4,3))\n    E = np.einsum('ijk-&gt;kji',D)\n\n\nOperations that benefit from Einsum\nNot all tensors operation can benefit from using einsum, however here are some of them that do.\n\nPermutation of Tensors\n\nSummation\n\nColumn sum\n\nRow sum\n\nMatrix-Vector multiplication\n\nMatrix-Matrix multiplication\n\nDot Product the first row with first row of a matrix\n\nDot product with matrix (multiplication and add)\n\nElement-wise multiplication (Hadamard Product) (multiplication no add)\n\nOuter Product\n\nBatch matrix multiplicaton e.g. a = 3,2,6 and b = 3,6,3\n\nwant to multiply the matrix of 2x6 with 6x3 matrix\n\nthese matrix must follow the multiplication rule\n\nthe first number is the batch size they must match, but not count as index\n\nthe torch.bmm function will do the same thing\n\n\nMatrix diagonal\n\nreturn the only the diagonal value of the matrix\n\n\nMatrix Trace\n\nsumming the value of the diagonal of a matrix\n\n\nTensor contration\n\nshrinking the dimension of tensor\n\nBilinear transformation\n\n\n\nEinsum in action\nHere some sample codes that use either Numpy, Pytorch compare to einsum to calculate Tensor\n\n1. Transpose\nFlipping the matrix or vector by switching the index of a matrix or vector \\[A_{ij} = B_{ji}\\]\nc_ntp = np.transpose(X)\nc_tp = torch.transpose(X,0,1)\ncein = torch.einsum('ij-&gt;ji',X)\nprint(f'numpy: {c_ntp}\\n pytorch: {c_tp}\\n \\neinsum: {cein}')\n\n\n\nnumpy: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]]) \npytorch: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\neinsum: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n\n\nSource: einsum.ipynb\n\n\n2. Summation of matrix\nSumming all values in the matrix that result in a scalar value \\[b = \\sum_i{}\\sum_j{A_{ij}} = A_{ij}\\]\ncma = torch.sum(X)\ncein = torch.einsum('ij-&gt;',X)\n\nprint(f'regular: {cma} \\neinsum: {cein}')\n\n\n\nregular: 3.8526394367218018 \neinsum: 3.8526394367218018\n\n\nSource: einsum.ipynb\n\n\n3. Row summation (1xn)\nAdd all values from each column together along the row \\[b_i = \\sum_{j}{Aij}\\]\nrows = torch.sum(X,dim=0)\ncein = torch.einsum('ij-&gt;i',X)\n\nprint(f'regular: {rows} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.21, 1.54, 1.10]) \neinsum: tensor([1.65, 2.20])\n\n\nSource: einsum.ipynb\n\n\n4. Comlumn summation (mx1)\nAdd all values from each row together along the column \\[b_j = \\sum_i{A_{ij}}\\]\nc_col = torch.sum(X,dim=1)\ncein = torch.einsum('ij-&gt;j',X)\n\nprint(f'regular: {c_col} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.65, 2.20]) \neinsum: tensor([1.21, 1.54, 1.10])\n\n\nSource: einsum.ipynb\n\n\n5. Matrix-vector multiplication\nThis a non equal dimension multiplication which in Python use broadcasting to padded (duplicate) the smaller vector to have equal size with the larger matrix before do multiplication \\[c_i = \\sum_k{A_{ik}}b_k\\]\nL = torch.rand((1,3))\nM = torch.rand((3,))\n\ncmm = torch.matmul(L,M)\ncein = torch.einsum('ij,j-&gt;i',L,M)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([0.76]) \neinsum: tensor([0.76])\n\n\nSource: einsum.ipynb\n\n\n6. Matrix-Matrix multiplication\n\\[C_{ij}= \\sum_k{}A_{ik}{B_{kj}}\\]\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]]) \neinsum: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n\nSource: einsum.ipynb\n\n\n7. Dot product (inner product)\nThis a matrix multiplication that result in a scalar value. It’s usually called multiply-add.\nsince after multiply the row to the column then the sum operation is carry out resulting as a scalar value\nvector: \\[c = \\sum_{i}{a_i}{b_i}\\] matrix: \\[d = \\sum_i{}\\sum_j{A_{ij} B_{ij}}\\]\nc = torch.rand((3))\nd = torch.rand((3))\n\nc_dot = torch.dot(c,d)\ncein = torch.einsum('i,i-&gt;',c,d)\n\nprint(f'c: {c}, c: {c.shape}')\nprint(f'c_dot: {c_dot}')\nprint(f'regular: {c_dot} \\n  einsum: {cein}')\n\n\n\nc: tensor([0.38, 0.88, 0.10]), c: torch.Size([3])\nc_dot: 0.3021569550037384\nregular: 0.3021569550037384 \n  einsum: 0.3021569550037384\n\n\nSource: einsum.ipynb\n\n\n8. Hadamard Product (elementwise multiplication without add)\nElement wise multiplication (multiply only)\nThis is a normal matrix multiplication which different from multiply add or dot product \\[C_{ij} = A_{ij}B_{ij}\\]\nc = torch.randn((3,2))\nd = torch.randn((3,2))\ncmm = c * d\ncein = torch.einsum('ij,ij-&gt;ij',c,d)\nprint(f'regular: {cmm} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]]) \n  einsum: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]])\n\n\nSource: einsum.ipynb\n\n\n9. Outer Product (vector multiply vector)\nMultiply vector of different size to get a matrix as output\nIn eisum must use different letter for index to represent size different\n\\[C_{ij} = a_{i}b_{j}\\]\nx = torch.rand(3)\ny = torch.rand(5)\nprint(f'x: {x}, x: {x.shape}')\nprint(f'y: {y}, y: {y.shape}')\n\nc_outer = torch.outer(x,y)\ncein = torch.einsum('i,j-&gt;ij',x,y)\nprint(f'regular: {c_outer} \\n  einsum: {cein}')\n\n\n\nx: tensor([0.91, 0.93, 0.21]), x: torch.Size([3])\ny: tensor([0.98, 0.62, 0.25, 0.90, 0.21]), y: torch.Size([5])\nregular: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]]) \n  einsum: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]])\n\n\nSource: einsum.ipynb\n\n\n10. Batch matrix multiplication\nMultiply matrix by the set of n, where n is batch size\nfor example multiply 3 set of the matrix of 2x6 with 6x3 matrix\nthe first number is the batch size must match but not count as index so i is ignore\nthe mxn * nxp must match with n as require by multiplication rule \\[C_{ijl}= \\sum_k{}A_{ijk}{B_{ikl}}\\]\nR = torch.rand(3,2,6)\nS = torch.rand(3,6,3)\ncmn = np.matmul(R,S)\ncmm = torch.matmul(R,S)\n\ncein = torch.einsum('ijk,ikl-&gt;ijl',R,S)\n\nprint(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n numpy: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]]) \n  einsum: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n\n\nSource: einsum.ipynb\n\n\n11. Diagonal Matrix\nreturn only the diagonal value of a square matrix and all other values in the matrix are 0 \\[\n\\forall i,j \\in \\{1, 2, \\ldots, n\\}, i \\ne j \\implies d_{i,j} = 0\n\\]\nT = torch.rand(3,3)\n\ncein = torch.einsum('ii-&gt;i',T)\nprint(f'T: {T} \\nT shape: {T.shape}')\nc_diag = torch.diag(T)\n\nprint(f'regular: {c_diag} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]]) \nT shape: torch.Size([3, 3])\nregular: tensor([0.33, 0.82, 0.18]) \n  einsum: tensor([0.33, 0.82, 0.18])\n\n\nSource: einsum.ipynb\n\n\n12. Trace (take sum along diagonal axis; square matrix only)\nTake the sum of all values along the diagonal axix of a matrix \\[tr(A)= \\sum_i{a_{ii}}\\]\nc_trace = torch.trace(T)\ncein = torch.einsum('ii-&gt;',T)\nprint(f'T: {T}')\nprint(f'regular: {c_trace} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]])\nregular: 1.3291736841201782 \n  einsum: 1.3291736841201782\n\n\nSource: einsum.ipynb\n\n\n13. Tensor contraction\nShrinking the dimension of the tensor\nmust provide the dimension to be ignored \\[C_{ilmno} = \\sum_j{}\\sum_k{A_{ijkl}{B_{mnjok}}}\\]\no = torch.rand((3,4,2))\np = torch.rand((4,3,6))\nprint(f'value: {o.shape} value2: {p.shape}')\n\nc_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\ncein = torch.einsum('ijk,jil-&gt;kl',o,p)\nprint(f'regular: {c_tdot} \\n  einsum: {cein}')\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb\n\n\n14. Bilinear Transformation\n\\[C_{im} = \\sum_j{}\\sum_o{A_{ij}{B_{mjo}}C_{io}}\\]\na = torch.rand(2,3)\nb = torch.rand(5,3,7)\nc = torch.rand(2,7)\n\ntorch.einsum('ik,jkl,il-&gt;ij',[a,b,c])\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb"
  },
  {
    "objectID": "Programming/Languages/Python/python.html#ein-stein-summation",
    "href": "Programming/Languages/Python/python.html#ein-stein-summation",
    "title": "Python",
    "section": "Ein stein Summation",
    "text": "Ein stein Summation\nA technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nCode: The standard way\nThe standard way to coding the matrix multiplication is using a loop this is simple but very inefficient with \\(O(n^2)\\). where the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\n\n\nThe Eisum Rules\nThe free indices: - The index that specify the output\nThe Summation index\n- All other indices that appear in the input argument but not show up in the output\nThe General rules:\n1. Same index in the a different input argument indicate that these indices will be multiplied and the product are outputed\n    M = np.einsum('ik,kj-&gt;ij',A,B)\n\nOmitting index indicate the index will be summed together\n\n    X = np.ones(3)\n    Y = np.einsum('i-&gt;',X)\n\nThe unsummed indices may return in any order\n\n    D = np.ones((5,4,3))\n    E = np.einsum('ijk-&gt;kji',D)\n\n\nOperation that benefit from Einsum\n\nPermutation of Tensors\n\nSummation\n\nColumn sum\n\nRow sum\n\nMatrix-Vector multiplication\n\nMatrix-Matrix multiplication\n\nDot Product the first row with first row of a matrix\n\nDot product with matrix (multiplication and add)\n\nElement-wise multiplication (Hadamard Product) (multiplication no add)\n\nOuter Product\n\nBatch matrix multiplicaton e.g. a = 3,2,6 and b = 3,6,3\n\nwant to multiply the matrix of 2x6 with 6x3 matrix\n\nthese matrix must follow the multiplication rule\n\nthe first number is the batch size they must match, but not count as index\n\nthe torch.bmm function will do the same thing\n\n\nMatrix diagonal\n\nreturn the only the diagonal value of the matrix\n\n\nMatrix Trace\n\nsumming the value of the diagonal of a matrix\n\n\n\n\nEinsum in action\nHere sample codes for that use Numpy, Pytorch or einsum to calculate Tensor\n\n1. Transpose\n\\[A_{ij} = B_{ji}\\]\nc_ntp = np.transpose(X)\nc_tp = torch.transpose(X,0,1)\ncein = torch.einsum('ij-&gt;ji',X)\nprint(f'numpy: {c_ntp}\\n pytorch: {c_tp}\\n \\neinsum: {cein}')\n\n\n\nnumpy: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]]) \npytorch: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\neinsum: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n\n\nSource: einsum.ipynb\n\n\n2. Summation of matrix\n\\[b = \\sum_i{}\\sum_j{A_{ij}} = A_{ij}\\]\ncma = torch.sum(X)\ncein = torch.einsum('ij-&gt;',X)\n\nprint(f'regular: {cma} \\neinsum: {cein}')\n\n\n\nregular: 3.8526394367218018 \neinsum: 3.8526394367218018\n\n\nSource: einsum.ipynb\n\n\n3. Row summation (1xn)\n\\[b_i = \\sum_{j}{Aij}\\]\nrows = torch.sum(X,dim=0)\ncein = torch.einsum('ij-&gt;i',X)\n\nprint(f'regular: {rows} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.21, 1.54, 1.10]) \neinsum: tensor([1.65, 2.20])\n\n\nSource: einsum.ipynb\n\n\n4. Comlumn summation (mx1)\n\\[b_j = \\sum_i{A_{ij}}\\]\nc_col = torch.sum(X,dim=1)\ncein = torch.einsum('ij-&gt;j',X)\n\nprint(f'regular: {c_col} \\neinsum: {cein}')\n\n\n\nregular: tensor([1.65, 2.20]) \neinsum: tensor([1.21, 1.54, 1.10])\n\n\nSource: einsum.ipynb\n\n\n5. Matrix-vector multiplication\n\\[c_i = \\sum_k{A_{ik}}b_k\\]\nL = torch.rand((1,3))\nM = torch.rand((3,))\n\ncmm = torch.matmul(L,M)\ncein = torch.einsum('ij,j-&gt;i',L,M)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([0.76]) \neinsum: tensor([0.76])\n\n\nSource: einsum.ipynb\n\n\n6. Matrix-Matrix multiplication\n\\[C_{ij}= \\sum_k{}A_{ik}{B_{kj}}\\]\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\n\n\nregular: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]]) \neinsum: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n\nSource: einsum.ipynb\n\n\n7. Dot product (inner product)\nvector: \\[c = \\sum_{i}{a_i}{b_i}\\] matrix: \\[d = \\sum_i{}\\sum_j{A_{ij} B_{ij}}\\]\nc = torch.rand((3))\nd = torch.rand((3))\n\nc_dot = torch.dot(c,d)\ncein = torch.einsum('i,i-&gt;',c,d)\n\nprint(f'c: {c}, c: {c.shape}')\nprint(f'c_dot: {c_dot}')\nprint(f'regular: {c_dot} \\n  einsum: {cein}')\n\n\n\nc: tensor([0.38, 0.88, 0.10]), c: torch.Size([3])\nc_dot: 0.3021569550037384\nregular: 0.3021569550037384 \n  einsum: 0.3021569550037384\n\n\nSource: einsum.ipynb\n\n\n8. Hadamard Product (elementwise multiplication without add)\n\\[C_{ij} = A_{ij}B_{ij}\\]\nc = torch.randn((3,2))\nd = torch.randn((3,2))\ncmm = c * d\ncein = torch.einsum('ij,ij-&gt;ij',c,d)\nprint(f'regular: {cmm} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]]) \n  einsum: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]])\n\n\nSource: einsum.ipynb\n\n\n9. Outer Product (vector multiply vector)\n\\[C_{ij} = a_{i}b_{j}\\]\nx = torch.rand(3)\ny = torch.rand(5)\nprint(f'x: {x}, x: {x.shape}')\nprint(f'y: {y}, y: {y.shape}')\n\nc_outer = torch.outer(x,y)\ncein = torch.einsum('i,j-&gt;ij',x,y)\nprint(f'regular: {c_outer} \\n  einsum: {cein}')\n\n\n\nx: tensor([0.91, 0.93, 0.21]), x: torch.Size([3])\ny: tensor([0.98, 0.62, 0.25, 0.90, 0.21]), y: torch.Size([5])\nregular: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]]) \n  einsum: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]])\n\n\nSource: einsum.ipynb\n\n\n10. batch matrix multiplication\n\\[C_{ijl}= \\sum_k{}A_{ijk}{B_{ikl}}\\]\nR = torch.rand(3,2,6)\nS = torch.rand(3,6,3)\ncmn = np.matmul(R,S)\ncmm = torch.matmul(R,S)\n\ncein = torch.einsum('ijk,ikl-&gt;ijl',R,S)\n\nprint(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n\n\n\nregular: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n numpy: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]]) \n  einsum: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n\n\nSource: einsum.ipynb\n\n\n11. Diagonal Matrix (return only the diagonal value of a matrix where all other value are 0)\n\\[\n\\forall i,j \\in \\{1, 2, \\ldots, n\\}, i \\ne j \\implies d_{i,j} = 0\n\\]\nT = torch.rand(3,3)\n\ncein = torch.einsum('ii-&gt;i',T)\nprint(f'T: {T} \\nT shape: {T.shape}')\nc_diag = torch.diag(T)\n\nprint(f'regular: {c_diag} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]]) \nT shape: torch.Size([3, 3])\nregular: tensor([0.33, 0.82, 0.18]) \n  einsum: tensor([0.33, 0.82, 0.18])\n\n\nSource: einsum.ipynb\n\n\n12. Trace (take sum along diagonal axis; square matrix only)\n\\[tr(A)= \\sum_i{a_{ii}}\\]\nc_trace = torch.trace(T)\ncein = torch.einsum('ii-&gt;',T)\nprint(f'T: {T}')\nprint(f'regular: {c_trace} \\n  einsum: {cein}')\n\n\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]])\nregular: 1.3291736841201782 \n  einsum: 1.3291736841201782\n\n\nSource: einsum.ipynb\n\n\n13. Tensor contraction\n\\[C_{ilmno} = \\sum_j{}\\sum_k{A_{ijkl}{B_{mnjok}}}\\]\no = torch.rand((3,4,2))\np = torch.rand((4,3,6))\nprint(f'value: {o.shape} value2: {p.shape}')\n\nc_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\ncein = torch.einsum('ijk,jil-&gt;kl',o,p)\nprint(f'regular: {c_tdot} \\n  einsum: {cein}')\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb\n\n\n14. Bilinear Transformation\n\\[C_{im} = \\sum_j{}\\sum_o{A_{ij}{B_{mjo}}C_{io}}\\]\na = torch.rand(2,3)\nb = torch.rand(5,3,7)\nc = torch.rand(2,7)\n\ntorch.einsum('ik,jkl,il-&gt;ij',[a,b,c])\n\n\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nSource: einsum.ipynb"
  },
  {
    "objectID": "Programming/Languages/Julia/julia.html",
    "href": "Programming/Languages/Julia/julia.html",
    "title": "Julia",
    "section": "",
    "text": "Here is the newest einsum call OMEinsum.jl package in Julia. It’s implement using about 20 lines of Julia code! Amazing."
  },
  {
    "objectID": "Programming/Languages/Julia/julia.html#about-tree-and-graph-data-structures",
    "href": "Programming/Languages/Julia/julia.html#about-tree-and-graph-data-structures",
    "title": "Julia",
    "section": "",
    "text": "Einsum in Julia\n\n\nCode\nusing Plots, OMEinsum, LinearAlgebra\n\nplot(sin, \n     x-&gt;sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))\n\n\n\n\n\nFigure 1: Parametric Plots\n\n\n\n\n\n\nCode\nusing Plots, OMEinsum, LinearAlgebra\n\nOMEinsum.asarray(a::Number, ::Diagonal) = fill(a,())\nein\"ij -&gt; \"(Diagonal([1,2,3]))\n\nfunction OMEinsum.einsum(::OMEinsum.Sum, ixs, iy, xs::Tuple{&lt;:Diagonal}, size_dict::Dict)\n    length(iy) == 1 && return diag(xs[1])\n    return sum(diag(xs[1]))\nend\n\nein\"ij -&gt; i\"(Diagonal([1,2,3]))\n\n\n3-element Vector{Int64}:\n 1\n 2\n 3"
  },
  {
    "objectID": "Programming/Languages/Python/Untitled.html",
    "href": "Programming/Languages/Python/Untitled.html",
    "title": "MachineIntell",
    "section": "",
    "text": "using OMEinsum, LinearAlgebra\n\nOMEinsum.asarray(a::Number, ::Diagonal) = fill(a,())\n\nLoadError: ArgumentError: Package OMEinsum not found in current path.\n- Run `import Pkg; Pkg.add(\"OMEinsum\")` to install the OMEinsum package."
  },
  {
    "objectID": "Programming/Languages/Python/Untitled1.html",
    "href": "Programming/Languages/Python/Untitled1.html",
    "title": "MachineIntell",
    "section": "",
    "text": "using OMEinsum, LinearAlgebra\n\nOMEinsum.asarray(a::Number, ::Diagonal) = fill(a,())\n\n\nein\"ij -&gt; \"(Diagonal([1,2,3]))\n\n0-dimensional Array{Int64, 0}:\n6\n\n\n\nfunction OMEinsum.einsum(::OMEinsum.Sum, ixs, iy, xs::Tuple{&lt;:Diagonal}, size_dict::Dict)\n    length(iy) == 1 && return diag(xs[1])\n    return sum(diag(xs[1]))\nend\n\n\nein\"ij -&gt; i\"(Diagonal([1,2,3]))\n\n3-element Vector{Int64}:\n 1\n 2\n 3"
  },
  {
    "objectID": "Programming/Languages/Julia/julia.html#einsum-in-julia",
    "href": "Programming/Languages/Julia/julia.html#einsum-in-julia",
    "title": "Julia",
    "section": "Einsum in Julia",
    "text": "Einsum in Julia\n\n\nCode\nusing Plots, OMEinsum, LinearAlgebra, SymEngine\n\nplot(sin, \n     x-&gt;sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))\n\n\n\n\n\nFigure 1: Parametric Plots\n\n\n\n\n\n\nCode\nusing OMEinsum, LinearAlgebra\n\nOMEinsum.asarray(a::Number, ::Diagonal) = fill(a,())\nein\"ij -&gt; \"(Diagonal([1,2,3]))\n\nfunction OMEinsum.einsum(::OMEinsum.Sum, ixs, iy, xs::Tuple{&lt;:Diagonal}, size_dict::Dict)\n    length(iy) == 1 && return diag(xs[1])\n    return sum(diag(xs[1]))\nend\n\n\n\n\nCode\nein\"ij -&gt; i\"(Diagonal([1,2,3]))\n\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n\nCode\ncatty = fill(Basic(:🐱), 2, 2)\n\n\n2×2 Matrix{Basic}:\n 🐱  🐱\n 🐱  🐱\n\n\n\n\nCode\nfish = fill(Basic(:🐟), 2, 3, 2)\n\n\n2×3×2 Array{Basic, 3}:\n[:, :, 1] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟\n[:, :, 2] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟\n\n\n\n\nCode\nsnake = fill(Basic(:🐍), 3, 3)\n\n\n3×3 Matrix{Basic}:\n 🐍  🐍  🐍\n 🐍  🐍  🐍\n 🐍  🐍  🐍\n\n\n\n\nCode\nmedicine = ein\"ij,jki,kk-&gt;k\"(catty, fish, snake)\n\n\n3-element Vector{Basic}:\n 4*🐱*🐍*🐟\n 4*🐱*🐍*🐟\n 4*🐱*🐍*🐟\n\n\n\n\nCode\nein\"ik,kj -&gt; ij\"(catty, catty) # multiply two matrices `a` and `b`\n\n\n2×2 Matrix{Basic}:\n 2*🐱^2  2*🐱^2\n 2*🐱^2  2*🐱^2\n\n\n\n\nCode\nein\"-&gt;ii\"(asarray(snake[1,1]), size_info=Dict('i'=&gt;5)) # get 5 x 5 identity matrix\n\n\n5×5 Matrix{Basic}:\n 🐍   0   0   0   0\n  0  🐍   0   0   0\n  0   0  🐍   0   0\n  0   0   0  🐍   0\n  0   0   0   0  🐍"
  },
  {
    "objectID": "Programming/Languages/Julia/julia.html#einstein-summation-in-julia",
    "href": "Programming/Languages/Julia/julia.html#einstein-summation-in-julia",
    "title": "Julia",
    "section": "",
    "text": "Here is the newest einsum call OMEinsum.jl package in Julia. It’s implement using about 20 lines of Julia code! Amazing."
  },
  {
    "objectID": "Programming/Languages/Julia/julia.html#einsum",
    "href": "Programming/Languages/Julia/julia.html#einsum",
    "title": "Julia",
    "section": "Einsum",
    "text": "Einsum\n\nDiagonal matrix\nreturn only the diagonal value of a square matrix and all other values in the matrix are 0 \\[\n\\forall i,j \\in \\{1, 2, \\ldots, n\\}, i \\ne j \\implies d_{i,j} = 0\n\\]\n\n\nCode\nusing OMEinsum, LinearAlgebra,SymEngine\nfish = fill(Basic(:🐟), 2, 3, 2)\n\n\n2×3×2 Array{Basic, 3}:\n[:, :, 1] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟\n[:, :, 2] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟"
  },
  {
    "objectID": "Programming/Languages/Python/julia_einsum.html",
    "href": "Programming/Languages/Python/julia_einsum.html",
    "title": "MachineIntell",
    "section": "",
    "text": "using OMEinsum, LinearAlgebra,SymEngine\n\nOMEinsum.asarray(a::Number, ::Diagonal) = fill(a,())\n\n\nein\"ij -&gt; \"(Diagonal([1,2,3]))\n\n6\n\n\n\nfunction OMEinsum.einsum(::OMEinsum.Sum, ixs, iy, xs::Tuple{&lt;:Diagonal}, size_dict::Dict)\n    length(iy) == 1 && return diag(xs[1])\n    return sum(diag(xs[1]))\nend\n\n\nein\"ij -&gt; i\"(Diagonal([1,2,3]))\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nfish = fill(Basic(:🐟), 2, 3, 2)\n\n2×3×2 Array{Basic, 3}:\n[:, :, 1] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟\n\n[:, :, 2] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟\n\n\n\ncatty = fill(Basic(:🐱), 2, 2)\n\n2×2 Matrix{Basic}:\n 🐱  🐱\n 🐱  🐱\n\n\n\nsnake = fill(Basic(:🐍), 3, 3)\n\n3×3 Matrix{Basic}:\n 🐍  🐍  🐍\n 🐍  🐍  🐍\n 🐍  🐍  🐍\n\n\n\nmedicine = ein\"ij,jki,kk-&gt;k\"(catty, fish, snake)\n\n3-element Vector{Basic}:\n 4*🐱*🐍*🐟\n 4*🐱*🐍*🐟\n 4*🐱*🐍*🐟\n\n\n\nein\"ik,kj -&gt; ij\"(catty, catty) # multiply two matrices `a` and `b`\n\n2×2 Matrix{Basic}:\n 2*🐱^2  2*🐱^2\n 2*🐱^2  2*🐱^2\n\n\n\nein\"-&gt;ii\"(asarray(snake[1,1]), size_info=Dict('i'=&gt;5)) # get 5 x 5 identity matrix\n\n5×5 Matrix{Basic}:\n 🐍   0   0   0   0\n  0  🐍   0   0   0\n  0   0  🐍   0   0\n  0   0   0  🐍   0\n  0   0   0   0  🐍"
  },
  {
    "objectID": "Programming/Languages/Julia/julia_einsum.html",
    "href": "Programming/Languages/Julia/julia_einsum.html",
    "title": "MachineIntell",
    "section": "",
    "text": "using OMEinsum, LinearAlgebra,SymEngine\n\nOMEinsum.asarray(a::Number, ::Diagonal) = fill(a,())\n\n\nein\"ij -&gt; \"(Diagonal([1,2,3]))\n\n0-dimensional Array{Int64, 0}:\n6\n\n\n\nfunction OMEinsum.einsum(::OMEinsum.Sum, ixs, iy, xs::Tuple{&lt;:Diagonal}, size_dict::Dict)\n    length(iy) == 1 && return diag(xs[1])\n    return sum(diag(xs[1]))\nend\n\n\nein\"ij -&gt; i\"(Diagonal([1,2,3]))\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nfish = fill(Basic(:🐟), 2, 3, 2)\n\n2×3×2 Array{Basic, 3}:\n[:, :, 1] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟\n\n[:, :, 2] =\n 🐟  🐟  🐟\n 🐟  🐟  🐟\n\n\n\ncatty = fill(Basic(:🐱), 2, 2)\n\n2×2 Matrix{Basic}:\n 🐱  🐱\n 🐱  🐱\n\n\n\nsnake = fill(Basic(:🐍), 3, 3)\n\n3×3 Matrix{Basic}:\n 🐍  🐍  🐍\n 🐍  🐍  🐍\n 🐍  🐍  🐍\n\n\n\nmedicine = ein\"ij,jki,kk-&gt;k\"(catty, fish, snake)\n\n3-element Vector{Basic}:\n 4*🐱*🐍*🐟\n 4*🐱*🐍*🐟\n 4*🐱*🐍*🐟\n\n\n\nein\"ik,kj -&gt; ij\"(catty, catty) # multiply two matrices `a` and `b`\n\n2×2 Matrix{Basic}:\n 2*🐱^2  2*🐱^2\n 2*🐱^2  2*🐱^2\n\n\n\nein\"-&gt;ii\"(asarray(snake[1,1]), size_info=Dict('i'=&gt;5)) # get 5 x 5 identity matrix\n\n5×5 Matrix{Basic}:\n 🐍   0   0   0   0\n  0  🐍   0   0   0\n  0   0  🐍   0   0\n  0   0   0  🐍   0\n  0   0   0   0  🐍"
  },
  {
    "objectID": "tip/tips.html#the-math-rule-that-govern-these-operation-are",
    "href": "tip/tips.html#the-math-rule-that-govern-these-operation-are",
    "title": "Julia Plots",
    "section": "The math rule that govern these operation are:",
    "text": "The math rule that govern these operation are:"
  },
  {
    "objectID": "tip/tips.html#python-specific",
    "href": "tip/tips.html#python-specific",
    "title": "Julia Plots",
    "section": "Python specific:",
    "text": "Python specific:"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html#eisntein-summation",
    "href": "Programming/Languages/Python/einsum.html#eisntein-summation",
    "title": "My Notes",
    "section": "",
    "text": "A technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHow does it work?\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nimport numpy as np\n\n\nA = np.random.rand(3,5)\nB = np.random.rand(5,2)\nM = np.empty((3,2))\n\nA.shape,B.shape,M.shape\n\n((3, 5), (5, 2), (3, 2))\n\n\n\nA,B,M\n\n(array([[6.82759644e-01, 3.32325362e-01, 5.10214353e-01, 8.76891766e-01,\n         1.72299180e-01],\n        [5.33438050e-01, 7.25523710e-01, 8.54886776e-01, 3.96140734e-01,\n         3.56604238e-01],\n        [5.45349948e-05, 8.97919640e-01, 1.86133084e-01, 8.73382052e-01,\n         1.11479305e-01]]),\n array([[0.18449366, 0.69112411],\n        [0.68456084, 0.92427429],\n        [0.39745901, 0.59819685],\n        [0.53124429, 0.82624474],\n        [0.37620928, 0.55749668]]),\n array([[1.13621302e-313, 0.00000000e+000],\n        [6.92770534e-310, 6.92770729e-310],\n        [6.92770292e-310, 6.92770291e-310]]))\n\n\n\nnp.set_printoptions(precision=2, linewidth=140)\n\n\n\nCoding as loop of the matrix multiplication above\nwhere the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\n\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\n\nthe matrix is: [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\nUsing the Eisum method\n\n\n# the i and j are free index\n# the k is sum index since it will be summed away after the operation\nM1 = np.einsum('ik,kj-&gt;ij',A,B)\n\nprint(f'the matix is : {M1}')\n\nthe matix is : [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\n\n\n\nVector or matrix multiplicaton \\(u \\cdot v\\) that result of scalar value\n\n\n\nvector multiply by vector that result in a matrix \\(u \\otimes v\\)\nExample 2 - Using the free index in the output\n- No summation index\n\n# example 2\nD = np.random.rand(5)\nE = np.random.rand(3)\nout = np.einsum('i,j-&gt;ij',D,E)\nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]\n\n\n\n# loop version\nfor i in range(5):\n    for j in range(3):\n        total = 0\n        total += D[i] * E[j]\n        out[i,j] = total\n        \nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html#eisnstein-summation",
    "href": "Programming/Languages/Python/einsum.html#eisnstein-summation",
    "title": "My Notes",
    "section": "",
    "text": "A technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHow does it work?\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nimport numpy as np\n\n\nA = np.random.rand(3,5)\nB = np.random.rand(5,2)\nM = np.empty((3,2))\n\nA.shape,B.shape,M.shape\n\n((3, 5), (5, 2), (3, 2))\n\n\n\nA,B,M\n\n(array([[6.82759644e-01, 3.32325362e-01, 5.10214353e-01, 8.76891766e-01,\n         1.72299180e-01],\n        [5.33438050e-01, 7.25523710e-01, 8.54886776e-01, 3.96140734e-01,\n         3.56604238e-01],\n        [5.45349948e-05, 8.97919640e-01, 1.86133084e-01, 8.73382052e-01,\n         1.11479305e-01]]),\n array([[0.18449366, 0.69112411],\n        [0.68456084, 0.92427429],\n        [0.39745901, 0.59819685],\n        [0.53124429, 0.82624474],\n        [0.37620928, 0.55749668]]),\n array([[1.13621302e-313, 0.00000000e+000],\n        [6.92770534e-310, 6.92770729e-310],\n        [6.92770292e-310, 6.92770291e-310]]))\n\n\n\nnp.set_printoptions(precision=2, linewidth=140)\n\n\n\nCoding as loop of the matrix multiplication above\nwhere the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\n\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\n\nthe matrix is: [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\nUsing the Eisum method\n\n\n# the i and j are free index\n# the k is sum index since it will be summed away after the operation\nM1 = np.einsum('ik,kj-&gt;ij',A,B)\n\nprint(f'the matix is : {M1}')\n\nthe matix is : [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\n\n\n\nVector or matrix multiplicaton \\(u \\cdot v\\) that result of scalar value\n\n\n\nvector multiply by vector that result in a matrix \\(u \\otimes v\\)\nExample 2 - Using the free index in the output\n- No summation index\n\n# example 2\nD = np.random.rand(5)\nE = np.random.rand(3)\nout = np.einsum('i,j-&gt;ij',D,E)\nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]\n\n\n\n# loop version\nfor i in range(5):\n    for j in range(3):\n        total = 0\n        total += D[i] * E[j]\n        out[i,j] = total\n        \nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html#einstein-summation",
    "href": "Programming/Languages/Python/einsum.html#einstein-summation",
    "title": "My Notes",
    "section": "",
    "text": "A technique for convenient and simplify the writing of syntax for many operation in linear algebra\n\nMatrix multiplication\nElement-wise matrix operation\nPermutation of matrix\nDot product of matrix\nOuter product of matrix\nSummation of matrix\nBatch multiplication of matrix (permute input to match function calls ordering)\n\nIt’s also speedup some of the above operation especially operation that can be combined into single call\nThe eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux…\nHow does it work?\nHere is the matrix multiplication\n\\[ M_{ij} = \\sum{A_{ik}B_{kj}} = A_{ik}B_{kj}\\]\n\nimport numpy as np\n\n\nA = np.random.rand(3,5)\nB = np.random.rand(5,2)\nM = np.empty((3,2))\n\nA.shape,B.shape,M.shape\n\n((3, 5), (5, 2), (3, 2))\n\n\n\nA,B,M\n\n(array([[6.82759644e-01, 3.32325362e-01, 5.10214353e-01, 8.76891766e-01,\n         1.72299180e-01],\n        [5.33438050e-01, 7.25523710e-01, 8.54886776e-01, 3.96140734e-01,\n         3.56604238e-01],\n        [5.45349948e-05, 8.97919640e-01, 1.86133084e-01, 8.73382052e-01,\n         1.11479305e-01]]),\n array([[0.18449366, 0.69112411],\n        [0.68456084, 0.92427429],\n        [0.39745901, 0.59819685],\n        [0.53124429, 0.82624474],\n        [0.37620928, 0.55749668]]),\n array([[1.13621302e-313, 0.00000000e+000],\n        [6.92770534e-310, 6.92770729e-310],\n        [6.92770292e-310, 6.92770291e-310]]))\n\n\n\nnp.set_printoptions(precision=2, linewidth=140)\n\n\n\nCoding as loop of the matrix multiplication above\nwhere the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule\n\nfor i in range(3):\n    for j in range(2):\n        total = 0\n        for k in range(5):\n            total += A[i,k] * B[k,j]\n        M[i,j] = total\n\nprint(f'the matrix is: {M}')\n\nthe matrix is: [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\nUsing the Eisum method\n\n\n# the i and j are free index\n# the k is sum index since it will be summed away after the operation\nM1 = np.einsum('ik,kj-&gt;ij',A,B)\n\nprint(f'the matix is : {M1}')\n\nthe matix is : [[1.09 1.9 ]\n [1.28 2.08]\n [1.19 1.73]]\n\n\n\n\n\nVector or matrix multiplicaton \\(u \\cdot v\\) that result of scalar value\n\n\n\nvector multiply by vector that result in a matrix \\(u \\otimes v\\)\nExample 2 - Using the free index in the output\n- No summation index\n\n# example 2\nD = np.random.rand(5)\nE = np.random.rand(3)\nout = np.einsum('i,j-&gt;ij',D,E)\nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]\n\n\n\n# loop version\nfor i in range(5):\n    for j in range(3):\n        total = 0\n        total += D[i] * E[j]\n        out[i,j] = total\n        \nprint(f'the matrix is: \\n{out}')\n\nthe matrix is: \n[[0.32 0.69 0.7 ]\n [0.37 0.8  0.8 ]\n [0.28 0.61 0.61]\n [0.37 0.79 0.79]\n [0.17 0.36 0.36]]"
  },
  {
    "objectID": "Programming/Languages/Python/einsum.html#the-einsum-rules",
    "href": "Programming/Languages/Python/einsum.html#the-einsum-rules",
    "title": "My Notes",
    "section": "",
    "text": "The free indices: - The index that specify the output\nThe Summation index\n- All other indices that appear in the input argument but not show up in the output\nThe General rules: 1. Same index in the a different input argument indicate that these indices will be multiplied and the product are outputed\n    M = np.einsum('ik,kj-&gt;ij',A,B)\n\nOmitting index indicate the index will be summed together\n\n    X = np.ones(3)\n    Y = np.einsum('i-&gt;',X)\n\nThe unsummed indices may return in any order\n\n    D = np.ones((5,4,3))\n    E = np.einsum('ijk-&gt;kji',D)\nOperation that benefit from Einsum 1. Permutation of Tensors 2. Summation 3. Column sum 4. Row sum 5. Matrix-Vector multiplication 6. Matrix-Matrix multiplication 7. Dot Product the first row with first row of a matrix 8. Dot product with matrix (multiplication and add) 9. Element-wise multiplication (Hadamard Product) (multiplication no add) 10. Outer Product 11. Batch matrix multiplicaton e.g. a = 3,2,6 and b = 3,6,3 - want to multiply the matrix of 2x6 with 6x3 matrix - these matrix must follow the multiplication rule - the first number is the batch size they must match, but not count as index - the torch.bmm function will do the same thing 12. Matrix diagonal\n- return the only the diagonal value of the matrix 13. Matrix Trace - summing the value of the diagonal of a matrix\n14. Tensor contration\n- shrinking the dimension of tensor\n15. Bilinear transformation\n\n# !pip install torch\n\n\nimport torch\nimport numpy as np\n\ntorch.set_printoptions(precision=2, linewidth=140)\n\nX = torch.rand((2,3))\nX\n\ntensor([[0.25, 0.74, 0.66],\n        [0.96, 0.80, 0.44]])\n\n\n\nTranspose Flipping the matrix or vector by switching the index of a matrix or vector\n\n\n\nCode\nc_ntp = np.transpose(X)\nc_tp = torch.transpose(X,0,1)\ncein = torch.einsum('ij-&gt;ji',X)\nprint(f'numpy: {c_ntp} \\npytorch: {c_tp}\\n \\neinsum: {cein}')\n\n\nnumpy: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]]) \npytorch: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n \neinsum: tensor([[0.25, 0.96],\n        [0.74, 0.80],\n        [0.66, 0.44]])\n\n\n\nMatrix summation\nSumming all value in the matrix that result in a scalar value\n\n\n#2. Summation\ncma = torch.sum(X)\ncein = torch.einsum('ij-&gt;',X)\n\nprint(f'regular: {cma} \\neinsum: {cein}')\n\nregular: 3.8526394367218018 \neinsum: 3.8526394367218018\n\n\n\nX\n\ntensor([[0.25, 0.74, 0.66],\n        [0.96, 0.80, 0.44]])\n\n\nRow sum (Left to right)\nAdd all values from each column together along the row\n\n#4 row summation\n# sum by columns\nrows = torch.sum(X,dim=0)\ncein = torch.einsum('ij-&gt;i',X)\n\nprint(f'regular: {rows} \\neinsum: {cein}')\n\nregular: tensor([1.21, 1.54, 1.10]) \neinsum: tensor([1.65, 2.20])\n\n\nColumn sum (Top down)\nAdd all value from each row together along the column\n\n#3 Column summation\n# sum by rows\nc_col = torch.sum(X,dim=1)\ncein = torch.einsum('ij-&gt;j',X)\n\nprint(f'regular: {c_col} \\neinsum: {cein}')\n\nregular: tensor([1.65, 2.20]) \neinsum: tensor([1.21, 1.54, 1.10])\n\n\n5 matrix-vector multiplication\nThis a non equal dimension multiplication which in Python use broadcasting to padded (duplicate) the smaller vector to have equal size with the larger matrix before do multiplication\n\n#5 matrix-vector multiplication\nL = torch.rand((1,3))\nM = torch.rand((3,))\n\ncmm = torch.matmul(L,M)\ncein = torch.einsum('ij,j-&gt;i',L,M)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\nregular: tensor([0.76]) \neinsum: tensor([0.76])\n\n\n6 matrix-matrix multiplication\nThis standard matrix to matrix multiplication which result in another matrix\n\n#6 matrix-matrix multiplication\n# torch.einsum('ij,kj-&gt;ik',M,M)\n\na = torch.ones((3,2))\nb = torch.ones((2,3))\ncmm = torch.matmul(a,b)\ncein = torch.einsum('ij,jl-&gt;il',a,b)\nprint(f'regular: {cmm} \\neinsum: {cein}')\n\nregular: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]]) \neinsum: tensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n\n\nN = torch.rand((3,3))\nM = torch.rand((2,3))\ntorch.einsum('ij,kj-&gt;ik',N,M)\n\ntensor([[0.98, 0.72],\n        [0.64, 0.60],\n        [1.58, 1.42]])\n\n\nDot product\nThis a matrix multiplication that result in a scalar value. It’s usually called multiply add.\nsince after multiply the row to the column then the sum operation is carry out resulting as a scalar value\n:bulb:\n\n# dot product of a matrix (multiply + add)\n#torch.einsum('ij,ij-&gt;',N,N)\n\n# c = torch.tensor([2,3])\n# d = torch.tensor([2,1])\nc = torch.rand((3))\nd = torch.rand((3))\n\nc_dot = torch.dot(c,d)\ncein = torch.einsum('i,i-&gt;',c,d)\n\nprint(f'c: {c}, c: {c.shape}')\nprint(f'c_dot: {c_dot}')\nprint(f'regular: {c_dot} \\n  einsum: {cein}')\n\nc: tensor([0.38, 0.88, 0.10]), c: torch.Size([3])\nc_dot: 0.3021569550037384\nregular: 0.3021569550037384 \n  einsum: 0.3021569550037384\n\n\n\n# dot product of only the first row of a matrix with first row of a matrix\ntorch.einsum('i,i-&gt;',N[0],N[0])\n\ntensor(0.89)\n\n\nHadamard Product Element wise multiplication (multiply only)\nThis is a normal matrix multiplication which different from multiply add or dot product\n\n# element wise multiplication (multiply only)\n# torch.einsum('ij,ij-&gt;ij',N,N)\n\nc = torch.randn((3,2))\nd = torch.randn((3,2))\ncmm = c * d\ncein = torch.einsum('ij,ij-&gt;ij',c,d)\nprint(f'regular: {cmm} \\n  einsum: {cein}')\n\nregular: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]]) \n  einsum: tensor([[ 1.20,  0.97],\n        [ 0.14, -0.19],\n        [-0.17, -0.11]])\n\n\nOuter Product\nMultiply vector of different size to get a matrix as output In eisum must use different letter for index to represent size different\n\n# outer product (inner product)\nx = torch.rand(3)\ny = torch.rand(5)\nprint(f'x: {x}, x: {x.shape}')\nprint(f'y: {y}, y: {y.shape}')\n\nc_outer = torch.outer(x,y)\ncein = torch.einsum('i,j-&gt;ij',x,y)\nprint(f'regular: {c_outer} \\n  einsum: {cein}')\n\nx: tensor([0.91, 0.93, 0.21]), x: torch.Size([3])\ny: tensor([0.98, 0.62, 0.25, 0.90, 0.21]), y: torch.Size([5])\nregular: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]]) \n  einsum: tensor([[0.89, 0.56, 0.23, 0.81, 0.19],\n        [0.91, 0.57, 0.23, 0.83, 0.19],\n        [0.20, 0.13, 0.05, 0.18, 0.04]])\n\n\nBatch matrix multiplication\nMultiply matrix by the set of n, where n is batch size\nwant to multiply 3 set of the matrix of 2x6 with 6x3 matrix\nthe first number is the batch size must match but not count as index so i is ignore\nthe mxn * nxp must match with n\n\n# batch matrix multiplicaton\n# want to multiply 3 set of the matrix of 2x6 with 6x3 matrix\n# the first number is the batch size must match but not count as index so i is ignore\n# the mxn * nxp must match with n\nR = torch.rand(3,2,6)\nS = torch.rand(3,6,3)\ncmn = np.matmul(R,S)\ncmm = torch.matmul(R,S)\n\ncein = torch.einsum('ijk,ikl-&gt;ijl',R,S)\n\nprint(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n\nregular: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n numpy: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]]) \n  einsum: tensor([[[1.55, 1.48, 1.68],\n         [1.26, 1.02, 1.42]],\n\n        [[1.81, 1.20, 1.37],\n         [1.91, 0.71, 0.93]],\n\n        [[0.75, 0.92, 1.20],\n         [1.32, 1.29, 1.85]]])\n\n\nDiagonal Matrix return the vector of value along the diagonal of a matrix\n\n# Diagonal matrix return only the diagonal value of a matrix\n\nT = torch.rand(3,3)\n\ncein = torch.einsum('ii-&gt;i',T)\nprint(f'T: {T} \\nT shape: {T.shape}')\nc_diag = torch.diag(T)\n\nprint(f'regular: {c_diag} \\n  einsum: {cein}')\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]]) \nT shape: torch.Size([3, 3])\nregular: tensor([0.33, 0.82, 0.18]) \n  einsum: tensor([0.33, 0.82, 0.18])\n\n\n\n# torch.einsum('ii-&gt;i',T)\n\nTrace\nTake the sum of all values along the diagonal axix of a matrix\n\n# matrix trace\n\nc_trace = torch.trace(T)\ncein = torch.einsum('ii-&gt;',T)\nprint(f'T: {T}')\nprint(f'regular: {c_trace} \\n  einsum: {cein}')\n\nT: tensor([[0.33, 0.11, 0.63],\n        [0.60, 0.82, 0.28],\n        [0.08, 0.99, 0.18]])\nregular: 1.3291736841201782 \n  einsum: 1.3291736841201782\n\n\nTensor Contraction\nShrinking the dimension of the tensor\nmust provide the dimension to be ignored\n\no = torch.rand((3,4,2))\np = torch.rand((4,3,6))\nprint(f'value: {o.shape} value2: {p.shape}')\n\nc_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\ncein = torch.einsum('ijk,jil-&gt;kl',o,p)\nprint(f'regular: {c_tdot} \\n  einsum: {cein}')\n\nvalue: torch.Size([3, 4, 2]) value2: torch.Size([4, 3, 6])\nregular: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]]) \n  einsum: tensor([[3.72, 2.19, 3.15, 3.26, 2.88, 1.90],\n        [5.16, 3.22, 4.84, 4.64, 4.52, 3.33]])\n\n\nBilinear transformation\n\na = torch.rand(2,3)\nb = torch.rand(5,3,7)\nc = torch.rand(2,7)\n\ntorch.einsum('ik,jkl,il-&gt;ij',[a,b,c])\n\ntensor([[3.12, 3.11, 2.71, 3.34, 2.55],\n        [3.39, 2.72, 2.68, 3.09, 2.96]])"
  },
  {
    "objectID": "tip/tips.html",
    "href": "tip/tips.html",
    "title": "Working with Tensors",
    "section": "",
    "text": "Working with Tensors\nTensor is multi-dimension arrays\n\nVector is 1D array\nMatrix is 2D array\nTensor is nD array\n\nSince tensor is core operation of Neural Network. Understand how to manipulate Tensor efficiently is the diffenrence between success and failure. The majority of the tasks are transforming different shape and size of these arrays to match the expected shape and size of the API.\n\n\n\n\n\n\nTip\n\n\n\nMany of these operations involve:\n\nSqueezing matrix into vector\n\nAdd dimension to vector to transform it into matrix\n\nConvert tensor into matrix or vice versa\n\nConvert tensor into vector or vice versa\n\n\n\n\nMost often use math rule that govern these operation are:\n\nDot-product\n\nMatrix multiplication\nMultiply scalar value to matrix\nMatrix addition\n\nPython has its own specific tools for these operation:\n\nVectorization\n\nElement-wise multiplication\nMatrix multiplication\n\nPytorch or Numpy specific tools:\n\nEinstein summation\n\n\n\n\n\n\n\nTip\n\n\n\nWhen working with these tensors always check the shape and size:\n\ndon’t forget to peek into the tensor as often as possible\n\ncheck the shape\ncheck the size\ncheck the content of the tensor\nverify that they are what they should be\n\n\n\n\nAnother useful field to know is statistic\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tip/tips.html#another-useful-field-to-know-is-statistic",
    "href": "tip/tips.html#another-useful-field-to-know-is-statistic",
    "title": "Working with Tensors",
    "section": "Another useful field to know is statistic",
    "text": "Another useful field to know is statistic"
  },
  {
    "objectID": "tip/tips.html#section",
    "href": "tip/tips.html#section",
    "title": "Working with Tensors",
    "section": "",
    "text": "Another useful field to know is statistic"
  },
  {
    "objectID": "tip/tips.html#statistic",
    "href": "tip/tips.html#statistic",
    "title": "Working with Tensors",
    "section": "statistic",
    "text": "statistic\nAnother useful field to know is statistic"
  },
  {
    "objectID": "tip/tips.html#statistics",
    "href": "tip/tips.html#statistics",
    "title": "Working with Tensors",
    "section": "Statistics",
    "text": "Statistics\nAnother useful field to know is statistic it’s one of the essential tool to analyze the behavior of these matrices and model that you’re working with.\nHere are some of th most often use:\n\nProbability\n\nMean\n\nVariance\n\nStandard deviation\n\n\n\n\n\n\n\nTip\n\n\n\n\nMean and variance are crucial to understand many tasks"
  }
]
---
title: Attention
subtitle: Attention and Transformer
about:
  template: marquee
  image: ../images/gradientv12.jpg 
  links:
    - icon: twitter
      text: twitter
      href: https://twitter.com
    - icon: github
      text: Github
      href: https://github.com
listing:
  sort: "date desc"
  contents: "posts"
  sort-ui: false
  filter-ui: false
  categories: true
  feed: true

page-layout: full
---

![](../AI/images/Transformer1.webp)

---
## Transformer  
An Architecture That have attention at the heart of the system. It consists of the encoder-decoder unit without recurrence or convolution. It requires that the position of the token in the input be provided since its cannot capture this information on its own.  

These positions information are generated with the sine and cosine function with different frequencies then sum with the embedding vector before pass on as the input to the Attention layer


### Components
Each layer have 2 sublayers with residual connections:  
1. Implement multi-heads attentions for parallel processing  
2. Implement fully connected feed forward network with 2 linear units with ReLU activation functon sandwich in between
$$ffn_{x} = ReLU(W_{1x} + b_1)W_2 + b_2$$  

The output from this unit then normalize by the layernorm layer that also take the value directly from the input $x$
$$layernorm( x + sublayer(x))$$

![](../AI/images/transformer3.png)

### Encoder
The encoder unit job is to map the input token to sequence of attented tokens to be later feed to the decoder.  


### Decoder
The decoder unit combines the input from previous state with the input from encoder state plus the input from the feed-forward unit then generate the output




## The Attention
A mechanism that let model extract information about previous tokens from stream of input plus its own state to generate information about the current token.   
- It also assign weight and relevancy to the token so   
- it can assert the priority of the token as how much attention that it should payed to the token.

### How does it work?  
Each attention unit comprise with three matrices that contains the weight of relevant the query $W_Q$ which represent the query for information, the key $W_K$ the holder of interested information, and the value $W_V$ the actual requested information.  

When a token is looking for information to assess its current state it generate a query vector $q_i = x_iW_Q$ to search for potential relevant information holder vector $k_i = x_iW_K$ of required information $v_i = x_iW_v$ 

To determine if the information match the requested a *dot product*  is perform between the two vectors $q_i$ and $k_j$ if the resultant value is large there is a match otherwise there's no match.

To calculate the attention value of all tokens together use the equation 
$$ Attention(Q,K,V) = softmax \bigg(\frac{QK^T}{\sqrt{d_k}} \bigg)V$$  
where Q is the query matrix  
      K is the key  matrix  
      V is the value matrix  
      $\sqrt{d_k}$ is the stability gradient factor to prevent fluctuation during training computation  


#### Attention head
A set of $W_Q,W_K,W_V$ is called a head. Each head may be assigned to process tokens and tasks that are relevant to a token. Each layer of the model may have many heads which's called *multi-head Attention* this increase the capability of the model process many different tokens in parallel.

#### Feed-Forward Unit
The output from these processing may be passed on to the feed-forward unit for additional process.  
This unit is another neural network that comprise with normalization unit,and residual unit.
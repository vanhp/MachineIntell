---
title: Attention
subtitle: Attention and Transformer
about:
  template: marquee
  image: ../images/gradientv12.jpg 
  links:
    - icon: twitter
      text: twitter
      href: https://twitter.com
    - icon: github
      text: Github
      href: https://github.com
listing:
  sort: "date desc"
  contents: "posts"
  sort-ui: false
  filter-ui: false
  categories: true
  feed: true

page-layout: full
---

![](../AI/images/Transformer1.webp)

---
## Transformer  
An Architecture That have attention at the heart of the system. It consists of the encoder-decoder unit without recurrence or convolution. It requires that the position of the token in the input be provided since its cannot capture this information on its own.  

These positions information are generated with the sine and cosine function with different frequencies then sum with the embedding vector before pass on as the input to the Attention layer


### embeddings (capturing semantic)

A process that convert the sequence of tokens e.g. text which has low dimension(1D) into higher dimension and store on the vector while also capture the meaning in the input tokens.  
Tokens that have similar meaning are grouping closer to each orther while tokens that have opposite meaning are placing further away.  
These vectors are suitable to be passed on to neural network model to learn on.

### Components
Each layer have 2 sublayers with residual connections:  
1. Implement multi-heads attentions for parallel processing  
2. Implement fully connected feed forward network with 2 linear units with ReLU activation functon sandwich in between
$$ffn_{x} = ReLU(W_{1x} + b_1)W_2 + b_2$$  

The output from this unit then normalize by the layernorm layer that also take the value directly from the input $x$
$$layernorm( x + sublayer(x))$$

![](../AI/images/transformer3.png)

### Encoder
The encoder unit job is to map all the input tokens (word) to sequence of attented tokens to be later feed to the decoder.     
These inputs tokens has been through embedding process and ready for the layer to extract information from.



### Decoder
The decoder unit role is to extract information only from all the tokens that preceeding the token that the model is expect to predict. Hence, the model doesn't know before hand what token(word) it's supposed to be. 


But it can combines the input from previous token's state with the input from encoder state plus the input from the feed-forward unit then generate the output

For multi-head attention
It's require a mask to extract only the relevance value and suppress the rest.

$$mask(QK^T) = mask \bigg(\begin{bmatrix}
   a_{11} & a_{11} & ... a_{1n} \\
   a_{21} & a_{22} &... a_{2n} \\
   a_{n1} & a_{n2} & ... a_{nn} 
\end{bmatrix}\bigg) = \begin{bmatrix}
   a_{11} & a_{11} & ... a_{1n} \\
   a_{21} & a_{22} &... a_{2n} \\
   a_{n1} & a_{n2} & ... a_{nn} 
\end{bmatrix}$$



## The Attention
A mechanism that let model extract information about previous tokens from stream of input plus its own state to generate information about the current token.   
- It also assign weight and relevancy to the token so   
- it can assert the priority of the token as how much attention that it should payed to the token.

### How does it work?  
Each attention unit comprise with three matrices that contains the weight of relevant the query $W_Q$ which represent the query for information, the key $W_K$ the holder of interested information, and the value $W_V$ the actual requested information.  

When a token is looking for information to assess its current state it generate a query vector $q_i = x_iW_Q$ to search for potential relevant information holder vector $k_i = x_iW_K$ of required information $v_i = x_iW_v$ 

To determine if the information match the requested a *dot product*  is perform between the two vectors $q_i$ and $k_j$ if the resultant value is large there is a match otherwise there's no match.

To calculate the attention value of all tokens together use the equation 
$$ Attention(Q,K,V) = softmax \bigg(\frac{QK^T}{\sqrt{d_k}} \bigg)V$$  
where Q is the query matrix  
      K is the key  matrix  
      V is the value matrix  
      T is the time step
      $\sqrt{d_k}$ is the stability gradient factor to prevent fluctuation during training computation  


#### Attention head
A set of $W_Q,W_K,W_V$ is called a head. Each head may be assigned to process tokens and tasks that are relevant to a token. Each layer of the model may have many heads which's called *multi-head Attention* this increase the capability of the model process many different tokens in parallel.

#### Feed-Forward Unit
The output from these processing may be passed on to the feed-forward unit for additional process.  
This unit is another neural network that comprise with normalization unit,and residual unit.
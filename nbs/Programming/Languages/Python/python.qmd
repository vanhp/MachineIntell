
---
title: Python
subtitle: Python language
about:
  template: marquee
  image: ../../../images/gradientv12.jpg 
  links:
    - icon: twitter
      text: twitter
      href: https://twitter.com
    - icon: github
      text: Github
      href: https://github.com
listing:
  sort: "date desc"
  contents: "posts"
  sort-ui: false
  filter-ui: false
  categories: true
  feed: true
format: 
  html: 
    code-fold: false
page-layout: full
---

![](../../../images/code3.png)

## Einsum in Python

[Einsum](https://en.wikipedia.org/wiki/Einstein_notation) stand for  Einstein summation convention with was invented by Albert Einstein. A hidden gem that less popular but would be an asset for those who discovers it. It's especially useful in Machine learning field where Tensor operation is at the core of Neural Network.

The convention help simplify the calculation of Tensor indices thus speedup the computation. Write more concise and efficient code especially in Python which not known for speed. It has more impact on Framework such as Pytorch that also generate GPU code.
as describe by [Tim Rocktaschel](https://rockt.github.io/2018/04/30/einsum)

## Einstein Summation

A technique for convenient and simplify the writing of syntax for many operation in linear algebra

- Matrix multiplication
- Element-wise matrix operation
- Permutation of matrix
- Dot product of matrix
- Outer product of matrix
- Summation of matrix
- Batch multiplication of matrix (permute input to match function calls ordering)

It's also speedup some of the above operation especially operation that can be combined into single call

The eisum is a build-in feature of most Machine learning frameworks, e.g. Pytorch, Tensorflow, Flux...



Here is the matrix multiplication  
$$ M_{ij} = \sum{A_{ik}B_{kj}} = A_{ik}B_{kj}$$


### Code: 
The standard way to code the matrix multiplication is using a loop although this is simple but very inefficient with $O(n^2)$
where the row is i and column is j, and k is the inner dimension of both matrix that must be equal and this index will be summed and disappeared by the operation of matrix rule
```python
for i in range(3):
    for j in range(2):
        total = 0
        for k in range(5):
            total += A[i,k] * B[k,j]
        M[i,j] = total

print(f'the matrix is: {M}')
```

Here the einsum version just as simple but much more efficient in term of memory usage and computation speed up.
```python
a = torch.ones((3,2))
b = torch.ones((2,3))
cmm = torch.matmul(a,b)
cein = torch.einsum('ij,jl->il',a,b)
print(f'regular: {cmm} \neinsum: {cein}')
```

### The Eisum Rules
Here some of the convention for einsum

The free indices:  

- The index that specify the output

The Summation index   

- All other indices that appear in the input argument but not show up in the output

The General rules:  

1. Same index in the a different input argument indicate that these indices will be multiplied and the product are outputed
```python
    M = np.einsum('ik,kj->ij',A,B)
```
2. Omitting index indicate the index will be summed together
``` python
    X = np.ones(3)
    Y = np.einsum('i->',X)
```
3. The unsummed indices may return in any order
``` python
    D = np.ones((5,4,3))
    E = np.einsum('ijk->kji',D)
```

### Operations that benefit from Einsum  
Not all tensors operation can benefit from using einsum, however here are some of them that do.

1. Permutation of Tensors  
2. Summation  
3. Column sum  
4. Row sum  
5. Matrix-Vector multiplication  
6. Matrix-Matrix multiplication  
7. Dot Product the first row with first row of a matrix  
8. Dot product with matrix (multiplication and add)  
9. Element-wise multiplication (Hadamard Product) (multiplication no add)  
10. Outer Product  
11. Batch matrix multiplicaton e.g. a = 3,2,6 and b = 3,6,3  
    - want to multiply the matrix of 2x6 with 6x3 matrix  
    - these matrix must follow the multiplication rule  
    - the first number  is the batch size they must match, but not count as index   
    - the torch.bmm function will do the same thing  
12. Matrix diagonal    
    - return the only the diagonal value of the matrix  
13. Matrix Trace  
    - summing the value of the diagonal of a matrix  
14. Tensor contration
    - shrinking the dimension of tensor
15. Bilinear transformation

### Einsum in action
Here some sample codes that use either Numpy, Pytorch compare to einsum to calculate Tensor

##### 1.  Transpose

Flipping the matrix or vector by switching the index of a matrix or vector
$$A_{ij} = B_{ji}$$

```python
c_ntp = np.transpose(X)
c_tp = torch.transpose(X,0,1)
cein = torch.einsum('ij->ji',X)
print(f'numpy: {c_ntp}\n pytorch: {c_tp}\n \neinsum: {cein}')
```
{{< embed einsum.ipynb#mat-transpose echo=true >}}

##### 2. Summation of matrix
Summing all values in the matrix that result in a scalar value
$$b = \sum_i{}\sum_j{A_{ij}} = A_{ij}$$
```python
cma = torch.sum(X)
cein = torch.einsum('ij->',X)

print(f'regular: {cma} \neinsum: {cein}')
```
{{< embed einsum.ipynb#matrix-sum echo=true >}}

##### 3. Row summation (1xn)

Add all values from each column together along the row
$$b_i = \sum_{j}{Aij}$$
```python
rows = torch.sum(X,dim=0)
cein = torch.einsum('ij->i',X)

print(f'regular: {rows} \neinsum: {cein}')
```
{{< embed einsum.ipynb#row-summation echo=true >}}

##### 4. Comlumn summation (mx1)

Add all values from each row together along the column
$$b_j = \sum_i{A_{ij}}$$
```python
c_col = torch.sum(X,dim=1)
cein = torch.einsum('ij->j',X)

print(f'regular: {c_col} \neinsum: {cein}')
```
{{< embed einsum.ipynb#column-summation echo=true >}}

##### 5. Matrix-vector multiplication
This a non equal dimension multiplication which in Python use broadcasting to padded (duplicate) the smaller vector to have equal size with the larger matrix before do multiplication
$$c_i = \sum_k{A_{ik}}b_k$$
```python
L = torch.rand((1,3))
M = torch.rand((3,))

cmm = torch.matmul(L,M)
cein = torch.einsum('ij,j->i',L,M)
print(f'regular: {cmm} \neinsum: {cein}')
```
{{< embed einsum.ipynb#matrix-vector echo=true >}}

##### 6. Matrix-Matrix multiplication
$$C_{ij}= \sum_k{}A_{ik}{B_{kj}}$$
```python
a = torch.ones((3,2))
b = torch.ones((2,3))
cmm = torch.matmul(a,b)
cein = torch.einsum('ij,jl->il',a,b)
print(f'regular: {cmm} \neinsum: {cein}')
```
{{< embed einsum.ipynb#matrix-matrix echo=true >}}

##### 7. Dot product (inner product)  
This a matrix multiplication that result in a scalar value. It's usually called multiply-add.  
since after multiply the row to the column then the sum operation is carry out resulting as a scalar value

vector: $$c = \sum_{i}{a_i}{b_i}$$
matrix: $$d = \sum_i{}\sum_j{A_{ij} B_{ij}}$$
```python
c = torch.rand((3))
d = torch.rand((3))

c_dot = torch.dot(c,d)
cein = torch.einsum('i,i->',c,d)

print(f'c: {c}, c: {c.shape}')
print(f'c_dot: {c_dot}')
print(f'regular: {c_dot} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#dot-product echo=true >}}

##### 8. Hadamard Product (elementwise multiplication without add)
Element wise multiplication (multiply only)  
This is a normal matrix multiplication which different from multiply add or dot product
$$C_{ij} = A_{ij}B_{ij}$$
```python
c = torch.randn((3,2))
d = torch.randn((3,2))
cmm = c * d
cein = torch.einsum('ij,ij->ij',c,d)
print(f'regular: {cmm} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#Hadamard-product echo=true >}}

##### 9. Outer Product (vector multiply vector)
Multiply vector of different size to get a matrix as output  
In eisum must use different letter for index to represent size different

$$C_{ij} = a_{i}b_{j}$$
```python
x = torch.rand(3)
y = torch.rand(5)
print(f'x: {x}, x: {x.shape}')
print(f'y: {y}, y: {y.shape}')

c_outer = torch.outer(x,y)
cein = torch.einsum('i,j->ij',x,y)
print(f'regular: {c_outer} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#outer-product echo=true >}}

##### 10. Batch matrix multiplication
Multiply matrix by the set of n, where n is batch size  

for example multiply 3 set of the matrix of 2x6 with 6x3 matrix  
the first number is the batch size must match but not count as index so i is ignore  
the mxn * nxp must match with n as require by multiplication rule
$$C_{ijl}= \sum_k{}A_{ijk}{B_{ikl}}$$
```python
R = torch.rand(3,2,6)
S = torch.rand(3,6,3)
cmn = np.matmul(R,S)
cmm = torch.matmul(R,S)

cein = torch.einsum('ijk,ikl->ijl',R,S)

print(f'regular: {cmm}\n numpy: {cmn} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#batch-matrix echo=true >}}

##### 11. Diagonal Matrix 
return only the diagonal value of a square matrix 
and all other values in the matrix are 0
$$
\forall i,j \in \{1, 2, \ldots, n\}, i \ne j \implies d_{i,j} = 0
$$

```python
T = torch.rand(3,3)

cein = torch.einsum('ii->i',T)
print(f'T: {T} \nT shape: {T.shape}')
c_diag = torch.diag(T)

print(f'regular: {c_diag} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#diag-mat echo=true >}}

##### 12. Trace (take sum along diagonal axis; square matrix only)
Take the sum of all values along the diagonal axix of a matrix
$$tr(A)= \sum_i{a_{ii}}$$

```python
c_trace = torch.trace(T)
cein = torch.einsum('ii->',T)
print(f'T: {T}')
print(f'regular: {c_trace} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#trace echo=true >}}

##### 13. Tensor contraction
Shrinking the dimension of the tensor  
must provide the dimension to be ignored
$$C_{ilmno} = \sum_j{}\sum_k{A_{ijkl}{B_{mnjok}}}$$

```python
o = torch.rand((3,4,2))
p = torch.rand((4,3,6))
print(f'value: {o.shape} value2: {p.shape}')

c_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))
cein = torch.einsum('ijk,jil->kl',o,p)
print(f'regular: {c_tdot} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#contraction echo=true >}}

##### 14. Bilinear Transformation

$$C_{im} = \sum_j{}\sum_o{A_{ij}{B_{mjo}}C_{io}}$$
```python
a = torch.rand(2,3)
b = torch.rand(5,3,7)
c = torch.rand(2,7)

torch.einsum('ik,jkl,il->ij',[a,b,c])
```
{{< embed einsum.ipynb#contraction echo=true >}}



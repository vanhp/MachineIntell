---
title: Python
subtitle: Python language
about:
  template: marquee
  image: ../../../images/gradientv12.jpg 
  links:
    - icon: twitter
      text: twitter
      href: https://twitter.com
    - icon: github
      text: Github
      href: https://github.com
listing:
  sort: "date desc"
  contents: "posts"
  sort-ui: false
  filter-ui: false
  categories: true
  feed: true
format: 
  html: 
    code-fold: false
page-layout: full
---

![](../../../images/code3.png)

## Einsum in Python

[Einsum](https://en.wikipedia.org/wiki/Einstein_notation) stand for  Einstein summation convention with was invented by Albert Einstein.
The convention help simplify the calculation of Tensor indices thus speedup the computation. Write more concise and efficient code especially in Python which not known for speed. It has more impact on Framework such as Pytorch that also generate GPU code.
as describe by [Tim Rocktaschel](https://rockt.github.io/2018/04/30/einsum)

Here sample code for
Tensor calculation using Numpy, Pytorch, einsum where available

##### 1.  Transpose
$$A_{ij} = B_{ji}$$

```python
c_ntp = np.transpose(X)
c_tp = torch.transpose(X,0,1)
cein = torch.einsum('ij->ji',X)
print(f'numpy: {c_ntp}\n pytorch: {c_tp}\n \neinsum: {cein}')
```
{{< embed einsum.ipynb#mat-transpose echo=true >}}

##### 2. Summation of matrix
$$b = \sum_i{}\sum_j{A_{ij}} = A_{ij}$$
```python
cma = torch.sum(X)
cein = torch.einsum('ij->',X)

print(f'regular: {cma} \neinsum: {cein}')
```
{{< embed einsum.ipynb#matrix-sum echo=true >}}

##### 3. Row summation

$$b_i = \sum_{j}{Aij}$$
```python
rows = torch.sum(X,dim=0)
cein = torch.einsum('ij->j',X)

print(f'regular: {rows} \neinsum: {cein}')
```
{{< embed einsum.ipynb#row-summation echo=true >}}

##### 4. Comlumn summation

$$b_j = \sum_i{A_{ij}}$$
```python
c_col = torch.sum(X,dim=1)
cein = torch.einsum('ij->i',X)

print(f'regular: {c_col} \neinsum: {cein}')
```
{{< embed einsum.ipynb#column-summation echo=true >}}

##### 5. Matrix-vector multiplication

$$c_i = \sum_k{A_{ik}}b_k$$
```python
L = torch.rand((1,3))
M = torch.rand((3,))

cmm = torch.matmul(L,M)
cein = torch.einsum('ij,j->i',L,M)
print(f'regular: {cmm} \neinsum: {cein}')
```
{{< embed einsum.ipynb#matrix-vector echo=true >}}

##### 6. Matrix-Matrix multiplication
$$C_{ij}= \sum_k{}A_{ik}{B_{kj}}$$
```python
a = torch.ones((3,2))
b = torch.ones((2,3))
cmm = torch.matmul(a,b)
cein = torch.einsum('ij,jl->il',a,b)
print(f'regular: {cmm} \neinsum: {cein}')
```
{{< embed einsum.ipynb#matrix-matrix echo=true >}}

##### 7. Dot product (inner product)  

vector: $$c = \sum_{i}{a_i}{b_i}$$
matrix: $$d = \sum_i{}\sum_j{A_{ij} B_{ij}}$$
```python
c = torch.rand((3))
d = torch.rand((3))

c_dot = torch.dot(c,d)
cein = torch.einsum('i,i->',c,d)

print(f'c: {c}, c: {c.shape}')
print(f'c_dot: {c_dot}')
print(f'regular: {c_dot} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#dot-product echo=true >}}

##### 8. Hadamard Product (elementwise multiplication without add)

$$C_{ij} = A_{ij}B_{ij}$$
```python
c = torch.randn((3,2))
d = torch.randn((3,2))
cmm = c * d
cein = torch.einsum('ij,ij->ij',c,d)
print(f'regular: {cmm} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#Hadamard-product echo=true >}}

##### 9. Outer Product (vector multiply vector)

$$C_{ij} = a_{i}b_{j}$$
```python
x = torch.rand(3)
y = torch.rand(5)
print(f'x: {x}, x: {x.shape}')
print(f'y: {y}, y: {y.shape}')

c_outer = torch.outer(x,y)
cein = torch.einsum('i,j->ij',x,y)
print(f'regular: {c_outer} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#outer-product echo=true >}}

##### 10. batch matrix multiplication

$$C_{ijl}= \sum_k{}A_{ijk}{B_{ikl}}$$
```python
R = torch.rand(3,2,6)
S = torch.rand(3,6,3)
cmn = np.matmul(R,S)
cmm = torch.matmul(R,S)

cein = torch.einsum('ijk,ikl->ijl',R,S)

print(f'regular: {cmm}\n numpy: {cmn} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#batch-matrix echo=true >}}

##### 11. Diagonal Matrix (return only the diagonal value of a matrix)
$$A_{e_j} = \sum_i{}a_{i,j}e_i$$

```python
T = torch.rand(3,3)

cein = torch.einsum('ii->i',T)
print(f'T: {T} \nT shape: {T.shape}')
c_diag = torch.diag(T)

print(f'regular: {c_diag} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#diag-mat echo=true >}}

##### 12. Trace (take sum along diagonal axis; square matrix only)
$$tr(A)= \sum_i{a_{ii}}$$

```python
c_trace = torch.trace(T)
cein = torch.einsum('ii->',T)
print(f'T: {T}')
print(f'regular: {c_trace} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#trace echo=true >}}

##### 13. Tensor contraction

$$C_{ilmno} = \sum_j{}\sum_k{A_{ijkl}{B_{mnjok}}}$$

```python
o = torch.rand((3,4,2))
p = torch.rand((4,3,6))
print(f'value: {o.shape} value2: {p.shape}')

c_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))
cein = torch.einsum('ijk,jil->kl',o,p)
print(f'regular: {c_tdot} \n  einsum: {cein}')
```
{{< embed einsum.ipynb#contraction echo=true >}}

##### 14. Bilinear Transformation

$$C_{im} = \sum_j{}\sum_o{A_{ij}{B_{mjo}}C_{io}}$$
```python
a = torch.rand(2,3)
b = torch.rand(5,3,7)
c = torch.rand(2,7)

torch.einsum('ik,jkl,il->ij',[a,b,c])
```
{{< embed einsum.ipynb#contraction echo=true >}}



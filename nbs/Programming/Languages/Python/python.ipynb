{
 "cells": [
  {
   "cell_type": "raw",
   "id": "157f6f55",
   "metadata": {},
   "source": [
    "---\n",
    "title: Python\n",
    "subtitle: Python language\n",
    "about:\n",
    "  template: marquee\n",
    "  image: ../../../images/gradientv12.jpg \n",
    "  links:\n",
    "    - icon: twitter\n",
    "      text: twitter\n",
    "      href: https://twitter.com\n",
    "    - icon: github\n",
    "      text: Github\n",
    "      href: https://github.com\n",
    "listing:\n",
    "  sort: \"date desc\"\n",
    "  contents: \"posts\"\n",
    "  sort-ui: false\n",
    "  filter-ui: false\n",
    "  categories: true\n",
    "  feed: true\n",
    "format: \n",
    "  html: \n",
    "    code-fold: false\n",
    "page-layout: full\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150be7ec",
   "metadata": {},
   "source": [
    "![](../../../images/code3.png)\n",
    "\n",
    "## Einsum in Python\n",
    "\n",
    "[Einsum](https://en.wikipedia.org/wiki/Einstein_notation) stand for  Einstein summation convention with was invented by Albert Einstein.\n",
    "The convention help simplify the calculation of Tensor indices thus speedup the computation. Write more concise and efficient code especially in Python which not known for speed. It has more impact on Framework such as Pytorch that also generate GPU code.\n",
    "as describe by [Tim Rocktaschel](https://rockt.github.io/2018/04/30/einsum)\n",
    "\n",
    "Here sample code for\n",
    "Tensor calculation using Numpy, Pytorch, einsum where available\n",
    "\n",
    "##### 1.  Transpose\n",
    "$$A_{ij} = B_{ji}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcba7ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c_ntp \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(X)\n\u001b[1;32m      2\u001b[0m c_tp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(X,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m cein \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij->ji\u001b[39m\u001b[38;5;124m'\u001b[39m,X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "c_ntp = np.transpose(X)\n",
    "c_tp = torch.transpose(X,0,1)\n",
    "cein = torch.einsum('ij->ji',X)\n",
    "print(f'numpy: {c_ntp}\\n pytorch: {c_tp}\\n \\neinsum: {cein}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67eff1",
   "metadata": {},
   "source": [
    "<!-- 12A0366C:einsum.ipynb#mat-transpose |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 2. Summation of matrix\n",
    "$$b = \\sum_i{}\\sum_j{A_{ij}} = A_{ij}$$\n",
    "```python\n",
    "cma = torch.sum(X)\n",
    "cein = torch.einsum('ij->',X)\n",
    "\n",
    "print(f'regular: {cma} \\neinsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#matrix-sum |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 3. Row summation (1xn)\n",
    "\n",
    "$$b_i = \\sum_{j}{Aij}$$\n",
    "```python\n",
    "rows = torch.sum(X,dim=0)\n",
    "cein = torch.einsum('ij->i',X)\n",
    "\n",
    "print(f'regular: {rows} \\neinsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#row-summation |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 4. Comlumn summation (mx1)\n",
    "\n",
    "$$b_j = \\sum_i{A_{ij}}$$\n",
    "```python\n",
    "c_col = torch.sum(X,dim=1)\n",
    "cein = torch.einsum('ij->j',X)\n",
    "\n",
    "print(f'regular: {c_col} \\neinsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#column-summation |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 5. Matrix-vector multiplication\n",
    "\n",
    "$$c_i = \\sum_k{A_{ik}}b_k$$\n",
    "```python\n",
    "L = torch.rand((1,3))\n",
    "M = torch.rand((3,))\n",
    "\n",
    "cmm = torch.matmul(L,M)\n",
    "cein = torch.einsum('ij,j->i',L,M)\n",
    "print(f'regular: {cmm} \\neinsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#matrix-vector |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 6. Matrix-Matrix multiplication\n",
    "$$C_{ij}= \\sum_k{}A_{ik}{B_{kj}}$$\n",
    "```python\n",
    "a = torch.ones((3,2))\n",
    "b = torch.ones((2,3))\n",
    "cmm = torch.matmul(a,b)\n",
    "cein = torch.einsum('ij,jl->il',a,b)\n",
    "print(f'regular: {cmm} \\neinsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#matrix-matrix |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 7. Dot product (inner product)  \n",
    "\n",
    "vector: $$c = \\sum_{i}{a_i}{b_i}$$\n",
    "matrix: $$d = \\sum_i{}\\sum_j{A_{ij} B_{ij}}$$\n",
    "```python\n",
    "c = torch.rand((3))\n",
    "d = torch.rand((3))\n",
    "\n",
    "c_dot = torch.dot(c,d)\n",
    "cein = torch.einsum('i,i->',c,d)\n",
    "\n",
    "print(f'c: {c}, c: {c.shape}')\n",
    "print(f'c_dot: {c_dot}')\n",
    "print(f'regular: {c_dot} \\n  einsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#dot-product |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 8. Hadamard Product (elementwise multiplication without add)\n",
    "\n",
    "$$C_{ij} = A_{ij}B_{ij}$$\n",
    "```python\n",
    "c = torch.randn((3,2))\n",
    "d = torch.randn((3,2))\n",
    "cmm = c * d\n",
    "cein = torch.einsum('ij,ij->ij',c,d)\n",
    "print(f'regular: {cmm} \\n  einsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#Hadamard-product |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 9. Outer Product (vector multiply vector)\n",
    "\n",
    "$$C_{ij} = a_{i}b_{j}$$\n",
    "```python\n",
    "x = torch.rand(3)\n",
    "y = torch.rand(5)\n",
    "print(f'x: {x}, x: {x.shape}')\n",
    "print(f'y: {y}, y: {y.shape}')\n",
    "\n",
    "c_outer = torch.outer(x,y)\n",
    "cein = torch.einsum('i,j->ij',x,y)\n",
    "print(f'regular: {c_outer} \\n  einsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#outer-product |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 10. batch matrix multiplication\n",
    "\n",
    "$$C_{ijl}= \\sum_k{}A_{ijk}{B_{ikl}}$$\n",
    "```python\n",
    "R = torch.rand(3,2,6)\n",
    "S = torch.rand(3,6,3)\n",
    "cmn = np.matmul(R,S)\n",
    "cmm = torch.matmul(R,S)\n",
    "\n",
    "cein = torch.einsum('ijk,ikl->ijl',R,S)\n",
    "\n",
    "print(f'regular: {cmm}\\n numpy: {cmn} \\n  einsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#batch-matrix |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 11. Diagonal Matrix (return only the diagonal value of a matrix where all other value are 0)\n",
    "$$\n",
    "\\forall i,j \\in \\{1, 2, \\ldots, n\\}, i \\ne j \\implies d_{i,j} = 0\n",
    "$$\n",
    "\n",
    "```python\n",
    "T = torch.rand(3,3)\n",
    "\n",
    "cein = torch.einsum('ii->i',T)\n",
    "print(f'T: {T} \\nT shape: {T.shape}')\n",
    "c_diag = torch.diag(T)\n",
    "\n",
    "print(f'regular: {c_diag} \\n  einsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#diag-mat |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 12. Trace (take sum along diagonal axis; square matrix only)\n",
    "$$tr(A)= \\sum_i{a_{ii}}$$\n",
    "\n",
    "```python\n",
    "c_trace = torch.trace(T)\n",
    "cein = torch.einsum('ii->',T)\n",
    "print(f'T: {T}')\n",
    "print(f'regular: {c_trace} \\n  einsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#trace |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 13. Tensor contraction\n",
    "\n",
    "$$C_{ilmno} = \\sum_j{}\\sum_k{A_{ijkl}{B_{mnjok}}}$$\n",
    "\n",
    "```python\n",
    "o = torch.rand((3,4,2))\n",
    "p = torch.rand((4,3,6))\n",
    "print(f'value: {o.shape} value2: {p.shape}')\n",
    "\n",
    "c_tdot = torch.tensordot(o,p,dims=([1,0],[0,1]))\n",
    "cein = torch.einsum('ijk,jil->kl',o,p)\n",
    "print(f'regular: {c_tdot} \\n  einsum: {cein}')\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#contraction |  | echo:false,warning:false,asis:true,eval:false -->\n",
    "\n",
    "##### 14. Bilinear Transformation\n",
    "\n",
    "$$C_{im} = \\sum_j{}\\sum_o{A_{ij}{B_{mjo}}C_{io}}$$\n",
    "```python\n",
    "a = torch.rand(2,3)\n",
    "b = torch.rand(5,3,7)\n",
    "c = torch.rand(2,7)\n",
    "\n",
    "torch.einsum('ik,jkl,il->ij',[a,b,c])\n",
    "```\n",
    "<!-- 12A0366C:einsum.ipynb#contraction |  | echo:false,warning:false,asis:true,eval:false -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
